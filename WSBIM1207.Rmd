---
title: "Introduction to bioinformatics"
author: "Laurent Gatto"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
knit: bookdown::preview_chapter
description: "Course material for the Introduction to bioinformatics (WSBIM1207) course at UCLouvain."
output:
  msmbstyle::msmb_html_book:
    toc: TRUE
    toc_depth: 1
    split_by: chapter
    split_bib: no
    css: style.css
link-citations: yes
bibliography: [refs.bib, packages.bib]
---


# Preamble {-}

The [WSBIM1207](https://uclouvain.be/cours-2019-wsbim1207.html) course
is an introduction to bioinformatics (and data science) for biology
and biomedical students. It introduces bioinformatics methodology and
technologies without relying on any prerequisites. The aim of this
course is for students to be in a position to understand important
notions of bioinformatics and tackle simple bioinformatics-related
problems in R, in particular to develop simple R analysis scripts and
reproducible analysis reports to interrogate, visualise and understand
data in a tidy tabular format.

The course is followed by *Bioinformatics*
([WSBIM1322](https://github.com/UCLouvain-CBIO/WSBIM1322)) and *Omics
data analysis*
([WSBIM2122](https://github.com/UCLouvain-CBIO/WSBIM2122)).

It is interesting to start this course by asking the students, who
have likely not yet been exposed to bioinformatics

> What is bioinformatics?


While [Wikipedia
defines](https://en.wikipedia.org/wiki/Bioinformatics) it as

> Bioinformatics is an interdisciplinary field that develops methods
  and software tools for understanding biological data.

this course won't try to answer that question by providing an overview
of the many context where methods and software tools are developed in
the frame of biological data. We will focus on the getting hands-on
experience in data manipulation. Hence, the material and how it is
taught will be focused on practice.

## Motivation {-}

Today, it is difficult to overestimate the very broad importance and
impact of *data*. Given the abundance of data around us, and the
sophistication of tools for their analysis and interpretation that are
readily available, data has become a tool of profound social
change. Resarch in general, and biomedical research in particular, is
at the centre of this evolution. And while bioinformatics has been
playing a central role in bio-medical research for many years now,
bioinformatics skills aren't well integrated in life science
curricula, limiting students in their career prospects and research
horizon [@WilsonSayres:2018]. It is important for young researchers to
acquire quantitative, computational and data skills to address the
challenges that lie ahead.


This first course will focus on the data skills. It will not teach to
use any specific piece of bioinformatics software. Once one has
identified a relevant piece of software, running it shouldn't be a
major problem[^runningsw]. The important part will be to understand
what it does, why it does it and how to assess if the output can be
trusted. The latter requires to explore and understand the data and
the results, i.e. data skills. As described by @Buffalo:2015, equating
learning a piece of bioinformatics software to learning bioinformatics
is like learning pipetting as a means to learn molecular biology.

[^runningsw]: Although, in all fairness, some bioinformatics software
are notoriously difficult to install and run.

Critical thinking is essential in any aspect of research, and
bioinformatics and data analysis doesn't escape that rule. Teaching
critical thinking is however very difficult, and arguably impossible
without first possessing the skills and master the tools to be
critical about data. Rather than teaching a limited set of software
tools, this course aims at teaching a set of hands-on skills and
methodologies that allow to interrogate and visualise data, and hence
be in a position to be critical with respect to new data. In addition,
while specific software become obsolete and are replaced, or are
specific to one specific field of research, critical investigation of
data will always be required and will never be replaced.

We will be using the [R](https://www.R-project.org/) language and
environment [@R] and the [RStudio integrated development
environment](https://www.rstudio.com/products/RStudio/) to acquire
these data skills. Other interactive language such as
[Python](https://www.R-project.org/) and the interactive [jupyer
notebooks](https://jupyter.org/) would also have been a good fit. One
motivation of this choice is the availability of numerous
R/[Bionductor](https://www.bioconductor.org/) packages [@Huber:2015]
for the analysis of high throughput biology data.

Another reason why the focus of the course ought to be on data skills
is that a notable difficulty in modern, multidisciplinary research is
communication. Wetlab biomedical scientists aren't required to become
bioinformaticians, statisticians, programmers, ... to be outstanding
researchers, but they will need to communicate efficiency with these
experts (and vice versa). What most often unites all these experts is
data, and communication around data is critical. The importance of
critical thinking and communication around data becomes more evident
when one realises that, when tracking work in bioinformatics core
facilities, only a minority of projects were purely routine and that
most researchers came to the bioinformatics core seeking customized
analysis, rather than a standardized package [@Chang:2015].

To illustrate the reasons why R in general (and in the case of
biomedical sciences, Bioconductor in particular) are worth learning, I
provide here some examples where these software are used. From a
bioinformatics point of view:

- Single cell transcriptomics data: [Bioconductor workflow for
  single-cell RNA sequencing: Normalization, dimensionality reduction,
  clustering, and lineage
  inference](https://f1000research.com/articles/6-1158/v1)

- Quantitative proteomics data: [A Bioconductor workflow for
  processing and analysing spatial proteomics
  data](https://f1000research.com/articles/5-2926/v2).

- Epigenetic data: [Differential methylation analysis of reduced
  representation bisulfite sequencing experiments using
  edgeR](https://f1000research.com/articles/6-2055/v2)

- ...

And more generally, at Google, Pfizer, Merck, GSK, Bank of America,
the InterContinental Hotels Group, Shell, ...

- [Data Analysts Captivated by R's
  Power](https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html)

In summary, the overall learning objectives of this course are:

- for students to apply and adapt the general data analysis techniques
  and principles that are presented to new data and new contexts;

- establish links between different concepts seen in the course such
  as, for example, the importance of tidy data in general, how it
  applies to dataframes in R, and how it enables reasoning on the
  data;

- become autonomous when being presented with new data and be in a
  position to explore and understand them.

## References and credits {-}

References are provided throughout the course. Several stand out
however, as they cover large parts of the material or provide
complementary resources.

The material for the first chapters, covering the *Introduction to
data science with R*, was originally based on the [**Data Carpentry**
Ecology
curiculum](https://datacarpentry.org/lessons/#ecology-workshop)
[@DCRecol]. The main dataset by [Blackmore *et al.*
(2017)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5544260/),
introduced in 2021, was prepared by the [Bioconductor education
group](https://github.com/Bioconductor/bioconductor-teaching/blob/master/README.md)
as part of their lesson development.

General references for this course are *R for Data Science*
[@r4ds:2017] and *Bioinformatics Data Skills* [@Buffalo:2015].

The [RStudio Cheat
Sheets](https://www.rstudio.com/resources/cheatsheets/) are also a
handy resource and readers will be pointed to specific sheets in the
respective chapters.

This course is being taught by Prof Laurent Gatto with invaluable
assistance from Mr Jean Fain (since 2019), Ms Valetine Robaux (since
2020), Dr Axelle Loriot (2018 - 2021) and Mr Kevin Missault (2018 -
2020) at the Faculty of Pharmacy and Biomedical Sciences (FASB) at the
UCLouvain, Belgium.

## Pre-requisites {-}

There are no programming or technical pre-requisities for this course,
other than basic computer usage, such as general knowledge about files
(binary and text files) and folders and as well as downloading
files. Familiarity with a spreadsheet editor is helpful for the first
chapter.

Software requirements are documented in the *Setup* section below.

## About this course material {-}

This material is written in R markdown [@R-rmarkdown] and compiled as a
book using `knitr` [@R-knitr] `bookdown` [@R-bookdown]. The source
code is publicly available in a Github repository
[https://github.com/uclouvain-cbio/WSBIM1207](https://github.com/uclouvain-cbio/WSBIM1207)
and the compiled material can be read at http://bit.ly/WSBIM1207.

Contributions to this material are welcome. The best way to contribute
or contact the maintainers is by means of pull requests and
[issues](https://github.com/uclouvain-cbio/WSBIM1207/issues). Please
familiarise yourself with the [code of
conduct](https://github.com/UCLouvain-CBIO/WSBIM1207/blob/master/CONDUCT.md). By
participating in this project you agree to abide by its terms.

## Citation {-}

If you use this course, please cite it as

> Laurent Gatto, Kevin Missault, Manon Martin & Axelle Loriot. (2021,
> September 24). UCLouvain-CBIO/WSBIM1207: Introduction to
> bioinformatics (Version
> v2.0.0). Zenodo. DOI: 10.5281/zenodo.5532658

[![DOI](https://zenodo.org/badge/147494586.svg)](https://zenodo.org/badge/latestdoi/147494586)


## License {-}

This material is licensed under the [Creative Commons
Attribution-ShareAlike 4.0
License](https://creativecommons.org/licenses/by-sa/4.0/).


## Setup {-}

For chapter \@ref(sec-dataorg) about *Data organisation with
Spreadsheets*, a spreadsheet programme is necessary.

We will be using the [R environment for statistical
computing](https://www.r-project.org/) as main data science language.
We will also use the
[RStudio](https://www.rstudio.com/products/RStudio/) interface to
interact with R and write scripts and reports. Both R and RStudio are
easy to install and works on all major operating systems.

Once R and RStudio are installed, a set of packages will need to be
installed. See section \@ref(sec-setup2) for details.

To build this book, you'll need `bookdown` [@R-bookdown] and a
fork[^msmbfork] of [`msmbstyle`
style](https://github.com/grimbough/msmbstyle/) [@R-msmbstyle].

[^msmbfork]: https://github.com/lgatto/msmbstyle

```{r combilebook1, eval=FALSE}
install.packages("bookdown")
devtools::install_github("lgatto/msmbstyle")
```

In the course's work directory, simply type

```{r combilebook2, eval=FALSE}
bookdown::render_book(".")
```

<!--chapter:end:index.Rmd-->

# Data organisation with Spreadsheets {#sec-dataorg}

**Learning Objectives**

- Learn about spreadsheets, their strenghts and weaknesses
- How do we format data in spreadsheets for effective data use?
- Learn about common spreadsheet errors and how to correct them.
- Organize your data according to tidy data principles.
- Learn about text-based spreadsheet formats such as the
  comma-separated (CSV) or tab-separated formats.

## Spreadsheet programs

**Question**

- What are basic principles for using spreadsheets for good data
  organization?

**Objective**

- Describe best practices for organizing data so computers can make
  the best use of data sets.

**Keypoint**

- Good data organization is the foundation of any research project.

Good data organization is the foundation of your research
project. Most researchers have data or do data entry in
spreadsheets. Spreadsheet programs are very useful graphical
interfaces for designing data tables and handling very basic data
quality control functions. See also @Broman:2018.

### Spreadsheet outline {-}

Spreadsheets are good for data entry. Therefore we have a lot of data
in spreadsheets.  Much of your time as a researcher will be spent in
this 'data wrangling' stage.  It's not the most fun, but it's
necessary. We'll teach you how to think about data organization and
some practices for more effective data wrangling.

### What this lesson will not teach you {-}

- How to do *statistics* in a spreadsheet
- How to do *plotting* in a spreadsheet
- How to *write code* in spreadsheet programs

If you're looking to do this, a good reference is [Head First
Excel](https://www.amazon.com/Head-First-Excel-learners-spreadsheets/dp/0596807694/),
published by O'Reilly.


### Why aren't we teaching data analysis in spreadsheets {-}

- Data analysis in spreadsheets usually requires a lot of manual
  work. If you want to change a parameter or run an analysis with a
  new dataset, you usually have to redo everything by hand. (We do
  know that you can create macros, but see the next point.)

- It is also difficult to track or reproduce statistical or plotting
  analyses done in spreadsheet programs when you want to go back to
  your work or someone asks for details of your analysis.

Many spreadsheet programs are available. Since most participants
utilise Excel as their primary spreadsheet program, this lesson will
make use of Excel examples. A free spreadsheet program that can also
be used is LibreOffice. Commands may differ a bit between programs,
but the general idea is the same.

Spreadsheet programs encompass a lot of the things we need to be able
to do as researchers. We can use them for:

- Data entry
- Organizing data
- Subsetting and sorting data
- Statistics
- Plotting

Spreadsheet programs use tables to represent and display data. Data
formatted as tables is also the main theme of this chapter, and we
will see how to organise data into tables in a standardised way to
ensure efficient downstream analysis.


<!-- Here are three examples from research papers: -->


<!-- ```{r, results='markup', fig.cap="A figure from a bioinformatics paper ([Hugues et al. (2014)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3866554/)) that compares different proteomics data processing methods (along the columns) and how (1) these methods control false positive rate for spiked-in data, (2) their effect on the variabilty due to different operators, and (2) how they affect the root mean square error (RMSE).", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'} -->
<!-- knitr::include_graphics("./figs/tab1.png") -->
<!-- ``` -->


<!-- ```{r, results='markup', fig.cap="This table summarises the spatial proteomics experiments that were re-analysed as part a meta-analysis by [Gatto et al. (2018)](https://www.sciencedirect.com/science/article/pii/S1367593118301339). ", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'} -->
<!-- knitr::include_graphics("./figs/tab2.png") -->
<!-- ``` -->

<!-- ```{r, results='markup', fig.cap="Parts of the quantitative data of a spatial proteomics dataset by [Geladaki et al. (2019)](https://www.nature.com/articles/s41467-018-08191-w). The supplementary table is available [here](https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-018-08191-w/MediaObjects/41467_2018_8191_MOESM4_ESM.xlsx)", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'} -->
<!-- knitr::include_graphics("./figs/tab3a.png") -->
<!-- ``` -->

<!-- ```{r, results='markup', fig.cap="Luminescence read-out of a 96 well plate. The file is available [here](https://github.com/UCLouvain-CBIO/WSBIM1207/files/2836821/luminescence.xlsx).", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'} -->
<!-- knitr::include_graphics("./figs/tab4.png") -->
<!-- ``` -->


`r msmbstyle::question_begin()`
Discuss the following points with your neighbour

- How many people have used spreadsheets, in their research, courses,
  or at home?
- What kind of operations do you do in spreadsheets?
- Which ones do you think spreadsheets are good for?
- How many people have accidentally done something that made them
  frustrated or sad?
`r msmbstyle::question_end()`

### Problems with Spreadsheets {-}

Spreadsheets are good for data entry, but in reality we tend to
use spreadsheet programs for much more than data entry. We use them
to create data tables for publications, to generate summary
statistics, and make figures.

Generating tables for publications in a spreadsheet is not
optimal - often, when formatting a data table for publication, we’re
reporting key summary statistics in a way that is not really meant to
be read as data, and often involves special formatting
(merging cells, creating borders, making it pretty). We advise you to
do this sort of operation within your document editing software.

The latter two applications, generating statistics and figures, should
be used with caution: because of the graphical, drag and drop nature of
spreadsheet programs, it can be very difficult, if not impossible, to
replicate your steps (much less retrace anyone else's), particularly if your
stats or figures require you to do more complex calculations. Furthermore,
in doing calculations in a spreadsheet, it’s easy to accidentally apply a
slightly different formula to multiple adjacent cells. When using a
command-line based statistics program like R or SAS, it’s practically
impossible to apply a calculation to one observation in your
dataset but not another unless you’re doing it on purpose.

### Using Spreadsheets for Data Entry and Cleaning {-}

However, there are circumstances where you might want to use a spreadsheet
program to produce “quick and dirty” calculations or figures, and data
cleaning will help you use some of these features. Data cleaning also
puts your data in a better format prior to importation into a
statistical analysis program. We will show you how to use some features of
spreadsheet programs to check your data quality along the way and produce
preliminary summary statistics.

In this lesson, we will assume that you are most likely using Excel as
your primary spreadsheet program - there are others (gnumeric, Calc
from OpenOffice), and their functionality is similar, but Excel seems
to be the program most used by biologists and biomedical researchers.

In this lesson we're going to talk about:

1. Formatting data tables in spreadsheets
2. Formatting problems
3. Dates as data
4. Quality control
5. Exporting data

## Formatting data tables in spreadsheets

**Questions**

- How do we format data in spreadsheets for effective data use?

**Objectives**

- Describe best practices for data entry and formatting in
  spreadsheets.

- Apply best practices to arrange variables and observations in a
  spreadsheet.

**Keypoints**

- Never modify your raw data. Always make a copy before making any
  changes.

- Keep track of all of the steps you take to clean your data in a
  plain text file.

- Organize your data according to tidy data principles.

The most common mistake made is treating spreadsheet programs like lab
notebooks, that is, relying on context, notes in the margin, spatial
layout of data and fields to convey information. As humans, we can
(usually) interpret these things, but computers don't view information
the same way, and unless we explain to the computer what every single
thing means (and that can be hard!), it will not be able to see how
our data fits together.

Using the power of computers, we can manage and analyze data in much
more effective and faster ways, but to use that power, we have to set
up our data for the computer to be able to understand it (and
computers are very literal).

This is why it’s extremely important to set up well-formatted tables
from the outset - before you even start entering data from your very
first preliminary experiment. Data organization is the foundation of
your research project. It can make it easier or harder to work with
your data throughout your analysis, so it's worth thinking about when
you're doing your data entry or setting up your experiment. You can
set things up in different ways in spreadsheets, but some of these
choices can limit your ability to work with the data in other programs
or have the you-of-6-months-from-now or your collaborator work with
the data.

**Note:** the best layouts/formats (as well as software and
interfaces) for data entry and data analysis might be different. It is
important to take this into account, and ideally automate the
conversion from one to another.

### Keeping track of your analyses

When you're working with spreadsheets, during data clean up or
analyses, it's very easy to end up with a spreadsheet that looks very
different from the one you started with. In order to be able to
reproduce your analyses or figure out what you did when a reviewer or
instructor asks for a different analysis, you should

- create a new file with your cleaned or analyzed data. Don't modify
  the original dataset, or you will never know where you started!

- keep track of the steps you took in your clean up or analysis. You
  should track these steps as you would any step in an experiment. We
  recommend that you do this in a plain text file stored in the same
  folder as the data file.

This might be an example of a spreadsheet setup:

![](./figs/spreadsheet-setup-updated.png)

Put these principles in to practice today during your exercises.

While versioning is out of scope for this course, you can look at the
Carpentries lesson on
['Git'](http://swcarpentry.github.io/git-novice/) to learn how to
maintain **version control** over your data. See also this [blog
post](https://lgatto.github.io/github-intro/) for a quick tutorial or
@Perez-Riverol:2016 for a more research-oriented use-case.


### Structuring data in spreadsheets

The cardinal rules of using spreadsheet programs for data:

1. Put all your variables in columns - the thing you're measuring,
   like 'weight' or 'temperature'.
2. Put each observation in its own row.
3. Don't combine multiple pieces of information in one cell. Sometimes
   it just seems like one thing, but think if that's the only way
   you'll want to be able to use or sort that data.
4. Leave the raw data raw - don't change it!
5. Export the cleaned data to a text-based format like CSV
   (comma-separated values) format. This ensures that anyone can use
   the data, and is required by most data repositories.

For instance, we have data from patient that visited several hospitals
from Brussels, Belgium. They recorded the date the visit, the
hospital, the patients gender, weight and blood group.


If they were to keep track of the data like this:

![](./figs/multiple-info.png)

the problem is that the ABO and rhesus groups are in the same `Blood`
type column. So, if they wanted to look at all observations of the A
group or look at weight distributions by ABO group, it would be tricky
to do this using this data setup. If instead we put the ABO and rhesus
groups in different columns, you can see that it would be much easier.

![](./figs/single-info.png)

An important rule when setting up a datasheet, is that **columns are
used for variables** and **rows are used for observations**:

- columns are variables
- rows are observations
- cells are individual values

`r msmbstyle::question_begin()`

We're going to take a messy data and describe how we would clean it
up.

1. Download a messy data by clicking
   [here](https://github.com/UCLouvain-CBIO/WSBIM1207/raw/master/data/messy_covid.xlsx).

2. Open up the data in a spreadsheet program.

3. You can see that there are two tabs. The data contains various
   clinical variables recorded in various hospitals in Brussels during
   the first and second COVID-19 waves in 2020. As you can see, the
   data have been recorded differently during the march and november
   waves. Now you're the person in charge of this project and you want
   to be able to start analyzing the data.

4. With the person next to you, identify what is wrong with this
   spreadsheet. Also discuss the steps you would need to take to clean
   up first and second wave tabs, and to put them all together in one
   spreadsheet.

**Important:** Do not forget our first piece of advice: to create a
new file (or tab) for the cleaned data, never modify your original
(raw) data.

After you go through this exercise, we'll discuss as a group what was wrong
with this data and how you would fix it.

`r msmbstyle::question_end()`

<!-- - Take about 10 minutes to work on this exercise. -->
<!-- - All the mistakes in the *common mistakes* section below are present -->
<!--   in the messy dataset. If the exercise is done during a workshop, ask -->
<!--   people what they saw as wrong with the data. As they bring up -->
<!--   different points, you can refer to the common mistakes or expand a -->
<!--   bit on the point they brought up. -->
<!-- - If you get a response where they've fixed the date, you can pause -->
<!--   and go to the dates lesson. Or you can say you'll come back to dates -->
<!--   at the end. -->

`r msmbstyle::question_begin()`

Once you have tidied up the data, answer the following questions:

- How many men and women took part in the study?
- How many A, AB, and B types have been tested?
- As above, but disregarding the contaminated samples?
- How many Rhesus + and - have been tested?
- How many universal donors (0-) have been tested?
- What is the average weight of AB men?
- How many samples have been tested in the different hospitals?

`r msmbstyle::question_end()`

An **excellent reference**, in particular with regard to R scripting
is the *Tidy Data* paper @Wickham:2014.

## Common Spreadsheet Errors

**Questions**

- What are some common challenges with formatting data in spreadsheets
  and how can we avoid them?

**Objectives**

- Recognize and resolve common spreadsheet formatting problems.

**Keypoints**

- Avoid using multiple tables within one spreadsheet.
- Avoid spreading data across multiple tabs.
- Record zeros as zeros.
- Use an appropriate null value to record missing data.
- Don't use formatting to convey information or to make your spreadsheet look pretty.
- Place comments in a separate column.
- Record units in column headers.
- Include only one piece of information in a cell.
- Avoid spaces, numbers and special characters in column headers.
- Avoid special characters in your data.
- Record metadata in a separate plain text file.

<!-- This lesson is meant to be used as a reference for discussion as -->
<!-- learners identify issues with the messy dataset discussed in the -->
<!-- previous lesson. Instructors: don't go through this lesson except to -->
<!-- refer to responses to the exercise in the previous lesson. -->

There are a few potential errors to be on the lookout for in your own
data as well as data from collaborators or the Internet. If you are
aware of the errors and the possible negative effect on downstream
data analysis and result interpretation, it might motivate yourself
and your project members to try and avoid them. Making small changes
to the way you format your data in spreadsheets, can have a great
impact on efficiency and reliability when it comes to data cleaning
and analysis.

- [Using multiple tables](#tables)
- [Using multiple tabs](#tabs)
- [Not filling in zeros](#zeros)
- [Using problematic null values](#null)
- [Using formatting to convey information](#formatting)
- [Using formatting to make the data sheet look pretty](#formatting_pretty)
- [Placing comments or units in cells](#units)
- [Entering more than one piece of information in a cell](#info)
- [Using problematic field names](#field_name)
- [Using special characters in data](#special)
- [Inclusion of metadata in data table](#metadata)

### Using multiple tables {#tables}

A common strategy is creating multiple data tables within
one spreadsheet. This confuses the computer, so don't do this!
When you create multiple tables within one
spreadsheet, you’re drawing false associations between things for the computer,
which sees each row as an observation. You’re also potentially using the same
field name in multiple places, which will make it harder to clean your data up
into a usable form. The example below depicts the problem:

![](./figs/2_datasheet_example.jpg)

In the example above, the computer will see (for example) row 4 and assume that all columns A-AF
refer to the same sample. This row actually represents four distinct samples
(sample 1 for each of four different collection dates - May 29th, June 12th, June 19th, and June 26th),
as well as some calculated summary statistics (an average (avr) and standard error of measurement (SEM)) for two of those samples. Other rows are similarly problematic.

### Using multiple tabs {#tabs}

But what about workbook tabs? That seems like an easy way to organize
data, right? Well, yes and no. When you create extra tabs, you fail to
allow the computer to see connections in the data that are there (you
have to introduce spreadsheet application-specific functions or
scripting to ensure this connection). Say, for instance, you make a
separate tab for each day you take a measurement.

This isn't good practice for two reasons:

1. you are more likely to accidentally add inconsistencies to your
   data if each time you take a measurement, you start recording data
   in a new tab, and

2. even if you manage to prevent all inconsistencies from creeping in,
   you will add an extra step for yourself before you analyze the data
   because you will have to combine these data into a single
   datatable. You will have to explicitly tell the computer how to
   combine tabs - and if the tabs are inconsistently formatted, you
   might even have to do it manually.

The next time you’re entering data, and you go to create another tab
or table, ask yourself if you could avoid adding this tab by adding
another column to your original spreadsheet. We used multiple tabs in
our example of a messy data file, but now you've seen how you can
reorganize your data to consolidate across tabs.

Your data sheet might get very long over the course of the
experiment. This makes it harder to enter data if you can’t see your
headers at the top of the spreadsheet. But don't repeat your header
row. These can easily get mixed into the data, leading to problems
down the road. Instead you can [freeze the column
headers](https://support.office.com/en-ca/article/Freeze-column-headings-for-easy-scrolling-57ccce0c-cf85-4725-9579-c5d13106ca6a)
so that they remain visible even when you have a spreadsheet with many
rows.

### Not filling in zeros {#zeros}

It might be that when you're measuring something, it's usually a zero,
say the number of times a rabbit is observed in the survey. Why bother
writing in the number zero in that column, when it's mostly zeros?

However, there's a difference between a zero and a blank cell in a
spreadsheet. To the computer, a zero is actually data. You measured or
counted it. A blank cell means that it wasn't measured and the
computer will interpret it as an unknown value (otherwise known as a
null value).

The spreadsheets or statistical programs will likely mis-interpret
blank cells that you intend to be zeros. By not entering the value of
your observation, you are telling your computer to represent that data
as unknown or missing (null). This can cause problems with subsequent
calculations or analyses. For example, the average of a set of numbers
which includes a single null value is always null (because the
computer can't guess the value of the missing observations). Because
of this, it's very important to record zeros as zeros and truly
missing data as nulls.


### Using problematic null values {#null}

**Example**: using -999 or other numerical values (or zero) to
  represent missing data.

**Solutions**:

There are a few reasons why null values get represented differently
within a dataset.  Sometimes confusing null values are automatically
recorded from the measuring device. If that's the case, there's not
much you can do, but it can be addressed in data cleaning with a tool
like
[OpenRefine](http://www.datacarpentry.org/OpenRefine-ecology-lesson/)
before analysis. Other times different null values are used to convey
different reasons why the data isn't there. This is important
information to capture, but is in effect using one column to capture
two pieces of information. Like for [using formatting to convey
information]((#formatting) it would be good here to create a new
column like 'data_missing' and use that column to capture the
different reasons.


Whatever the reason, it's a problem if unknown or missing data is
recorded as -999, 999, or 0.

Many statistical programs will not recognize that these are intended
to represent missing (null) values. How these values are interpreted
will depend on the software you use to analyze your data. It is
essential to use a clearly defined and consistent null indicator.

Blanks (most applications) and NA (for R) are good
choices. @White:2013 explain good choices for indicating null values
for different software applications in their article:

![](./figs/3_white_table_1.jpg)

### Using formatting to convey information {#formatting}

**Example**: highlighting cells, rows or columns that should be
  excluded from an analysis, leaving blank rows to indicate
  separations in data.

![](./figs/formatting.png)

**Solution**: create a new field to encode which data should be
  excluded.

![](./figs/good_formatting.png)


### Using formatting to make the data sheet look pretty {#formatting_pretty}

**Example**: merging cells.

**Solution**: If you’re not careful, formatting a worksheet to be more
aesthetically pleasing can compromise your computer’s ability to see
associations in the data. Merged cells will make your data unreadable
by statistics software. Consider restructuring your data in such a way
that you will not need to merge cells to organize your data.


### Placing comments or units in cells {#units}

Most analysis software can't see Excel or LibreOffice comments, and
would be confused by comments placed within your data cells. As
described above for formatting, create another field if you need to
add notes to cells. Similarly, don’t include units in cells: ideally,
all the measurements you place in one column should be in the same
unit, but if for some reason they aren’t, create another field and
specify the units the cell is in.


### Entering more than one piece of information in a cell {#info}

**Example**: Recording ABO and Rhesus groups in one cell, such as A+,
B+, A-, ...

**Solution**: Don't include more than one piece of information in a
cell. This will limit the ways in which you can analyze your data.  If
you need both these measurements, design your data sheet to include
this information. For example, include one column the ABO group and
one for the Rhesus group.

### Using problematic field names {#field_name}

Choose descriptive field names, but be careful not to include spaces,
numbers, or special characters of any kind. Spaces can be
misinterpreted by parsers that use whitespace as delimiters and some
programs don’t like field names that are text strings that start with
numbers.

Underscores (`_`) are a good alternative to spaces. Consider writing
names in camel case (like this: ExampleFileName) to improve
readability. Remember that abbreviations that make sense at the moment
may not be so obvious in 6 months, but don't overdo it with names that
are excessively long. Including the units in the field names avoids
confusion and enables others to readily interpret your fields.

**Examples**

| Good Name | Good Alternative | Avoid  |
|-----------|------------------|--------|
| Max_temp_C | MaxTemp | Maximum Temp (°C)  |
| Precipitation_mm | Precipitation | precmm |
| Mean_year_growth | MeanYearGrowth  | Mean growth/year |
| sex | sex | M/F |
| weight | weight | w.|
| cell_type | CellType | Cell Type |
| Observation_01  | first_observation | 1st Obs |


### Using special characters in data {#special}

**Example**: You treat your spreadsheet program as a word processor
when writing notes, for example copying data directly from Word or
other applications.

**Solution**: This is a common strategy. For example, when writing
longer text in a cell, people often include line breaks, em-dashes,
etc in their spreadsheet.  Also, when copying data in from
applications such as Word, formatting and fancy non-standard
characters (such as left- and right-aligned quotation marks) are
included.  When exporting this data into a coding/statistical
environment or into a relational database, dangerous things may occur,
such as lines being cut in half and encoding errors being thrown.

General best practice is to avoid adding characters such as newlines,
tabs, and vertical tabs.  In other words, treat a text cell as if it
were a simple web form that can only contain text and spaces.


### Inclusion of metadata in data table {#metadata}

**Example**: You add a legend at the top or bottom of your data table
  explaining column meaning, units, exceptions, etc.

**Solution**: Recording data about your data (“metadata”) is
essential. You may be on intimate terms with your dataset while you
are collecting and analysing it, but the chances that you will still
remember that the variable "sglmemgp" means single member of group,
for example, or the exact algorithm you used to transform a variable
or create a derived one, after a few months, a year, or more are slim.

As well, there are many reasons other people may want to examine or
use your data - to understand your findings, to verify your findings,
to review your submitted publication, to replicate your results, to
design a similar study, or even to archive your data for access and
re-use by others. While digital data by definition are
machine-readable, understanding their meaning is a job for human
beings. The importance of documenting your data during the collection
and analysis phase of your research cannot be overestimated,
especially if your research is going to be part of the scholarly
record.

However, metadata should not be contained in the data file
itself. Unlike a table in a paper or a supplemental file, metadata (in
the form of legends) should not be included in a data file since this
information is not data, and including it can disrupt how computer
programs interpret your data file. Rather, metadata should be stored
as a separate file in the same directory as your data file, preferably
in plain text format with a name that clearly associates it with your
data file. Because metadata files are free text format, they also
allow you to encode comments, units, information about how null values
are encoded, etc. that are important to document but can disrupt the
formatting of your data file.

Additionally, file or database level metadata describes how files that
make up the dataset relate to each other; what format are they in;
and whether they supercede or are superceded by previous files. A
folder-level readme.txt file is the classic way of accounting for all
the files and folders in a project.

(Text on metadata adapted from the online course Research Data
[MANTRA](http://datalib.edina.ac.uk/mantra) by EDINA and Data Library,
University of Edinburgh. MANTRA is licensed under a [Creative Commons
Attribution 4.0 International
License](https://creativecommons.org/licenses/by/4.0/).)


## Exporting data

**Question**

- How can we export data from spreadsheets in a way that is useful for
  downstream applications?

**Objectives**

- Store spreadsheet data in universal file formats.
- Export data from a spreadsheet to a CSV file.

**Keypoints**

- Data stored in common spreadsheet formats will often not be read
  correctly into data analysis software, introducing errors into your
  data.

- Exporting data from spreadsheets to formats like CSV or TSV puts it
  in a format that can be used consistently by most programs.

Storing the data you're going to work with for your analyses in Excel
default file format (`*.xls` or `*.xlsx` - depending on the Excel
version) isn't a good idea. Why?

- Because it is a proprietary format, and it is possible that in the
  future, technology won’t exist (or will become sufficiently rare) to
  make it inconvenient, if not impossible, to open the file.

- Other spreadsheet software may not be able to open files saved in a
  proprietary Excel format.

- Different versions of Excel may handle data differently, leading to
  inconsistencies. [Dates](https://datacarpentry.org/spreadsheet-ecology-lesson/03-dates-as-data/index.html)
  is a well-documented example of inconsistencies in data storage.

- Finally, more journals and grant agencies are requiring you to
  deposit your data in a data repository, and most of them don't
  accept Excel format. It needs to be in one of the formats discussed
  below.

- The above points also apply to other formats such as open data
  formats used by LibreOffice / Open Office. These formats are not
  static and do not get parsed the same way by different software
  packages.


Storing data in a universal, open, and static format will help deal
with this problem. Try tab-delimited (tab separated values or TSV) or
comma-delimited (comma separated values or CSV). CSV files are plain
text files where the columns are separated by commas, hence 'comma
separated values' or CSV. The advantage of a CSV file over an
Excel/SPSS/etc. file is that we can open and read a CSV file using
just about any software, including plain text editors like TextEdit or
NotePad.  Data in a CSV file can also be easily imported into other
formats and environments, such as SQLite and R. We're not tied to a
certain version of a certain expensive program when we work with CSV
files, so it's a good format to work with for maximum portability and
endurance. Most spreadsheet programs can save to delimited text
formats like CSV easily, although they may give you a warning during
the file export.

To save a file you have opened in Excel in CSV format:

1. From the top menu select 'File' and 'Save as'.
2. In the 'Format' field, from the list, select 'Comma Separated
   Values' (`*.csv`).
3. Double check the file name and the location where you want to save
   it and hit 'Save'.

An important note for backwards compatibility: you can open CSV files
in Excel!

```{r, results='markup', fig.cap="Saving an Excel file to CSV.", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/excel-to-csv.png")
```

**A note on R and `xls`**: There are R packages that can read `xls`
files (as well as Google spreadsheets). It is even possible to access
different worksheets in the `xls` documents.

**But**

- some of these only work on Windows
- this equates to replacing a (simple but manual) export to `csv` with
  additional complexity/dependencies in the data analysis R code
- data formatting best practice still apply
- Is there really a good reason why `csv` (or similar) is not
  adequate?

### Caveats on commas {-}

In some datasets, the data values themselves may include commas
(,). In that case, the software which you use (including Excel) will
most likely incorrectly display the data in columns. This is because
the commas which are a part of the data values will be interpreted as
delimiters.


For example, our data might look like this:

```
species_id,genus,species,taxa
AB,Amphispiza,bilineata,Bird
AH,Ammospermophilus,harrisi,Rodent, not censused
AS,Ammodramus,savannarum,Bird
BA,Baiomys,taylori,Rodent
```

In the record `AH,Ammospermophilus,harrisi,Rodent, not censused` the
value for `taxa` includes a comma (`Rodent, not censused`).  If we try
to read the above into Excel (or other spreadsheet program), we will
get something like this:


```{r, results='markup', fig.cap = "The risks of having commas inside comma-separated data.", echo=FALSE, purl=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("./figs/csv-mistake.png")
```

The value for `taxa` was split into two columns (instead of being put
in one column `D`). This can propagate to a number of further
errors. For example, the extra column will be interpreted as a column
with many missing values (and without a proper header). In addition to
that, the value in column `D` for the record in row 3 (so the one
where the value for 'taxa' contained the comma) is now incorrect.

If you want to store your data in `csv` format and expect that your
data values may contain commas, you can avoid the problem discussed
above by putting the values in quotes (""). Applying this rule, our
data might look like this:

```
species_id,genus,species,taxa
"AB","Amphispiza","bilineata","Bird"
"AH","Ammospermophilus","harrisi","Rodent, not censused"
"AS","Ammodramus","savannarum","Bird"
"BA","Baiomys","taylori","Rodent"
```

Now opening this file as a `csv` in Excel will not lead to an extra
column, because Excel will only use commas that fall outside of
quotation marks as delimiting characters.

Alternatively, if you are working with data that contains commas, you
likely will need to use another delimiter when working in a
spreadsheet[^decsep]. In this case, consider using tabs as your delimiter and
working with TSV files. TSV files can be exported from spreadsheet
programs in the same way as CSV files.

[^decsep]: This is of course particularly relevant in European
           countries where the comma is used as a decimal
           separator. In such cases, the default value separator in a
           csv file will be the semi-colon (;), or values will be
           systematically quoted.

If you are working with an already existing dataset in which the data
values are not included in "" but which have commas as both delimiters
and parts of data values, you are potentially facing a major problem
with data cleaning. If the dataset you're dealing with contains
hundreds or thousands of records, cleaning them up manually (by either
removing commas from the data values or putting the values into quotes
- "") is not only going to take hours and hours but may potentially
end up with you accidentally introducing many errors.

Cleaning up datasets is one of the major problems in many scientific
disciplines. The approach almost always depends on the particular
context. However, it is a good practice to clean the data in an
automated fashion, for example by writing and running a script. The
Python and R lessons will give you the basis for developing skills to
build relevant scripts.


## Summary

```{r analysis, results='asis', fig.margin=TRUE, fig.cap="A typical data analysis workflow.", fig.width=7, fig.height=4, echo=FALSE, purl=FALSE}
knitr::include_graphics("./figs/analysis.png")
```

A typical data analysis worflow is illustrated in figure
\@ref(fig:analysis), where data is repeatedly tranformed, visualised,
modelled. This iteration is repeated multiple times until the data is
understood. In many real-life cases, however, most time is spend in
clearning up and preparing the data, rather than actually analysing
and understanding it.

An agile data analysis workflow, with several fast iteration of the
transform/visualise/model cycle are only feasible is the data is
formatted in a predictable way and one can reason on the data without
having to look at it and/or fix it.


## Additional exercises

`r msmbstyle::question_begin()`
Download the supplementary
[table](https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-018-08191-w/MediaObjects/41467_2018_8191_MOESM4_ESM.xlsx)
from [Geladaki et
al. (2019)](https://www.nature.com/articles/s41467-018-08191-w).

Using the best practice documented above, export the file to a
text-based spreadsheet (`csv`, `tsv`, ...). If necessary, manually fix
the table, but making sure you keep a copy of the original data and
document your modifications.
`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`

Download this
[table](https://github.com/UCLouvain-CBIO/WSBIM1207/files/2836821/luminescence.xlsx)
that presents luminescence read-out of a 96 well plate. Reformat it in
a way that makes it amenable to data analysis. Hint: The letters A to
H and the numbers 1 to 12 represent respectively the rows and the
columns of the plate.


`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`

Imagine the following experiment, and produce a data table, making up
values for a dozen of observations.

> Rodents, mice and rats, where collected on various days by operators
> A and B. Their weight and tail length were measured and assigned to
> either a control group (and administered water) or a condition group
> (and administered drug X). Exactly one week later, the body
> measurements were repeated.

`r msmbstyle::question_end()`

<!--chapter:end:10-data-organisation.Rmd-->

# R and RStudio  {#sec-rrstudio}


**Learning Objectives**

- Describe the purpose of the RStudio Script, Console, Environment, and Plots
  panes.

- Organize files and directories for a set of analyses as an R
  project, and understand the purpose of the working directory.

- Use the built-in RStudio help interface to search for more
  information on R functions.

- Demonstrate how to provide sufficient information for
  troubleshooting with the R user community.


## What is R? What is RStudio?

The term [R](https://www.r-project.org/) is used to refer to both the
*programming language*, the *environment for statistical computing*
and *the software* that interprets the scripts written using it.

[RStudio](https://rstudio.com) is currently a very popular way to not
only write your R scripts but also to interact with the R
software[^plainr]. To function correctly, RStudio needs R and
therefore both need to be installed on your computer.

[^plainr]: As opposed to using R directly from the command line
    console. There exist other software that interface and integrate
    with R, but RStudio is particularly well suited for beginners and
    while providing numerous very advanced features.

The [RStudio IDE Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf)
provides much more information that will be covered here, but can be
useful to learn keyboard shortcuts and discover new features.

[Other RStudio Cheat sheets](https://www.rstudio.com/resources/cheatsheets/) are available for specific topics.

## Why learn R?

### R does not involve lots of pointing and clicking, and that's a good thing {-}

The learning curve might be steeper than with other software, but with
R, the results of your analysis do not rely on remembering a
succession of pointing and clicking, but instead on a series of
written commands, and that's a good thing! So, if you want to redo
your analysis because you collected more data, you don't have to
remember which button you clicked in which order to obtain your
results; you just have to run your script again.

Working with scripts makes the steps you used in your analysis clear,
and the code you write can be inspected by someone else who can give
you feedback and spot mistakes.

Working with scripts forces you to have a deeper understanding of what
you are doing, and facilitates your learning and comprehension of the
methods you use.

### R code is great for reproducibility {-}

Reproducibility is when someone else (including your future self) can
obtain the same results from the same dataset when using the same
analysis.

R integrates with other tools to generate manuscripts from your
code. If you collect more data, or fix a mistake in your dataset, the
figures and the statistical tests in your manuscript are updated
automatically.

An increasing number of journals and funding agencies expect analyses
to be reproducible, so knowing R will give you an edge with these
requirements.

We will learn more about reproducibility and reproducible research in
chapter \@ref(sec-rr).

### R is interdisciplinary and extensible {-}

With 10000+ packages[^whatarepkgs] that can be installed to extend its
capabilities, R provides a framework that allows you to combine
statistical approaches from many scientific disciplines to best suit
the analytical framework you need to analyze your data. For instance,
R has packages for image analysis, GIS, time series, population
genetics, and a lot more.

[^whatarepkgs]: i.e. add-ons that confer R with new functionality,
    such as bioinformatics data analysis - see chapter
    \@ref(sec-bioinfo)

```{r, fig.cap = "Exponential increase of the number of packages available on [CRAN](https://cran.r-project.org/), the Comprehensive R Archive Network. From the R Journal, Volume 10/2, December 2018.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/cran.png")
```

### R works on data of all shapes and sizes {-}

The skills you learn with R scale easily with the size of your
dataset. Whether your dataset has hundreds or millions of lines, it
won't make much difference to you.

R is designed for data analysis. It comes with special data structures
and data types that make handling of missing data and statistical
factors convenient.

R can connect to spreadsheets, databases, and many other data formats,
on your computer or on the web.


### R produces high-quality graphics {-}

The plotting functionalities in R are endless, and allow you to adjust
any aspect of your graph to convey most effectively the message from
your data.


### R has a large and welcoming community {-}

Thousands of people use R daily. Many of them are willing to help you
through mailing lists and websites such as [Stack
Overflow](https://stackoverflow.com/), or on the [RStudio
community](https://community.rstudio.com/). These broad user community
extends to specialised areas such as bioinformatics.


### Not only is R free, but it is also open-source and cross-platform {-}

Anyone can inspect the source code to see how R works. Because of this
transparency, there is less chance for mistakes, and if you (or
someone else) find some, you can report and fix bugs.

## Knowing your way around RStudio

Let's start by learning about [RStudio](https://www.rstudio.com/),
which is an Integrated Development Environment (IDE) for working with
R.

The RStudio IDE open-source product is free under the [Affero General
Public License (AGPL) v3](https://www.gnu.org/licenses/agpl-3.0.en.html).
The RStudio IDE is also available with a commercial license and
priority email support from RStudio, Inc.

We will use RStudio IDE to write code, navigate the files on our
computer, inspect the variables we are going to create, and visualize
the plots we will generate. RStudio can also be used for other things
(e.g., version control, developing packages, writing Shiny apps) that
we will not cover during the workshop.


```{r, results='markup', fig.cap="RStudio interface screenshot. Clockwise from top left: Source, Environment/History, Files/Plots/Packages/Help/Viewer, Console.", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("./figs/rstudio-screenshot.png")
```

RStudio is divided into 4 "Panes":

- the **Source** for your scripts and documents (top-left, in the
  default layout)
- your **Environment/History** (top-right),
- your **Files/Plots/Packages/Help/Viewer** (bottom-right), and
- the R **Console** (bottom-left).

The placement of these panes and their content can be customized (see
menu, `Tools -> Global Options -> Pane Layout`).

One of the advantages of using RStudio is that all the information you
need to write code is available in a single window. Additionally, with
many shortcuts, **autocompletion**, and **highlighting** for the major
file types you use while developing in R, RStudio will make typing
easier and less error-prone.

## Getting set up

It is good practice to keep a set of related data, analyses, and text
self-contained in a single folder, called the **working
directory**. All of the scripts within this folder can then use
**relative paths** to files that indicate where inside the project a
file is located (as opposed to absolute paths, which point to where a
file is on a specific computer). Working this way makes it a lot
easier to move your project around on your computer and share it with
others without worrying about whether or not the underlying scripts
will still work.

RStudio provides a helpful set of tools to do this through its "Projects"
interface, which not only creates a working directory for you, but also remembers
its location (allowing you to quickly navigate to it) and optionally preserves
custom settings and open files to make it easier to resume work after a
break. Go through the steps for creating an "R Project" for this
tutorial below.

1. Start RStudio.
2. Under the `File` menu, click on `New project`. Choose `New directory`, then
  `New project`.
3. Enter a name for this new folder (or "directory"), and choose a
   convenient location for it. This will be your **working directory**
   for this session (or whole course) (e.g., `wsbim1207`).
4. Click on `Create project`.
5. (Optional) Set Preferences to 'Never' save workspace in RStudio.

RStudio's default preferences generally work well, but saving a workspace to
.RData can be cumbersome, especially if you are working with larger datasets.
To turn that off, go to Tools --> 'Global Options' and select the 'Never' option
for 'Save workspace to .RData' on exit.'

```{r, results='markup', fig.cap="Set 'Save workspace to .RData on exit' to 'Never'", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/rstudio-preferences.png")
```

To avoid [character encoding issue between Windows and other operating
systems](https://yihui.name/en/2018/11/biggest-regret-knitr/), we are
going to set UTF-8 by default:


```{r, results='markup', fig.cap="Set the default text encoding to UTF-8 to save us headache in the coming future. (Figure from the link above).", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/utf8.png")
```

### Organizing your working directory

Using a consistent folder structure across your projects will help keep things
organized, and will also make it easy to find/file things in the future. This
can be especially helpful when you have multiple projects. In general, you may
create directories (folders) for **scripts**, **data**, and **documents**.

 - **`data/`** Use this folder to store your raw data and intermediate
   datasets you may create for the need of a particular analysis. For
   the sake of transparency and
   [provenance](https://en.wikipedia.org/wiki/Provenance), you should
   *always* keep a copy of your raw data accessible and do as much of
   your data cleanup and preprocessing programmatically (i.e., with
   scripts, rather than manually) as possible. Separating raw data
   from processed data is also a good idea. For example, you could
   have files `data/raw/data1.txt` and `...data2.txt` kept
   separate from a `data/processed/norm_data.csv` file generated by
   the `scripts/01_preprocess_all_data.R` script.
 - **`documents/`** This would be a place to keep outlines, drafts,
   and other text.
 - **`scripts/`** (or `src`) This would be the location to keep your R
   scripts for different analyses or plotting, and potentially a
   separate folder for your functions (more on that later).

You may want additional directories or subdirectories depending on
your project needs, but these should form the backbone of your working
directory.


```{r, results='markup', fig.cap="Example of a working directory structure.", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("./figs/working-directory-structure.png")
```

For this course, we will need a `data/` folder to store our raw data,
and we will use `data_output/` for when we learn how to export data as
CSV files, and `fig_output/` folder for the figures that we will save.

`r msmbstyle::question_begin()`

Under the `Files` tab on the right of the screen, click on `New Folder` and
create a folder named `data` within your newly created working directory
(e.g., `~/wsbim1207/data`). (Alternatively, type `dir.create("data")` at
your R console.) Repeat these operations to create a `data_output/` and a
`fig_output` folders.

`r msmbstyle::question_end()`

We are going to keep the script in the root of our working directory
because we are only going to use one file and it will make things
easier.

Your working directory should now look like this:

```{r, results='markup',fig.cap="How it should look like at the beginning of this lesson", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("./figs/r-starting-how-it-should-look-like.png")
```

**Project management** is also applicable to bioinformatics projects,
of course[^bioindatascience]. William Noble (@Noble:2009) proposes the
following directory structure:

[^bioindatascience]: In this course, we consider bioinformatics as
    data science applied to biological or bio-medical data.

> Directory names are in large typeface, and filenames are in smaller
> typeface. Only a subset of the files are shown here. Note that the
> dates are formatted `<year>-<month>-<day>` so that they can be
> sorted in chronological order. The source code `src/ms-analysis.c`
> is compiled to create `bin/ms-analysis` and is documented in
> `doc/ms-analysis.html`. The `README` files in the data directories
> specify who downloaded the data files from what URL on what
> date. The driver script `results/2009-01-15/runall` automatically
> generates the three subdirectories split1, split2, and split3,
> corresponding to three cross-validation splits. The
> `bin/parse-sqt.py` script is called by both of the `runall` driver
> scripts.


```{r bioinfoproj, fig.cap="Directory structure for a sample bioinformatics project.", out.width='100%', echo=FALSE}
knitr::include_graphics("./figs/noble-bioinfo-project.png")
```

The most important aspect of a well defined and well documented
project directory is to enable someone unfamiliar with the
project[^futureself] to

1. understand what the project is about, what data are available, what
   analyses were run, and what results were produced and, most
   importantly to

2. repeat the analysis over again - with new data, or changing some
   analysis parameters.

[^futureself]: That someone could be, and very likely will be your
    future self, a couple of months or years after the analyses were
    run.



### The working directory

The working directory is an important concept to understand. It is the
place from where R will be looking for and saving the files. When you
write code for your project, it should refer to files in relation to
the root of your working directory and only need files within this
structure with relative paths.

Using RStudio projects makes this easy and ensures that your working
directory is set properly. If you need to check it, you can use
`getwd()`. If for some reason your working directory is not what it
should be, you can change it in the RStudio interface by navigating in
the file browser where your working directory should be, and clicking
on the blue gear icon `More`, and select `Set As Working Directory`.
Alternatively you can use `setwd("/path/to/working/directory")` to
reset your working directory. However, your scripts should not include
this line because it will fail on someone else's computer.

**Example**

The schema below represents the working directory `wsbim1207` with the
`data` and `fig_output` sub-directories, and 2 files in the latter:

```
wsbim1207/data/
         /fig_output/fig1.pdf
         /fig_output/fig2.png
```

If we were in the working directory (`bioinfo_training_intro`), we could refer to the `fig1.pdf`
file using the relative path `fig_output/fig1.pdf` or the
absolute path `/home/user/wsbim1207/fig_output/fig1.pdf`.

If we were in the `data` directory, we would use the relative path
`../fig_output/fig1.pdf` or the same absolute path
`/home/user/wsbim1207/fig_output/fig1.pdf`.

## Interacting with R

The basis of programming is that we write down instructions for the
computer to follow, and then we tell the computer to follow those
instructions. We write, or *code*, instructions in R because it is a
common language that both the computer and we can understand. We call
the instructions *commands* and we tell the computer to follow the
instructions by *executing* (also called *running*) those commands.

There are two main ways of interacting with R: by using the
**console** or by using **scripts** (plain text files that contain
your code). The console pane (in RStudio, the bottom left panel) is
the place where commands written in the R language can be typed and
executed immediately by the computer. It is also where the results
will be shown for commands that have been executed. You can type
commands directly into the console and press `Enter` to execute those
commands, but they will be forgotten when you close the session.

Because we want our code and workflow to be reproducible, it is better
to type the commands we want in the script editor, and save the
script. This way, there is a complete record of what we did, and
anyone (including our future selves!)  can easily replicate the
results on their computer.

RStudio allows you to execute commands directly from the script editor
by using the `Ctrl` + `Enter` shortcut (on Macs, `Cmd` + `Return` will
work, too). The command on the current line in the script (indicated
by the cursor) or all of the commands in the currently selected text
will be sent to the console and executed when you press `Ctrl` +
`Enter`. You can find other keyboard shortcuts in this [RStudio
cheatsheet about the RStudio
IDE](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf).

At some point in your analysis you may want to check the content of a
variable or the structure of an object, without necessarily keeping a
record of it in your script. You can type these commands and execute
them directly in the console.  RStudio provides the `Ctrl` + `1` and
`Ctrl` + `2` shortcuts allow you to jump between the script and the
console panes.

If R is ready to accept commands, the R console shows a `>` prompt. If
it receives a command (by typing, copy-pasting or sent from the script
editor using `Ctrl` + `Enter`), R will try to execute it, and when
ready, will show the results and come back with a new `>` prompt to
wait for new commands.

If R is still waiting for you to enter more data because it isn't
complete yet, the console will show a `+` prompt. It means that you
haven't finished entering a complete command. This is because you have
not 'closed' a parenthesis or quotation, i.e. you don't have the same
number of left-parentheses as right-parentheses, or the same number of
opening and closing quotation marks.  When this happens, and you
thought you finished typing your command, click inside the console
window and press `Esc`; this will cancel the incomplete command and
return you to the `>` prompt.


## How to learn more during and after the course?

The material we cover during this course will give you an initial
taste of how you can use R to analyse data for your own
research. However, you will need to learn more to do advanced
operations such as cleaning your dataset, using statistical methods,
or creating beautiful graphics[^inthiscoure]. The best way to become
proficient and efficient at R, as with any other tool, is to use it to
address your actual research questions. As a beginner, it can feel
daunting to have to write a script from scratch, and given that many
people make their code available online, modifying existing code to
suit your purpose might make it easier for you to get started.

[^inthiscoure]: We will introduce most of these (except statistics)
    here, but will only manage to scratch the surface of the wealth of
    what is possible to do with R.

```{r kitten, results='markup', echo=FALSE, purl=FALSE, out.width='400px', fig.align='center'}
knitr::include_graphics("./figs/kitten-try-things.jpg")
```

## Seeking help

### Use the built-in RStudio help interface to search for more information on R functions {-}

```{r rstudiohelp, fig.cap="RStudio help interface.", results='markup', echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/rstudiohelp.png")
```

One of the fastest ways to get help, is to use the RStudio help
interface. This panel by default can be found at the lower right hand
panel of RStudio. As seen in the screenshot, by typing the word
"Mean", RStudio tries to also give a number of suggestions that you
might be interested in. The description is then shown in the display
window.

### I know the name of the function I want to use, but I'm not sure how to use it {-}

If you need help with a specific function, let's say `barplot()`, you
can type:

```{r, eval=FALSE, purl=FALSE}
?barplot
```

If you just need to remind yourself of the names of the arguments, you can use:

```{r, eval=FALSE, purl=FALSE}
args(lm)
```

### I want to use a function that does X, there must be a function for it but I don't know which one... {-}

If you are looking for a function to do a particular task, you can use the
`help.search()` function, which is called by the double question mark `??`.
However, this only looks through the installed packages for help pages with a
match to your search request

```{r, eval=FALSE, purl=FALSE}
??kruskal
```

If you can't find what you are looking for, you can use
the [rdocumentation.org](http://www.rdocumentation.org) website that searches
through the help files across all packages available.

Finally, a generic Google or internet search "R \<task\>" will often either send
you to the appropriate package documentation or a helpful forum where someone
else has already asked your question.

### I am stuck... I get an error message that I don't understand {-}

Start by googling the error message. However, this doesn't always work very well
because often, package developers rely on the error catching provided by R. You
end up with general error messages that might not be very helpful to diagnose a
problem (e.g. "subscript out of bounds"). If the message is very generic, you
might also include the name of the function or package you're using in your
query.

However, you should check Stack Overflow. Search using the `[r]` tag. Most
questions have already been answered, but the challenge is to use the right
words in the search to find the
answers:

[http://stackoverflow.com/questions/tagged/r](http://stackoverflow.com/questions/tagged/r)

The [Introduction to R](http://cran.r-project.org/doc/manuals/R-intro.pdf) can
also be dense for people with little programming experience but it is a good
place to understand the underpinnings of the R language.

The [R FAQ](http://cran.r-project.org/doc/FAQ/R-FAQ.html) is dense and technical
but it is full of useful information.

### Asking for help {-}

The key to receiving help from someone is for them to rapidly grasp
your problem. You should make it as easy as possible to pinpoint where
the issue might be.

Try to use the correct words to describe your problem. For instance, a
package is not the same thing as a library. Most people will
understand what you meant, but others have really strong feelings
about the difference in meaning. The key point is that it can make
things confusing for people trying to help you. Be as precise as
possible when describing your problem.

If possible, try to reduce what doesn't work to a simple *reproducible
example*. If you can reproduce the problem using a very small data
frame instead of your 50000 rows and 10000 columns one, provide the
small one with the description of your problem. When appropriate, try
to generalize what you are doing so even people who are not in your
field can understand the question. For instance instead of using a
subset of your real dataset, create a small (3 columns, 5 rows)
generic one. For more information on how to write a reproducible
example see [this article by Hadley
Wickham](http://adv-r.had.co.nz/Reproducibility.html).

To share an object with someone else, if it's relatively small, you
can use the function `dput()`. It will output R code that can be used
to recreate the exact same object as the one in memory:

```{r, results='show', purl=FALSE}
## iris is an example data frame that comes with R and head() is a
## function that returns the first part of the data frame
dput(head(iris))
```

If the object is larger, provide either the raw file (i.e., your CSV
file) with your script up to the point of the error (and after
removing everything that is not relevant to your
issue). Alternatively, in particular if your question is not related
to a data frame, you can save any R object to a file[^export]:

```{r, eval=FALSE, purl=FALSE}
saveRDS(iris, file="/tmp/iris.rds")
```

[^export]: See section \@ref(sec-exportandsave) for a better
    introduction about exporting and saving data.

The content of this file is however not human readable and cannot be
posted directly on Stack Overflow. Instead, it can be sent to someone
by email who can read it with the `readRDS()` command (here it is
assumed that the downloaded file is in a `Downloads` folder in the
user's home directory):

```{r, eval=FALSE, purl=FALSE}
some_data <- readRDS(file="~/Downloads/iris.rds")
```

Last, but certainly not least, **always include the output of `sessionInfo()`**
as it provides critical information about your platform, the versions of R and
the packages that you are using, and other information that can be very helpful
to understand your problem.

```{r, results='show', purl=FALSE}
sessionInfo()
```

### Where to ask for help? {-}

* The person sitting next to you during the course. Don't hesitate to
  talk to your neighbour during the workshop, compare your answers,
  and ask for help.
* Your friendly colleagues: if you know someone with more experience
  than you, they might be able and willing to help you.
* [Stack Overflow](http://stackoverflow.com/questions/tagged/r): if
  your question hasn't been answered before and is well crafted,
  chances are you will get an answer in less than 5 min. Remember to
  follow their guidelines on [how to ask a good
  question](http://stackoverflow.com/help/how-to-ask).
* The [R-help mailing
  list](https://stat.ethz.ch/mailman/listinfo/r-help): it is read by a
  lot of people (including most of the R core team), a lot of people
  post to it, but the tone can be pretty dry, and it is not always
  very welcoming to new users. If your question is valid, you are
  likely to get an answer very fast but don't expect that it will come
  with smiley faces. Also, here more than anywhere else, be sure to
  use correct vocabulary (otherwise you might get an answer pointing
  to the misuse of your words rather than answering your
  question). You will also have more success if your question is about
  a base function rather than a specific package.
* If your question is about a specific package, see if there is a
  mailing list for it. Usually it's included in the DESCRIPTION file
  of the package that can be accessed using
  `packageDescription("name-of-package")`. You may also want to try to
  email the author of the package directly, or open an issue on the
  code repository (e.g., GitHub).
* There are also some topic-specific mailing lists (GIS,
  phylogenetics, etc...), the complete list is
  [here](http://www.r-project.org/mail.html).

### More resources {-}

- The [Posting Guide](http://www.r-project.org/posting-guide.html) for
  the R mailing lists.

- [How to ask for R
  help](http://blog.revolutionanalytics.com/2014/01/how-to-ask-for-r-help.html)
  useful guidelines

- [This blog post by Jon
  Skeet](http://codeblog.jonskeet.uk/2010/08/29/writing-the-perfect-question/)
  has quite comprehensive advice on how to ask programming questions.

- The [reprex](https://cran.rstudio.com/web/packages/reprex/) package
  is very helpful to create reproducible examples when asking for
  help. The rOpenSci community call "How to ask questions so they get
  answered" ([Github
  link](https://github.com/ropensci/commcalls/issues/14) and [video
  recording](https://vimeo.com/208749032)) includes a presentation of
  the reprex package and of its philosophy.

## R packages

R packages need to be : (1) installed (+ their dependencies), and (2) loaded in your environment

### Loading packages

As we have seen above, R packages play a fundamental role in R. The
make use of a package's functionality, assuming it is installed, we
first need to load it to be able to use it. This is done with the
`library()` function. Below, we load `ggplot2`.

```{r loadp, eval=FALSE, purl=FALSE}
library("ggplot2")
```

### Installing packages

The default package repository is The *Comprehensive R Archive
Network* (CRAN), and any package that is available on CRAN can be
installed with the `install.packages()` function. Below, for example,
we install the `dplyr` package that we will learn about late.

```{r craninstall, eval=FALSE, purl=FALSE}
install.packages("dplyr")
```

This command will install the `dplyr` package as well as all its
dependencies, i.e. all the packages that it relies on to function.

[Github](https://github.com/) is a general-purpose online software
project repository and is well suited for R package development. To
install a package from Gtihub, one can use the `install_github()`
function from the `devtools` package. Below we first install the
latter from CRAN (as show above), then we install `rWSBIM1207`
directly from the user `UCLouvain-CBIO` github repository.

```{r ghinstall, eval=FALSE, purl=FALSE}
install.packages("devtools")
library("devtools")
install_github("UCLouvain-CBIO/rWSBIM1207")
```

In section \@ref(sec-bioconductor), we will see how to install
[Bioconductor](http://www.bioconductor.org), a project dedicated to
bioinformatics and omics packages.

<!--chapter:end:20-r-rstudio.Rmd-->

# Introduction to R {#sec-startr}

**Learning Objectives**

- Define the following terms as they relate to R: object, assign,
  call, function, arguments, options.
- Assign values to objects in R.
- Learn how to _name_ objects
- Use comments to inform script.
- Solve simple arithmetic operations in R.
- Call functions and use arguments to change their default options.
- Inspect the content of vectors and manipulate their content.
- Subset and extract values from vectors.
- Analyze vectors with missing data.

## Creating objects in R

You can get output from R simply by typing math in the console:

```{r, purl=FALSE}
3 + 5
12 / 7
```

However, to do useful and interesting things, we need to assign _values_ to
_objects_. To create an object, we need to give it a name followed by the
assignment operator `<-`, and the value we want to give it:

```{r, purl=FALSE}
weight_kg <- 55
```

`<-` is the assignment operator. It assigns values on the right to
objects on the left. So, after executing `x <- 3`, the value of `x` is
`3`. The arrow can be read as 3 **goes into** `x`.  For historical
reasons, you can also use `=` for assignments, but not in every
context. Because of the
[slight](http://blog.revolutionanalytics.com/2008/12/use-equals-or-arrow-for-assignment.html)
[differences](http://r.789695.n4.nabble.com/Is-there-any-difference-between-and-tp878594p878598.html)
in syntax, it is good practice to always use `<-` for assignments.

In RStudio, typing <kbd>Alt</kbd> + <kbd>-</kbd> (push <kbd>Alt</kbd>
at the same time as the <kbd>-</kbd> key) will write ` <- ` in a
single keystroke in a PC, while typing <kbd>Option</kbd> +
<kbd>-</kbd> (push <kbd>Option</kbd> at the same time as the
<kbd>-</kbd> key) does the same in a Mac.

### Naming variables {-}

Objects can be given any name such as `x`, `current_temperature`, or
`subject_id`. You want your object names to be explicit and not too
long. They cannot start with a number (`2x` is not valid, but `x2`
is). R is case sensitive (e.g., `weight_kg` is different from
`Weight_kg`). There are some names that cannot be used because they
are the names of fundamental functions in R (e.g., `if`, `else`,
`for`, see
[here](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Reserved.html)
for a complete list). In general, even if it's allowed, it's best to
not use other function names (e.g., `c`, `T`, `mean`, `data`, `df`,
`weights`). If in doubt, check the help to see if the name is already
in use. It's also best to avoid dots (`.`) within an object name as in
`my.dataset`. There are many functions in R with dots in their names
for historical reasons, but because dots have a special meaning in R
(for methods) and other programming languages, it's best to avoid
them. It is also recommended to use nouns for object names, and verbs
for function names. It's important to be consistent in the styling of
your code (where you put spaces, how you name objects, etc.). Using a
consistent coding style makes your code clearer to read for your
future self and your collaborators. In R, some popular style guides
are [Google's](https://google.github.io/styleguide/Rguide.xml), the
[tidyverse's](http://style.tidyverse.org/) style and the [Bioconductor
style
guide](https://bioconductor.org/developers/how-to/coding-style/). The
tidyverse's is very comprehensive and may seem overwhelming at
first. You can install the
[**`lintr`**](https://github.com/jimhester/lintr) package to
automatically check for issues in the styling of your code.

> **Objects vs. variables** What are known as `objects` in `R` are
> known as `variables` in many other programming languages. Depending
> on the context, `object` and `variable` can have drastically
> different meanings. However, in this lesson, the two words are used
> synonymously. For more information see:
> https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Objects


When assigning a value to an object, R does not print anything. You
can force R to print the value by using parentheses or by typing the
object name:

```{r, purl=FALSE}
weight_kg <- 55    # doesn't print anything
(weight_kg <- 55)  # but putting parenthesis around the call prints the value of `weight_kg`
weight_kg          # and so does typing the name of the object
```

Now that R has `weight_kg` in memory, we can do arithmetic with it. For
instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):

```{r, purl=FALSE}
2.2 * weight_kg
```

We can also change an object's value by assigning it a new one:

```{r, purl=FALSE}
weight_kg <- 57.5
2.2 * weight_kg
```

This means that assigning a value to one object does not change the values of
other objects. For example, let's store the animal's weight in pounds in a new
object, `weight_lb`:

```{r, purl=FALSE}
weight_lb <- 2.2 * weight_kg
```

and then change `weight_kg` to 100.

```{r}
weight_kg <- 100
```

`r msmbstyle::question_begin()`

What do you think is the current content of the object `weight_lb`?
126.5 or 220?

`r msmbstyle::question_end()`

## Comments

The comment character in R is `#`, anything to the right of a `#` in a
script will be ignored by R. It is useful to leave notes, and
explanations in your scripts.

RStudio makes it easy to comment or uncomment a paragraph: after
selecting the lines you want to comment, press at the same time on
your keyboard <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>C</kbd>. If
you only want to comment out one line, you can put the cursor at any
location of that line (i.e. no need to select the whole line), then
press <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>C</kbd>.

`r msmbstyle::question_begin()`
What are the values after each statement in the following?

```{r, purl=FALSE}
mass <- 47.5            # mass?
age  <- 122             # age?
mass <- mass * 2.0      # mass?
age  <- age - 20        # age?
mass_index <- mass/age  # mass_index?
```
`r msmbstyle::question_end()`

## Functions and their arguments

Functions are "canned scripts" that automate more complicated sets of commands
including operations assignments, etc. Many functions are predefined, or can be
made available by importing R *packages* (more on that later). A function
usually gets one or more inputs called *arguments*. Functions often (but not
always) return a *value*. A typical example would be the function `sqrt()`. The
input (the argument) must be a number, and the return value (in fact, the
output) is the square root of that number. Executing a function ('running it')
is called *calling* the function. An example of a function call is:

```{r, eval=FALSE, purl=FALSE}
b <- sqrt(a)
```

Here, the value of `a` is given to the `sqrt()` function, the `sqrt()` function
calculates the square root, and returns the value which is then assigned to
the object `b`. This function is very simple, because it takes just one argument.

The return 'value' of a function need not be numerical (like that of `sqrt()`),
and it also does not need to be a single item: it can be a set of things, or
even a dataset. We'll see that when we read data files into R.

Arguments can be anything, not only numbers or filenames, but also other
objects. Exactly what each argument means differs per function, and must be
looked up in the documentation (see below). Some functions take arguments which
may either be specified by the user, or, if left out, take on a *default* value:
these are called *options*. Options are typically used to alter the way the
function operates, such as whether it ignores 'bad values', or what symbol to
use in a plot.  However, if you want something specific, you can specify a value
of your choice which will be used instead of the default.

Let's try a function that can take multiple arguments: `round()`.

```{r, results='show', purl=FALSE}
round(3.14159)
```

Here, we've called `round()` with just one argument, `3.14159`, and it has
returned the value `3`.  That's because the default is to round to the nearest
whole number. If we want more digits we can see how to do that by getting
information about the `round` function.  We can use `args(round)` or look at the
help for this function using `?round`.

```{r, results='show', purl=FALSE}
args(round)
```

```{r, eval=FALSE, purl=FALSE}
?round
```

We see that if we want a different number of digits, we can
type `digits=2` or however many we want.

```{r, results='show', purl=FALSE}
round(3.14159, digits = 2)
```

If you provide the arguments in the exact same order as they are defined you
don't have to name them:

```{r, results='show', purl=FALSE}
round(3.14159, 2)
```

And if you do name the arguments, you can switch their order:

```{r, results='show', purl=FALSE}
round(digits = 2, x = 3.14159)
```

It's good practice to put the non-optional arguments (like the number you're
rounding) first in your function call, and to specify the names of all optional
arguments.  If you don't, someone reading your code might have to look up the
definition of a function with unfamiliar arguments to understand what you're
doing.


## Vectors and data types


A vector is the most common and basic data type in R, and is pretty much
the workhorse of R. A vector is composed by a series of values, which can be
either numbers or characters. We can assign a series of values to a vector using
the `c()` function. For example we can create a vector of animal weights and assign
it to a new object `weight_g`:

```{r, purl=FALSE}
weight_g <- c(50, 60, 65, 82)
weight_g
```

A vector can also contain characters:

```{r, purl=FALSE}
molecules <- c("dna", "rna", "protein")
molecules
```

The quotes around "dna", "rna", etc. are essential here. Without the
quotes R will assume there are objects called `dna`, `rna` and
`protein`. As these objects don't exist in R's memory, there will be
an error message.

There are many functions that allow you to inspect the content of a
vector. `length()` tells you how many elements are in a particular vector:

```{r, purl=FALSE}
length(weight_g)
length(molecules)
```

An important feature of a vector, is that all of the elements are the
same type of data.  The function `class()` indicates the class (the
type of element) of an object:

```{r, purl=FALSE}
class(weight_g)
class(molecules)
```

The function `str()` provides an overview of the structure of an
object and its elements. It is a useful function when working with
large and complex objects:

```{r, purl=FALSE}
str(weight_g)
str(molecules)
```

You can use the `c()` function to add other elements to your vector:

```{r}
weight_g <- c(weight_g, 90) # add to the end of the vector
weight_g <- c(30, weight_g) # add to the beginning of the vector
weight_g
```

In the first line, we take the original vector `weight_g`, add the
value `90` to the end of it, and save the result back into
`weight_g`. Then we add the value `30` to the beginning, again saving
the result back into `weight_g`.

We can do this over and over again to grow a vector, or assemble a
dataset.  As we program, this may be useful to add results that we are
collecting or calculating.

An **atomic vector** is the simplest R **data type** and is a linear
vector of a single type. Above, we saw 2 of the 6 main **atomic
vector** types that R uses: `"character"` and `"numeric"` (or
`"double"`). These are the basic building blocks that all R objects
are built from. The other 4 **atomic vector** types are:

- `"logical"` for `TRUE` and `FALSE` (the boolean data type)
- `"integer"` for integer numbers (e.g., `2L`, the `L` indicates to R
  that it's an integer)
- `"complex"` to represent complex numbers with real and imaginary
  parts (e.g., `1 + 4i`) and that's all we're going to say about them
- `"raw"` for bitstreams that we won't discuss further

You can check the type of your vector using the `typeof()` function
and inputting your vector as the argument.

Vectors are one of the many **data structures** that R uses. Other
important ones are lists (`list`), matrices (`matrix`), data frames
(`data.frame`), factors (`factor`) and arrays (`array`).


`r msmbstyle::question(text = "We've seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector?")`

`r msmbstyle::solution(text = "R implicitly converts them to all be the same type")`

`r msmbstyle::question_begin()`

What will happen in each of these examples? (hint: use `class()` to
check the data type of your objects):

```{r, eval=TRUE}
num_char <- c(1, 2, 3, "a")
num_logical <- c(1, 2, 3, TRUE)
char_logical <- c("a", "b", "c", TRUE)
tricky <- c(1, 2, 3, "4")
```

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
class(num_char)
class(num_logical)
class(char_logical)
class(tricky)
```
`r msmbstyle::solution_end()`


`r msmbstyle::question(text = "Why do you think it happens?")`

`r msmbstyle::solution(text = "Vectors can be of only one data type. R tries to convert (coerce) the content of this vector to find a *common denominator* that doesn't lose any information.")`


`r msmbstyle::question_begin()`

How many values in `combined_logical` are `"TRUE"` (as a character) in
the following example:


```{r, eval=TRUE}
num_logical <- c(1, 2, 3, TRUE)
char_logical <- c("a", "b", "c", TRUE)
combined_logical <- c(num_logical, char_logical)
```

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

Only one. There is no memory of past data types, and the coercion
happens the first time the vector is evaluated. Therefore, the `TRUE`
in `num_logical` gets converted into a `1` before it gets converted
into `"1"` in `combined_logical`.

```{r}
combined_logical
```

`r msmbstyle::solution_end()`


`r msmbstyle::question(text = " In R, we call converting objects from one class into another class _coercion_. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced?")`

`r msmbstyle::solution(text = "logical &#8594; numeric &#8594; character &#8592; logical")`



```{r, echo=FALSE, eval=FALSE, purl=TRUE}
## We’ve seen that atomic vectors can be of type character, numeric, integer, and
## logical. But what happens if we try to mix these types in a single
## vector?

## What will happen in each of these examples? (hint: use `class()` to
## check the data type of your object)
num_char <- c(1, 2, 3, "a")

num_logical <- c(1, 2, 3, TRUE)

char_logical <- c("a", "b", "c", TRUE)

tricky <- c(1, 2, 3, "4")

## Why do you think it happens?

## You've probably noticed that objects of different types get
## converted into a single, shared type within a vector. In R, we call
## converting objects from one class into another class
## _coercion_. These conversions happen according to a hierarchy,
## whereby some types get preferentially coerced into other types. Can
## you draw a diagram that represents the hierarchy of how these data
## types are coerced?
```

## Subsetting vectors

If we want to extract one or several values from a vector, we must
provide one or several indices in square brackets. For instance:

```{r, results='show', purl=FALSE}
molecules <- c("dna", "rna", "peptide", "protein")
molecules[2]
molecules[c(3, 2)]
```

We can also repeat the indices to create an object with more elements
than the original one:

```{r, results='show', purl=FALSE}
more_molecules <- molecules[c(1, 2, 3, 2, 1, 4)]
more_molecules
```

Note: R indices start at 1. Programming languages like Fortran, MATLAB,
Julia, and R start counting at 1, because that's what human beings
typically do. Languages in the C family (including C++, Java, Perl,
and Python) count from 0 because that's simpler for computers to do.

Finally, it is also possible to get all the elements of a vector
except some specified elements using negative indices:

```{r}
molecules ## all molecules
molecules[-1] ## all but the first one
molecules[-c(1, 3)] ## all but 1st/3rd ones
molecules[c(-1, -3)] ## all but 1st/3rd ones
```

`r msmbstyle::question_begin()`

Here is another example of a character vector called `fruits`:
```{r}
fruits <- c("apple", "orange", "grape")
```

* add the elements *melon* and *pineapple* to this vector
* sort them in alphabetic order 
    + manually by using their index position, 
    + and by using `sort()` (see `?sort`).  

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
# add the elements *melon* and *pineapple*
fruits <- c(fruits, "melon", "pineapple")
# sorting based on the index position
fruits[c(1,3, 4,2,5)]
# sorting based on sort()
sort(fruits)
```


`r msmbstyle::solution_end()`


## Conditional subsetting

Another common way of subsetting is by using a logical vector. `TRUE` will
select the element with the same index, while `FALSE` will not:

```{r, purl = FALSE}
weight_g <- c(21, 34, 39, 54, 55)
weight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]
```

Typically, these logical vectors are not typed by hand, but are the
output of other functions or logical tests. For instance, if you
wanted to select only the values above 50:

```{r, purl = FALSE}
## will return logicals with TRUE for the indices that meet
## the condition
weight_g > 50
## so we can use this to select only the values above 50
weight_g[weight_g > 50]
```

You can combine multiple tests using `&` (both conditions are true,
AND) or `|` (at least one of the conditions is true, OR):

```{r, results='show', purl=FALSE}
weight_g[weight_g < 30 | weight_g > 50]
weight_g[weight_g >= 30 & weight_g == 21]
```

Here, `<` stands for "less than", `>` for "greater than", `>=` for
"greater than or equal to", and `==` for "equal to". The double equal
sign `==` is a test for numerical equality between the left and right
hand sides, and should not be confused with the single `=` sign, which
performs variable assignment (similar to `<-`).

A common task is to search for certain strings in a vector.  One could
use the "or" operator `|` to test for equality to multiple values, but
this can quickly become tedious. The function `%in%` allows you to
test if any of the elements of a search vector are found:

```{r, purl = FALSE}
molecules <- c("dan", "rna", "protein", "peptide")
molecules[molecules == "rna" | molecules == "dna"] # returns both rna and dna
molecules %in% c("rna", "dna", "metabolite", "peptide", "glycerol")
molecules[molecules %in% c("rna", "dna", "metabolite", "peptide", "glycerol")]
```

`r msmbstyle::question_begin()`
Based on the `height` vector below, select heights that are above 190 or below or equal to 170
```{r}
height <- c(163, 189, 210, 177, 168, 192, 170)
```
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
height[height>190 | height <= 170]
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`
Based on the `fruits` vector below:

* subset the vector to only have melon and apple
* test that orange is included in this vector and mango is not

```{r}
fruits <- c("apple", "orange", "grape", "melon", "pineapple",
            "banana", "grape", "orange", "melon")
```
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
# subset the vector to only have melon and apple
fruits[fruits == "melon" | fruits == "apple"]
# test that orange is included in this vector and mango is not
"orange" %in% fruits
"mango" %in% fruits
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`
Can you figure out why `"four" > "five"` returns `TRUE`?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
"four" > "five"
```

When using `>` or `<` on strings, R compares their alphabetical order.
Here `"four"` comes after `"five"`, and therefore is *greater than*
it.

`r msmbstyle::solution_end()`

## Names

It is possible to name each element of a vector. The code chunk below
show a initial vector without any names, how names are set, and
retrieved.

```{r}
x <- c(1, 5, 3, 5, 10)
names(x) ## no names
names(x) <- c("A", "B", "C", "D", "E")
names(x) ## now we have names
```

When a vector has names, it is possible to access elements by their
name, in addition to their index.

```{r}
x[c(1, 3)]
x[c("A", "C")]
```

## Missing data

As R was designed to analyze datasets, it includes the concept of
missing data (which is uncommon in other programming
languages). Missing data are represented in vectors as `NA`.

When doing operations on numbers, most functions will return `NA` if
the data you are working with include missing values. This feature
makes it harder to overlook the cases where you are dealing with
missing data.  You can add the argument `na.rm = TRUE` to calculate
the result while ignoring the missing values.

```{r}
heights <- c(2, 4, 4, NA, 6)
mean(heights)
max(heights)
mean(heights, na.rm = TRUE)
max(heights, na.rm = TRUE)
```

If your data include missing values, you may want to become familiar
with the functions `is.na()`, `na.omit()`, and `complete.cases()`. See
below for examples.


```{r}
## Extract those elements which are not missing values.
heights[!is.na(heights)]

## Returns the object with incomplete cases removed.
## The returned object is an atomic vector of type `"numeric"`
## (or `"double"`).
na.omit(heights)

## Extract those elements which are complete cases.
## The returned object is an atomic vector of type `"numeric"`
## (or `"double"`).
heights[complete.cases(heights)]
```


`r msmbstyle::question_begin()`

1. Using this vector of heights in inches, create a new vector with the NAs removed.
```{r}
heights <- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 
             63, NA, 72, 65, 64, 70, 63, 65)
```
2. Use the function `median()` to calculate the median of the `heights` vector.
3. Use R to figure out how many people in the set are taller than 67 inches.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
heights_no_na <- heights[!is.na(heights)]
## or
heights_no_na <- na.omit(heights)
```

```{r}
median(heights, na.rm = TRUE)
```

```{r}
heights_above_67 <- heights_no_na[heights_no_na > 67]
length(heights_above_67)
```
`r msmbstyle::solution_end()`


## Generating vectors {#sec-genvec}

```{r, echo = FALSE}
set.seed(1)
```

### Constructors {-}

There exists some functions to generate vectors of different type. To
generate a vector of numerics, one can use the `numeric()`
constructor, providing the length of the output vector as
parameter. The values will be initialised with 0.

```{r}
numeric(3)
numeric(10)
```

Note that if we ask for a vector of numerics of length 0, we obtain
exactly that:

```{r}
numeric(0)
```

There are similar constructors for characters and logicals, named
`character()` and `logical()` respectively.

`r msmbstyle::question_begin()`
What are the defaults for character and logical vectors?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
character(2) ## the empty charater
logical(2)   ## FALSE
```
`r msmbstyle::solution_end()`

### Replicate elements {-}

The `rep` function allow to repeat a value a certain number of
times. If we want to initiate a vector of numerics of length 5 with
the value -1, for example, we could do the following:

```{r}
rep(-1, 5)
```

Similarly, to generate a vector populated with missing values, which
is often a good way to start, without setting assumptions on the data
to be collected:

```{r}
rep(NA, 5)
```

`rep` can take vectors of any length as input (above, we used vectors
of length 1) and any type. For example, if we want to repeat the
values 1, 2 and 3 five times, we would do the following:

```{r}
rep(c(1, 2, 3), 5)
```

`r msmbstyle::question_begin()`
What if we wanted to repeat the values 1, 2 and 3 five times, but
obtain five 1s, five 2s and five 3s in that order? There are two
possibilities - see `?rep` or `?sort` for help.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
rep(c(1, 2, 3), each = 5)
sort(rep(c(1, 2, 3), 5))
```
`r msmbstyle::solution_end()`

### Sequence generation {-}

Another very useful function is `seq`, to generate a sequence of
numbers. For example, to generate a sequence of integers from 1 to 20
by steps of 2, one would use:

```{r}
seq(from = 1, to = 20, by = 2)
```

The default value of `by` is 1 and, given that the generate of a
sequence of one value to another with steps of 1 is frequently used,
there's a shortcut:

```{r}
seq(1, 5, 1)
seq(1, 5) ## default by
1:5
```

To generate a sequence of numbers from 1 to 20 of final length of 3,
one would use:

```{r}
seq(from = 1, to = 20, length.out = 3)
```

### Random samples and permutations {-}

A last group of useful functions are those that generate random
data. The first one, `sample`, generates a random permutation of
another vector. For example, to draw a random order to 10 students
oral example, I first assign each student a number from 1 to then (for
instance based on the alphabetic order of their name) and then:

```{r}
sample(1:10)
```

Without further arguments, `sample` will return a permutation of all
elements of the vector. If I want a random sample of a certain size, I
would set this value as second argument. Below, I sample 5 random
letters from the alphabet contained in the pre-defined `letters` vector:

```{r}
sample(letters, 5)
```

If I wanted an output larger than the input vector, or being able to
draw some elements multiple times, I would need to set the `replace`
argument to `TRUE`:

```{r}
sample(1:5, 10, replace = TRUE)
```

`r msmbstyle::question_begin()`

When trying the functions above out, you will have realised that the
samples are indeed random and that one doesn't get the same
permutation twice. To be able to reproduce these random draws, one can
set the random number generation seed manually with `set.seed()`
before drawing the random sample.

- Test this feature with your neighbour. First draw two random
  permutations of `1:10` independently and observe that you get
  different results.

- Now set the seed with, for example, `set.seed(123)` and repeat the
  random draw. Observe that you now get the same random draws.

- Repeat by setting a different seed.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Different permutations

```{r}
sample(1:10)
sample(1:10)
```

Same permutations with seed 123

```{r}
set.seed(123)
sample(1:10)
set.seed(123)
sample(1:10)
```

A different seed

```{r}
set.seed(1)
sample(1:10)
set.seed(1)
sample(1:10)
```

`r msmbstyle::solution_end()`

### Drawing samples from a normal distribution {-}

The last function we are going to see is `rnorm`, that draws a random
sample from a normal distribution. Two normal distributions of means 0
and 100 and standard deviations 1 and 5, noted noted *N(0, 1)* and
*N(100, 5)*, are shown below

```{r echo=FALSE, fig.width = 12, fig.height = 6, fig.cap = "Two normal distributions: *N(0, 1)* on the left and *N(100, 5)* on the right."}
par(mfrow = c(1, 2))
plot(density(rnorm(1000)), main = "", sub = "N(0, 1)")
plot(density(rnorm(1000, 100, 5)), main = "", sub = "N(100, 5)")
```

The three arguments, `n`, `mean` and `sd`, define the size of the
sample, and the parameters of the normal distribution, i.e the mean
and its standard deviation. The defaults of the latter are 0 and 1.


```{r}
rnorm(5)
rnorm(5, 2, 2)
rnorm(5, 100, 5)
```

Now that we have learned how to write scripts, and the basics of R's
data structures, we are ready to start working with larger data, and
learn about data frames.

## Additional exercises


`r msmbstyle::question_begin()`
- Create two vectors `x` and `y` containing the numbers 1 to 10 and 10
  to 1 respectively. You can use the `seq` or `:` functions rather
  than constructing them by hand.
- Check their type. Depending how they were created, they can be
  integers or doubles.
- Take the sum (see the `sum()` function) of each vector and verify
  they are identical.
- Sum vectors element-wise, and verify that all results are identical.
- Swap the value or `x` and `y`.
`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`
- Create a vector named x containing the numbers 20 to 2. Retrieve
  elements that are strictly larger than 5 and smaller or equal than 15.

- Remove the first 8 elements from `x` and store the result in `x2`.
`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
x <- 20:2
x
x[x > 5 & x < 15]
x2 <- x[-(1:8)]
```


`r msmbstyle::question_begin()`

You're doing an colony counting experiment, counting every day, from
Monday to Friday how many molds you see in your cell cultures.

- Create a vector named `molds` containing the results of your counts:
  1, 2, 5, 8 and 10.

- Set the names of `molds` using week days and extract the number of
  molds identified on Wednesday.

`r msmbstyle::question_end()`

```{r, echo=FALSE, include=FALSE}
molds <- c(1, 2, 5, 8, 10)
days <-  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
names(molds) <- days
molds["Wednesday"]
```

`r msmbstyle::question_begin()`

- Calculate the mean of a random distribution *N(15, 1)* of size 100
  and store it in variable `m1`.
- Calculate the mean of a random distribution *N(0, 1)* of size 100
  and store it in variable `m2`.
- Calculate the mean of another random distribution *N(15, 1)* of size
  1000 and store it in variable `m3`.
- Can you guess which one of `m1` and `m2` will be larger? Verify in R.
- Can you guess which one of `m1` and `m3` will be larger? Verify in R.

`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`

- Using the `sample` function, simulate a set of 100 students voting
  (randomly) for 1, 2 or 3 breaks during the WSBIM1207 course.

- Display the values as  a table of votes.

- Compute the number of students that wanted more that 1 break.

- Bonus: as above, but setting the probability for votes to 1/5, 2/5
  and 2/5 respectively. Read `?sample` to find out how to do that.

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
m1 <- mean(rnorm(100, 15, 1))
m2 <- mean(rnorm(100, 0, 1))
m3 <- mean(rnorm(1000, 15, 1))

## From the nature of the distributions, I expect m1 > m2
m1 > m2

## I cannot predict which one of m1 and m3 will be larger, only that
## they will be very close to each other, variating around 15
m1
m3
```

`r msmbstyle::question_begin()`

Given vectors `v1`, `v2` and `v3` below

```{r}
v1 <- c(1, 2, 3, "4")
v2 <- c(45, 23, TRUE, 21, 12, 34)
v3 <- c(v1, v2)
```

- What is the class of `v3`?
- What is the length of `v3`?
- Assign names `"a"`, `"b"`, .. to the `v3`.
- What is the value of `v3["e"]`?
- Re-using `v1`, create a vector `v4` containing

```{r, echo = FALSE, comment = NA}
(v4 <- c(v1[2:1], "NEW", v1[3:4]))
```

- What is the command to round 3.1234 to two decimanl digits?
- If you execute `round(3.1234)`, you get `3`. Why?

The WSBIM1207 students were asked how many breaks they wanted during
the four-hour Thursday morning sessions. The answers are stored in
vectors `p1` (only one break of 30 minutes), `p2` (two breaks of 15
minutes) and `p3` (three breaks of 10 minutes).

```{r}
p1 <- c(1, 1, 1)
names(p1) <- c("A34", "D3", "F12")
p2 <- c(2, 2, 2, 2)
names(p2) <- c("W4", "A21", "K7", "K8")
p3 <- c(3, 3, 3, 3, 3, 3, 3)
names(p3) <- c("D1", "D2", "A10", "D5", "D15", "A16", "B22")
```

- What command would you use to identify the number of respective
  answers?
- Concatenate all answers into a single vector `p`.
- What command would you use to get the vote for student `D2` from vector `p`?

`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`

Copy and paset the code chunk below to generate a vector of marks,
including some students with missing values that didn't take that
test.


```{r, eval = FALSE}
c(student1 = 12, student2 = 11, student3 = 4, student4 = 6, student5 = 7,
  student6 = 8.5, student7 = 13.5, student8 = 5.5, student9 = 13.5,
  student10 = 2.5, student11 = 17, student12 = 18, student13 = 15,
  student14 = 8, student15 = 7, student16 = 12, student17 = 18.5,
  student18 = 7.5, student19 = 13.5, student20 = 6, student21 = 9,
  student22 = 16, student23 = 8.5, student24 = 9, student25 = NA,
  student26 = NA, student27 = 14, student28 = 16.5, student29 = 12,
  student30 = NA, student31 = 12.5, student32 = 3, student33 = NA,
  student34 = 17, student35 = 16, student36 = 9, student37 = 6,
  student38 = 7, student39 = 8.5, student40 = 8.5, student41 = 8,
  student42 = 16.5, student43 = 4.5, student44 = NA, student45 = 8,
  student46 = 8, student47 = 7.5, student48 = 8.5, student49 = 2,
  student50 = 14, student51 = 6.5, student52 = 12, student53 = 16.5,
  student54 = 7, student55 = 9.5, student56 = 12, student57 = 8.5,
  student58 = 15.5, student59 = 9, student60 = 13.5, student61 = 18,
  student62 = 12.5, student63 = 19.5, student64 = 13, student65 = 17.5,
  student66 = 8.5, student67 = 9, student68 = 7, student69 = 12.5,
  student70 = NA, student71 = 19, student72 = 11.5, student73 = 9,
  student74 = 9.5, student75 = 12, student76 = 11, student77 = 12,
  student78 = 14, student79 = 17, student80 = 8.5, student81 = 10,
  student82 = 10, student83 = NA, student84 = 10.5, student85 = 14,
  student86 = 7.5, student87 = 4, student88 = 9, student89 = 6.5,
  student90 = 10.5, student91 = 9.5, student92 = 13, student93 = 11.5,
  student94 = NA, student95 = 6, student96 = 12.5, student97 = 11.5,
  student98 = 4, student99 = 11.5, student100 = 8)
```

- What is the number of students that have a mark > 10?

- What is the number of students that have a mark greater than the average score?

`r msmbstyle::question_end()`

<!--chapter:end:23-starting-with-r.Rmd-->

# Starting with data  {#sec-startdata}

**Learning Objectives**

- Describe what a data frame is.
- Load external data from a .csv file into a data frame.
- Summarize the contents of a data frame.
- Describe what a factor is.
- Convert between strings and factors.
- Reorder and rename factors.
- Format dates.
- Other R objects: matrices and lists
- Export and save data.

## Presentation of the gene expression data

We are going to use part of the data published by [Blackmore *et al.*
(2017)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5544260/), *The
effect of upper-respiratory infection on transcriptomic changes in the
CNS*. The goal of the study was to determine the effect of an
upper-respiratory infection on changes in RNA transcription occuring
in the cerebellum and spinal cord post infection. Gender matched eight
week old C57BL/6 mice were inoculated saline or with Influenza A by
intranasal route and transcriptomic changes in the cerebellum and
spinal cord tissues were evaluated by RNA-seq at days 0
(non-infected), 4 and 8.


The dataset is stored as a comma separated value (CSV) file.  Each row
holds information for a single RNA expression measurement, and the
columns represent:

| Column     | Description                                                                                  |
|------------|----------------------------------------------------------------------------------------------|
| gene       | The name of the gene that was measured                                                       |
| sample     | The name of the sample the gene expression was measured in                                   |
| expression | The value of the gene expression                                                             |
| organism   | The organism/species - here all data stem from mice                                          |
| age        | The age of the mouse (all mice were 8 weeks here)                                            |
| sex        | The sex of the mouse                                                                         |
| infection  | The infection state of the mouse, i.e. infected with Influenza A or not infected.            |
| strain     | The Influenza A strain; C57BL/6 in all cases.                                                |
| time       | The duration of the infection (in days).                                                     |
| tissue     | The tissue that was used for the gene expression experiment, i.e. cerebellum or spinal cord. |
| mouse      | The mouse unique identifier.                                                                 |
| ENTREZID   | The gene ID for the ENTREZ database                                                          |
| product    | The gene product                                                                             |
| ensembl_gene_id     | The ID of the gene from the ENSEMBL database                                        |
| external_synonym    | A name synonym for the gene                                                         |
| chromosome_name                      | The chromosome name of the gene                                    |
| gene_biotype                         | The gene biotype                                                   |
| phenotype_description                | The phenotype description of the gene                              |
| hsapiens_homolog_associated_gene_name| The human homologous gene                                          |

We are going to use the R function `download.file()` to download the
CSV file that contains the gene expression data, and we will use
`read.csv()` to load into memory the content of the CSV file as an
object of class `data.frame`.  Inside the `download.file` command, the
first entry is a character string with the source URL. This source URL
downloads a CSV file from a GitHub repository. The text after the
comma (`"data/rnaseq.csv"`) is the destination of the file on your
local machine. You'll need to have a folder on your machine called
`"data"` where you'll download the file. So this command downloads the
remote file, names it `"rnaseq.csv"` and adds it to a preexisting
folder named `"data"`.


```{r, eval = TRUE, echo = FALSE}
if (!file.exists("data/rnaseq.csv"))
    download.file(url = "https://raw.githubusercontent.com/Bioconductor/bioconductor-teaching/master/data/GSE96870/rnaseq.csv",
                  destfile = "data/rnaseq.csv")
```

```{r, eval = FALSE}
download.file(url = "https://raw.githubusercontent.com/Bioconductor/bioconductor-teaching/master/data/GSE96870/rnaseq.csv",
              destfile = "data/rnaseq.csv")
```

You are now ready to load the data:

```{r, eval = TRUE}
rna <- read.csv("data/rnaseq.csv")
```

This statement doesn't produce any output because, as you might
recall, assignments don't display anything. If we want to check that
our data has been loaded, we can see the contents of the data frame by
typing its name:

```{r, eval = FALSE}
rna
```

Wow... that was a lot of output. At least it means the data loaded
properly. Let's check the top (the first 6 lines) of this data frame
using the function `head()`:

```{r}
head(rna)
## Try also
## View(rna)
```

**Note**

`read.csv()` assumes that fields are delineated by commas, however, in
several countries, the comma is used as a decimal separator and the
semicolon (;) is used as a field delineator. If you want to read in
this type of files in R, you can use the `read.csv2()` function. It
behaves exactly like `read.csv()` but uses different parameters for
the decimal and the field separators. If you are working with another
format, they can be both specified by the user. Check out the help for
`read.csv()` by typing `?read.csv` to learn more. There is also the
`read.delim()` for in tab separated data files. It is important to
note that all of these functions are actually wrapper functions for
the main `read.table()` function with different arguments.  As such,
the data above could have also been loaded by using `read.table()`
with the separation argument as `,`. The code is as follows:

```{r, eval = TRUE}
rna <- read.table(file = "data/rnaseq.csv",
                  sep = ",", quote = "\"",
                  header = TRUE)
```

The header argument has to be set to `TRUE` to be able to read the
headers as by default `read.table()` has the header argument set to
FALSE. The quote argument has to be set to `"\""` to only allow "
as a quoting character (see how is written the `product` variable
for gene `Rtca` in rnaseq.csv).


## What are data frames?

Data frames are the _de facto_ data structure for most tabular data,
and what we use for statistics and plotting.

A data frame can be created by hand, but most commonly they are
generated by the functions `read.csv()` or `read.table()`; in other
words, when importing spreadsheets from your hard drive (or the web).

A data frame is the representation of data in the format of a table
where the columns are vectors that all have the same length. Because
columns are vectors, each column must contain a single type of data
(e.g., characters, integers, factors). For example, here is a figure
depicting a data frame comprising a numeric, a character, and a
logical vector.

![](./figs/data-frame.svg)


We can see this when inspecting the <b>str</b>ucture of a data frame
with the function `str()`:

```{r}
str(rna)
```

## Inspecting `data.frame` Objects

We already saw how the functions `head()` and `str()` can be useful to
check the content and the structure of a data frame. Here is a
non-exhaustive list of functions to get a sense of the
content/structure of the data. Let's try them out!

**Size**:

- `dim(rna)` - returns a vector with the number of rows in the first
   element, and the number of columns as the second element (the
   **dim**ensions of the object)
- `nrow(rna)` - returns the number of rows
- `ncol(rna)` - returns the number of columns

**Content**:

- `head(rna)` - shows the first 6 rows
- `tail(rna)` - shows the last 6 rows

**Names**:

- `names(rna)` - returns the column names (synonym of `colnames()` for
   `data.frame` objects)
- `rownames(rna)` - returns the row names

**Summary**:

- `str(rna)` - structure of the object and information about the
  class, length and content of each column
- `summary(rna)` - summary statistics for each column


Note: most of these functions are "generic", they can be used on other types of
objects besides `data.frame`.


`r msmbstyle::question_begin()`

Based on the output of `str(rna)`, can you answer the following
questions?

- What is the class of the object `rna`?
- How many rows and how many columns are in this object?
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

- class: data frame
- how many rows: 32428, how many columns: 21
`r msmbstyle::solution_end()`


## Indexing and subsetting data frames

Our `rna` data frame has rows and columns (it has 2 dimensions), if we
want to extract some specific data from it, we need to specify the
"coordinates" we want from it. Row numbers come first, followed by
column numbers. However, note that different ways of specifying these
coordinates lead to results with different classes.


```{r, eval = FALSE}
# first element in the first column of the data frame (as a vector)
rna[1, 1]
# first element in the 6th column (as a vector)
rna[1, 6]
# first column of the data frame (as a vector)
rna[, 1]
# first column of the data frame (as a data.frame)
rna[1]
# first three elements in the 7th column (as a vector)
rna[1:3, 7]
# the 3rd row of the data frame (as a data.frame)
rna[3, ]
# equivalent to head_rna <- head(rna)
head_rna <- rna[1:6, ]
head_rna
```

`:` is a special function that creates numeric vectors of integers in
increasing or decreasing order, test `1:10` and `10:1` for
instance. See section \@ref(sec-genvec) for details.

You can also exclude certain indices of a data frame using the "`-`" sign:

```{r, eval = FALSE}
rna[, -1]          ## The whole data frame, except the first column
rna[-c(7:32428), ] ## Equivalent to head(rna)
```

Data frames can be subset by calling indices (as shown previously),
but also by calling their column names directly:

```{r, eval = FALSE}
rna["gene"]       # Result is a data.frame
rna[, "gene"]     # Result is a vector
rna[["gene"]]     # Result is a vector
rna$gene          # Result is a vector
```

In RStudio, you can use the autocompletion feature to get the full and
correct names of the columns.

When we inspect the elements of the column
`hsapiens_homolog_associated_gene_name` (for example with `View(rna)`),
we can see that some cells contain NA values. If we wanted to extract
only mouse genes of this table that have a human homologous,
we could combine `is.na()` and data frames subsetting:

```{r, eval = TRUE}
is_missing_hsapiens_homolog <- is.na(rna$hsapiens_homolog_associated_gene_name)
rna_hsapiens_homolog <- rna[!is_missing_hsapiens_homolog,]
```

```{r, eval = FALSE}
head(rna_hsapiens_homolog)
```


`r msmbstyle::question_begin()`
How many mouse genes do not have a human homologous?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
> ```{r}
> rna_missing_hsapiens_homolog <- rna[is_missing_hsapiens_homolog,]
> nrow(rna_missing_hsapiens_homolog)
> ```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`

1. Create a `data.frame` (`rna_200`) containing only the data in
   row 200 of the `rna` dataset.

2. Notice how `nrow()` gave you the number of rows in a `data.frame`?

- Use that number to pull out just that last row in the initial
  `rna` data frame.

- Compare that with what you see as the last row using `tail()` to
  make sure it's meeting expectations.

- Pull out that last row using `nrow()` instead of the row number.

- Create a new data frame (`rna_last`) from that last row.

3. Use `nrow()` to extract the row that is in the middle of the
   `rna` dataframe. Store the content of this row in an object
   named `rna_middle`.

4. Combine `nrow()` with the `-` notation above to reproduce the
   behavior of `head(rna)`, keeping just the first through 6th
   rows of the rna dataset.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
> ```{r}
> ## 1.
> rna_200 <- rna[200, ]
> ## 2.
> ## Saving `n_rows` to improve readability and reduce duplication
> n_rows <- nrow(rna)
> rna_last <- rna[n_rows, ]
> ## 3.
> rna_middle <- rna[n_rows / 2, ]
> ## 4.
> rna_head <- rna[-(7:n_rows), ]
> ```
`r msmbstyle::solution_end()`



## Factors

Factors represent **categorical data**. They are stored as integers
associated with labels and they can be ordered or unordered. While
factors look (and often behave) like character vectors, they are
actually treated as integer vectors by R. So you need to be very
careful when treating them as strings.

Once created, factors can only contain a pre-defined set of values,
known as *levels*. By default, R always sorts levels in alphabetical
order. For instance, if you have a factor with 2 levels:

```{r}
sex <- factor(c("male", "female", "female", "male", "female"))
```

R will assign `1` to the level `"female"` and `2` to the level
`"male"` (because `f` comes before `m`, even though the first element
in this vector is `"male"`). You can see this by using the function
`levels()` and you can find the number of levels using `nlevels()`:


```{r}
levels(sex)
nlevels(sex)
```

`r msmbstyle::question_begin()`

- How many genes have been measured in this experiment?

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
nlevels(factor(rna$gene))
```

`r msmbstyle::solution_end()`


Sometimes, the order of the factors does not matter, other times you
might want to specify the order because it is meaningful (e.g., "low",
"medium", "high"), it improves your visualization, or it is required
by a particular type of analysis. Here, one way to reorder our levels
in the `sex` vector would be:


```{r}
sex ## current order
sex <- factor(sex, levels = c("male", "female"))
sex ## after re-ordering
```

In R's memory, these factors are represented by integers (1, 2, 3),
but are more informative than integers because factors are self
describing: `"female"`, `"male"` is more descriptive than `1`,
`2`. Which one is "male"?  You wouldn't be able to tell just from the
integer data. Factors, on the other hand, have this information built
in. It is particularly helpful when there are many levels (like the
species names in our example dataset).

### Converting to characters {-}

If you need to convert a factor to a character vector, you use
`as.character(x)`.

```{r}
as.character(sex)
# try also: as.numeric(sex)
```

<!-- ### Numeric factors {-} -->

<!-- Converting factors where the levels appear as numbers (such as -->
<!-- concentration levels, or years) to a numeric vector is a little -->
<!-- trickier. The `as.numeric()` function returns the index values of the -->
<!-- factor, not its levels, so it will result in an entirely new (and -->
<!-- unwanted in this case) set of numbers.  One method to avoid this is to -->
<!-- convert factors to characters, and then to numbers.  Another method is -->
<!-- to use the `levels()` function. Compare: -->

<!-- ```{r} -->
<!-- year_fct <- factor(c(1990, 1983, 1977, 1998, 1990)) -->
<!-- as.numeric(year_fct)  ## Wrong! And there is no warning... -->
<!-- as.numeric(as.character(year_fct)) ## Works... -->
<!-- as.numeric(levels(year_fct))[year_fct] ## The recommended way. -->
<!-- ```

<!-- Notice that in the `levels()` approach, three important steps occur: -->

<!-- * We obtain all the factor levels using `levels(year_fct)` -->
<!-- * We convert these levels to numeric values using `as.numeric(levels(year_fct))` -->
<!-- * We then access these numeric values using the underlying integers of the -->
<!--   vector `year_fct` inside the square brackets -->

### Renaming factors {-}

When your data is stored as a factor, you can use the `plot()`
function to get a quick glance at the number of observations
represented by each factor level. Let's look at the number of males
and females in our data.

```{r firstfactorplot, fig.cap = "Bar plot of the number of females and males."}
plot(sex)
```

If we want to rename these factor, it is sufficient to change its
levels:

```{r}
levels(sex)
levels(sex) <- c("M", "F")
sex
plot(sex)
```

`r msmbstyle::question_begin()`

- Rename "female" and "male" to "Female" and "Male" respectively.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`

We have seen how data frames are created when using `read.csv()`, but
they can also be created by hand with the `data.frame()` function.
There are a few mistakes in this hand-crafted `data.frame`. Can you
spot and fix them?  Don't hesitate to experiment!

```{r, eval=FALSE}
animal_data <- data.frame(
       animal = c(dog, cat, sea cucumber, sea urchin),
       feel = c("furry", "squishy", "spiny"),
       weight = c(45, 8 1.1, 0.8))
```
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
- missing quotations around the names of the animals
- missing one entry in the "feel" column (probably for one of the furry animals)
- missing one comma in the weight column
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Can you predict the class for each of the columns in the following
example?

Check your guesses using `str(country_climate)`:

- Are they what you expected?  Why? Why not?

- Try again by adding `stringsAsFactors = TRUE` after the last
  variable when creating the data frame? What is happening now?
  `stringsAsFactors` can also be set when reading text-based
  spreadsheets into R using `read.csv()`.

```{r, eval = FALSE}
country_climate <- data.frame(
       country = c("Canada", "Panama", "South Africa", "Australia"),
       climate = c("cold", "hot", "temperate", "hot/temperate"),
       temperature = c(10, 30, 18, "15"),
       northern_hemisphere = c(TRUE, TRUE, FALSE, "FALSE"),
       has_kangaroo = c(FALSE, FALSE, FALSE, 1)
       )
```
`r msmbstyle::question_end()`

The automatic conversion of data type is sometimes a blessing, sometimes an
annoyance. Be aware that it exists, learn the rules, and double check that data
you import in R are of the correct type within your data frame. If not, use it
to your advantage to detect mistakes that might have been introduced during data
entry (a letter in a column that should only contain numbers for instance).

Learn more in this [RStudio
tutorial](https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio)

## Matrices

Before proceeding, now that we have learnt about dataframes, let's
recap package installation and learn about a new data type, namely the
`matrix`. Like a `data.frame`, a matrix has two dimensions, rows and
columns. But the major difference is that all cells in a `matrix` must
be of the same type: `numeric`, `character`, `logical`, ... In that
respect, matrices are closer to a `vector` than a `data.frame`.

The default constructor for a matrix is `matrix`. It takes a vector of
values to populate the matrix and the number of row and/or
columns[^ncol]. The values are sorted along the columns, as illustrated
below but you can also sort them along the row with the argument `byrow = TRUE`.

```{r mat1}
m <- matrix(1:9, ncol = 3, nrow = 3)
m
# try now with byrow = TRUE
```

[^ncol]: Either the number of rows or columns are enough, as the other
one can be deduced from the length of the values. Try out what happens
if the values and number of rows/columns don't add up.

```{r pkg_qst, echo = FALSE}
msmbstyle::question(text = "Using the function `installed.packages()`, create a `character` matrix containing the information about all packages currently installed on your computer. Explore it.")
```

`r msmbstyle::solution_begin()`

```{r pkg_sln, eval = FALSE}
## create the matrix
ip <- installed.packages()
head(ip)
## try also View(ip)
## number of package
nrow(ip)
## names of all installed packages
rownames(ip)
## type of information we have about each package
colnames(ip)
```

`r msmbstyle::solution_end()`


It is often useful to create large random data matrices as test
data. The exercise below asks you to create such a matrix with random
data drawn from a normal distribution of mean 0 and standard deviation
1, which can be done with the `rnorm()` function.

```{r rnormmat_qst, echo = FALSE}
msmbstyle::question(text = "Construct a matrix of dimension 1000 by 3 of normally distributed data (mean 0, standard deviation 1).")
```


`r msmbstyle::solution_begin()`

```{r rnormmat_sln}
set.seed(123)
m <- matrix(rnorm(3000), ncol = 3)
dim(m)
head(m)
```

`r msmbstyle::solution_end()`


## Formatting Dates

One of the most common issues that new (and experienced!) R users have
is converting date and time information into a variable that is
appropriate and usable during analyses.

### Note on dates in spreadsheet programs {-}

Dates in spreadsheets are generally stored in a single column. While
this seems the most natural way to record dates, it actually is not
best practice. A spreadsheet application will display the dates in a
seemingly correct way (to a human observer) but how it actually
handles and stores the dates may be problematic. It is often much
safer to store dates with YEAR, MONTH and DAY in separate columns or
as YEAR and DAY-OF-YEAR in separate columns.

Spreadsheet programs such as LibreOffice, Microsoft Excel, OpenOffice,
Gnumeric, ... have different (and often incompatible) ways of encoding
dates (even for the same program between versions and operating
systems). Additionally, Excel can [turn things that aren't dates into
dates](https://nsaunders.wordpress.com/2012/10/22/gene-name-errors-and-excel-lessons-not-learned/)
(@Zeeberg:2004), for example names or identifiers like MAR1, DEC1,
OCT4. So if you're avoiding the date format overall, it's easier to
identify these issues.

The [Dates as
data](https://datacarpentry.org/spreadsheet-ecology-lesson/03-dates-as-data/index.html)
section of the Data Carpentry lesson provides additional insights
about pitfalls of dates with spreadsheets.


We are going to use the `ymd()` function from the package
**`lubridate`** (which belongs to the **`tidyverse`**; learn more
[here](https://www.tidyverse.org/)). . **`lubridate`** gets installed
as part as the **`tidyverse`** installation. When you load the
**`tidyverse`** (`library(tidyverse)`), the core packages (the
packages used in most data analyses) get loaded. **`lubridate`**
however does not belong to the core tidyverse, so you have to load it
explicitly with `library(lubridate)`

Start by loading the required package:

```{r loadlibridate, message=FALSE}
library("lubridate")
```

`ymd()` takes a vector representing year, month, and day, and converts
it to a `Date` vector. `Date` is a class of data recognized by R as
being a date and can be manipulated as such. The argument that the
function requires is flexible, but, as a best practice, is a character
vector formatted as "YYYY-MM-DD".


Let's create a date object and inspect the structure:

```{r}
my_date <- ymd("2015-01-01")
str(my_date)
```

Now let's paste the year, month, and day separately - we get the same result:

```{r}
# sep indicates the character to use to separate each component
my_date <- ymd(paste("2015", "1", "1", sep = "-"))
str(my_date)
```

Let's now familiarise ourselves with a typical date manipulation
pipeline. The small data below has stored dates in different `year`,
`month` and `day` columns.

```{r}
x <- data.frame(year = c(1996, 1992, 1987, 1986, 2000, 1990, 2002, 1994, 1997, 1985),
                month = c(2,  3,  3, 10,  1,  8,  3,  4,  5,  5),
                day = c(24,  8,  1,  5,  8, 17, 13, 10, 11, 24),
                value = c(4,  5,  1,  9,  3,  8, 10,  2,  6,  7))
x
```

Now we apply this function to the `x` dataset. We first dreate a
character vector from the `year`, `month`, and `day` columns of `x`
using `paste()`:

```{r}
paste(x$year, x$month, x$day, sep = "-")
```

This character vector can be used as the argument for `ymd()`:

```{r}
ymd(paste(x$year, x$month, x$day, sep = "-"))
```

The resulting `Date` vector can be added to `x` as a new column called `date`:

```{r}
x$date <- ymd(paste(x$year, x$month, x$day, sep = "-"))
str(x) # notice the new column, with 'date' as the class
```

Let's make sure everything worked correctly. One way to inspect the
new column is to use `summary()`:

```{r}
summary(x$date)
```

Note that `ymd()` expects to have the year, month and day, in that
order. If you have for instance day, month and year, you would need
`dmy()`.

```{r}
dmy(paste(x$day, x$month, x$month, sep = "-"))
```

`lubdridate` has many functions to address all date variations.

## Summary of R objects

So far, we have seen several types of R object varying in the number
of dimensions and whether they could store a single or multiple data
types:

- **`vector`**: one dimension (they have a length), single type of data.
- **`matrix`**: two dimensions, single type of data.
- **`data.frame`**: two dimensions, one type per column.

## Lists

A data type that we haven't seen yet, but that is useful to know, and
follows from the summary that we have just seen are lists:

- **`list`**: one dimension, every item can be of a different data
    type.

Below, let's create a list containing a vector of numbers, characters,
a matrix, a dataframe and another list:

```{r list0}
l <- list(1:10, ## numeric
          letters, ## character
          installed.packages(), ## a matrix
          cars, ## a data.frame
          list(1, 2, 3)) ## a list
length(l)
str(l)
```

List subsetting is done using `[]` to subset a new sub-list or `[[]]`
to extract a single element of that list (using indices or names, of
the list is named).

```{r}
l[[1]] ## first element
l[1:2] ## a list of length 2
l[1]   ## a list of length 1
```

r msmbstyle::question_begin()`
You can also attribute a name to each element of a list. Give the following names (in that order) to the list elements with the function `names()`: numbers, alphabet, installed_packages, cars, random. Check if the names were correctly attributed.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
names(l) <- c("numbers", "alphabet", "installed_packages", 
              "cars", "random")
names(l)
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Subset the previously defined list `l`, to keep:

* a list with all but the matrix of installed packages
* only the `cars` data.frame
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
# list with all but the matrix of installed packages
l[-3]
# only the `cars` data.frame
l[["cars"]]
# equivalent to:
l[[4]]
```

`r msmbstyle::solution_end()`


## Exporting and saving data {#sec-exportandsave}

### Exporting tabular data {-}

We have seen how to read a text-based spreadsheet into R using the
`read.table` family of functions. To export a `data.frame` to a
text-based spreadsheet, we can use the `write.table` set of functions
(`write.csv`, `write.delim`, ...). They all take the variable to be
exported and the file to be exported to. For example, to export the
`rna` data to the `my_rnaseq.csv` file in the `data_output`
directory, we would execute:

```{r, eval = FALSE}
write.csv(rna, file = "data_output/my_rnaseq.csv")
```
This new csv file can now be shared with other collaborators who
aren't familiar with R.

### Saving data {-}

Exporting data to a spreadsheet has several limitations, such as those
described in the first chapter such as possible inconsistencies with
`,` and `.` for decimal separators and lack of variable type
definitions. Furthermore, exporting data to a spreadsheet is only
relevant for rectangular data such as dataframes and matrices.

A more general way to save data, that is specific to R and is
guaranteed to work on any operating system, is to use the `save`
function. Saving objects will generate a binary representation of the
object on disk, a *R Data* file (`rda` extension) that guarantees to
produce the same object once loaded back into R using the `load`
function.

```{r, eval = FALSE}
save(rna, file = "data_output/rnaseq.rda")
rm(rna)
load("data_output/rnaseq.rda")
head(rna)
```

Note about how the function `load` loads the object in the file
directly in the global environment.


There is also the `saveRDS` and `readRDS` functions that save R
objects to binary files (using the `rds` extension here) and read
these back into R. From a user's perspective, main different is that,
`load` loads an object in the global environment while `readRDS` reads
the data from disk and returns it. It is this necessary to store the
output of `readRDS` in a variable:

```{r}
saveRDS(rna, file = "data_output/rnaseq.rds")
rm(rna)
rna <- readRDS("data_output/rnaseq.rds")
head(rna)
```

```{r echo=FALSE}
unlink("data_output/rnaseq.rda")
unlink("data_output/rnaseq.rds")
```

To conclude, when it comes to saving data from R that will be loaded
again in R, saving and loading is the preferred approach. If tabular
data need to be shared with somebody that is not using R, then
exporting to a text-based spreadsheet is a good alternative.

## Additional exercises

`r msmbstyle::question_begin()`

You're doing an colony counting experiment, counting every day how
many molds you see in your cell cultures.

- Create a vector named `molds` containing the results of your counts:
  1, 2, 5, 8 and 10. Create a vector `days` containing the week day,
  from Monday to Friday. Use these two vector to create a `data.frame`
  named `molds_study` containing two variables, `Day` and
  `Molds_count`.

- Create a new `data.frame` that contains the observations where more
  than 2 colonies were counted. How many observations are there? How
  many counts are there in total for these observations.

You repeat the molds study experiment the following week and count the
following numbers of molds: 1, 6, 6, 5 and 4.

- Add these data as a third column to the `molds_study` `data.frame`
  and rename the variables as `Day`, `Molds_1` and `Molds_2`.

- Calculate for each experiment the total number of molds
  counted. Check if the first experiment counted more molds than the
  second one.

- Save the `molds_study` variable in a file named `molds_study.rda`.

`r msmbstyle::question_end()`

<!-- `r msmbstyle::solution_begin()` -->
<!-- ```{r} -->
<!-- molds <- c(1, 2, 5, 8, 10) -->
<!-- days <-  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") -->
<!-- molds_study <- data.frame("Day" = days, "Molds_count" = molds) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- molds2 <- molds_study[molds_study$Molds_count > 2, ] -->
<!-- nrow(molds2) -->
<!-- sum(molds2$Molds_count) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- molds_study$Molds_count2 <- c(1, 6, 6, 5, 4) -->
<!-- names(molds_study)[2:3] <- c("Molds_1", "Molds_2") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- total1 <- sum(molds_study$Molds_1) -->
<!-- total2 <- sum(molds_study$Molds_2) -->
<!-- total1 > total2 -->
<!-- ``` -->
<!-- `r msmbstyle::solution_end()` -->


`r msmbstyle::question_begin()`

We are going to analyse beer consumption in 48 individuals. The data
are available in the `rWSBIM1207` package. The data illustrated the
fictive beer consumption in liters per year at different age according
to gender and employment.

- Load the `rWSBIM1207` package. If the package isn't installed of its
  version is greater than 0.1.1, install it from the
  `UCLouvain-CBIO/rWSBIM1207` GitHub repository using the
  `remotes::install_github()` function. If you use a recent Renku
  enironment, the package is already available.

- Using the `beers.csv()` function from `rWSBIM1207`, find the path
  the `beers.csv` file and read it to produce a `data.frame` named
  `beers`. The spreadsheet uses semi-colons `;` to separate cells. Use
  `read.csv2()` and `read.delim()` and set the separator
  appropriately, and verify that the two variables are identical.

- Check the number of observations and identify the variables that are
  available. Calculate a summary of each variable using the `summary`
  function directly on the `data.frame`.

- Calculate the mean and the median age and consumption.

- Do men consume more beer than women on average? To answer this
  question, calculate the mean consumption for men only, selecting the
  observations with `Gender` equal to `Male`. Then do the same for
  observations with `Gender` equal to `Female`.

- Calculate a two-way table of gender and employment status.

- Remove observations with missing values and export the data into a
  new `csv` file called `beers_no_na.csv`.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`

We are going to analyse clinical data from The Cancer Genome Atlas
(TCGA). The data are available in the `rWSBIM1207` package.

- Load the `rWSBIM1207` package. If the package isn't installed of its
  version is greater than 0.1.1, install it from the
  `UCLouvain-CBIO/rWSBIM1207` GitHub repository using the
  `remotes::install_github()` function. If you use a recent Renku
  enironment, the package is already available.

- Obtain the path to th csv file containing the clinical data need for
  this exercise using the `clinical1.csv` function and read it into R
  as a `data.frame` called `clinical`.

- Inspect the data using `str` and `View`. How many patients are
  recorded in the table?

- Print the column names using two different functions.

- Create a smaller data frame called `clinical_mini` containing only
  the columns corresponding to the `patientID`, `gender`,
  `age_at_diagnosis` and `smoking_history`. Try to do this using
  column indices and column names.

- Inspect the `smoking_history` column. How many categories are
  recorded? How many observations are there for each category?

- The column age at diagnosis is recorded in days. Create a new column
  `years_at_diagnosis` corresponding to the age at diagnosis converted
  in years.

- Calculate the mean and median age at diagnosis. Hint: pay attention
  to missing values!

- Is there a difference between the `years_at_diagnosis` for male and
  female patients?

- Use the `quantile` function to calculate the first and last quartile
  of age at diagnosis. Use the help function (`?quantile`) to see how
  to use the `quantile` function.

- Use the `summary` function to confirm your previous results.

`r msmbstyle::question_end()`


<!-- `r msmbstyle::solution_begin()` -->

<!-- ```{r} -->
<!-- library("rWSBIM1207") -->
<!-- clinical <- read.csv(clinical1.csv()) -->

<!-- str(clinical) -->
<!-- names(clinical) -->
<!-- ## or -->
<!-- colnames(clinical) -->

<!-- clinical_mini <- clinical[, c(1, 3, 4, 12)] -->
<!-- ## or -->
<!-- clinical_mini <- clinical[, c("patientID", "gender", "age_at_diagnosis", "smoking_history")] -->

<!-- levels(clinical_mini$smoking_history) -->
<!-- table(clinical_mini$smoking_history) -->

<!-- clinical_mini$years_at_diagnosis <- clinical_mini$age_at_diagnosis / 365 -->

<!-- mean(clinical_mini$years_at_diagnosis, na.rm = TRUE) -->
<!-- median(clinical_mini$years_at_diagnosis, na.rm = TRUE) -->

<!-- quantile(clinical_mini$years_at_diagnosis, 0.25, na.rm = TRUE) -->
<!-- quantile(clinical_mini$years_at_diagnosis, 0.75, na.rm = TRUE) -->

<!-- summary(clinical_mini$years_at_diagnosis) -->
<!-- ``` -->

<!-- `r msmbstyle::solution_end()` -->

<!--chapter:end:25-starting-with-data.Rmd-->

# Manipulating and analyzing data with dplyr  {#sec-dplyr}


**Learning Objectives**

- Describe the purpose of the **`dplyr`** and **`tidyr`** packages.

- Select certain columns in a data frame with the **`dplyr`** function
  `select`.

- Select certain rows in a data frame according to filtering
  conditions with the **`dplyr`** function `filter` .

- Link the output of one **`dplyr`** function to the input of another
  function with the 'pipe' operator `%>%`.

- Add new columns to a data frame that are functions of existing
  columns with `mutate`.

- Use the split-apply-combine concept for data analysis.

- Use `summarize`, `group_by`, and `count` to split a data frame into
  groups of observations, apply summary statistics for each group, and
  then combine the results.

- Describe the concept of a wide and a long table format and for which
  purpose those formats are useful.

- Reshape a data frame from long to wide format and back with the
  `pivot_wider()` and `pivot_longer()` commands from the
  **`tidyr`** package.



## Data Manipulation using **`dplyr`** and **`tidyr`**

Bracket subsetting is handy, but it can be cumbersome and difficult to
read, especially for complicated operations. Enter
**`dplyr`**. **`dplyr`** is a package for making tabular data
manipulation easier. It pairs nicely with **`tidyr`** which enables
you to swiftly convert between different data formats for plotting and
analysis.

Packages in R are basically sets of additional functions that let you
do more stuff. The functions we've been using so far, like `str()` or
`data.frame()`, come built into R; packages give you access to more of
them. Before you use a package for the first time you need to install
it on your machine, and then you should import it in every subsequent
R session when you need it. You should already have installed the
**`tidyverse`** package. This is an "umbrella-package" that installs
several packages useful for data analysis which work together well
such as **`tidyr`**, **`dplyr`**, **`ggplot2`**, **`tibble`**, etc.


The **`tidyverse`** package tries to address 3 common issues that
arise when doing data analysis with some of functions that come with
R:

1. The results from a base R function sometimes depend on the type of data.
2. Using R expressions in a non standard way, which can be confusing for new
   learners.
3. Hidden arguments, having default operations that new learners are not aware
   of.

We have seen in our previous lesson that when building or importing a
data frame, the columns that contain characters (i.e., text) are
coerced (=converted) into the `factor` data type. We had to set
**`stringsAsFactors`** to **`FALSE`** to avoid this hidden argument to
convert our data type.

This time will use the **`tidyverse`** package to read the data and
avoid having to set **`stringsAsFactors`** to **`FALSE`**

To load the package type:


```{r, message = FALSE, purl = FALSE}
## load the tidyverse packages, incl. dplyr
library("tidyverse")
```

The [Data Transformation Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)
provides an overview of the `dplyr` grammar, offering more details and
functions that we will see in this chapter.

## What are **`dplyr`** and **`tidyr`**?

The package **`dplyr`** provides easy tools for the most common data manipulation
tasks. It is built to work directly with data frames, with many common tasks
optimized by being written in a compiled language (C++). An additional feature is the
ability to work directly with data stored in an external database. The benefits of
doing this are that the data can be managed natively in a relational database,
queries can be conducted on that database, and only the results of the query are
returned.

This addresses a common problem with R in that all operations are conducted
in-memory and thus the amount of data you can work with is limited by available
memory. The database connections essentially remove that limitation in that you
can connect to a database of many hundreds of GB, conduct queries on it directly, and pull
back into R only what you need for analysis.

The package **`tidyr`** addresses the common problem of wanting to
reshape your data for plotting and use by different R
functions. Sometimes we want data sets where we have one row per
measurement. Sometimes we want a data frame where each measurement
type has its own column, and rows are instead more aggregated groups -
like plots or aquaria. Moving back and forth between these formats is
nontrivial, and **`tidyr`** gives you tools for this and more
sophisticated data manipulation.

To learn more about **`dplyr`** and **`tidyr`** after the workshop,
you may want to check out this [handy data transformation with
**`dplyr`**
cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)
and this [one about
**`tidyr`**](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf).

We'll read in our data using the `read_csv()` function, from the
tidyverse package **`readr`**, instead of `read.csv()`.


```{r dplyr_setp, results = 'hide', purl = FALSE}
rna <- read_csv("data/rnaseq.csv")

## inspect the data
str(rna)
## preview the data
# View(rna)
```

Notice that the class of the data is now `tbl_df`

This is referred to as a "tibble". Tibbles tweak some of the behaviors
of the data frame objects we introduced in the previous episode. The
data structure is very similar to a data frame. For our purposes the
only differences are that:

1. In addition to displaying the data type of each column under its name, it
   only prints the first few rows of data and only as many columns as fit on one
   screen.
2. Columns of class `character` are never converted into factors.


We're going to learn some of the most common **`dplyr`** functions:

- `select()`: subset columns
- `filter()`: subset rows on conditions
- `mutate()`: create new columns by using information from other columns
- `group_by()` and `summarize()`: create summary statisitcs on grouped data
- `arrange()`: sort results
- `count()`: count discrete values

## Selecting columns and filtering rows

To select columns of a data frame, use `select()`. The first argument
to this function is the data frame (`rna`), and the subsequent
arguments are the columns to keep.

```{r}
select(rna, gene, sample, tissue, expression)
```

To select all columns *except* certain ones, put a "-" in front of
the variable to exclude it.

```{r}
select(rna, -organism, -strain)
```

This will select all the variables in `rna` except `organism`
and `strain`.

To choose rows based on a specific criteria, use `filter()`:

```{r, purl = FALSE}
filter(rna, sex == "Male")
filter(rna, sex == "Male" & infection == "NonInfected")
```

## Pipes

What if you want to select and filter at the same time? There are three
ways to do this: use intermediate steps, nested functions, or pipes.

With intermediate steps, you create a temporary data frame and use
that as input to the next function, like this:

```{r, purl = FALSE}
rna2 <- filter(rna, sex == "Male")
rna3 <- select(rna2, gene, sample, tissue, expression)
rna3
```

This is readable, but can clutter up your workspace with lots of objects that you have to name individually. With multiple steps, that can be hard to keep track of.

You can also nest functions (i.e. one function inside of another), like this:

```{r, purl = FALSE}
rna3 <- select(filter(rna, sex == "Male"), gene, sample, tissue, expression)
rna3
```

This is handy, but can be difficult to read if too many functions are nested, as
R evaluates the expression from the inside out (in this case, filtering, then selecting).

The last option, *pipes*, are a recent addition to R. Pipes let you take
the output of one function and send it directly to the next, which is useful
when you need to do many things to the same dataset.  Pipes in R look like
`%>%` and are made available via the **`magrittr`** package, installed automatically
with **`dplyr`**. If you use RStudio, you can type the pipe with <kbd>Ctrl</kbd>
+ <kbd>Shift</kbd> + <kbd>M</kbd> if you have a PC or <kbd>Cmd</kbd> +
<kbd>Shift</kbd> + <kbd>M</kbd> if you have a Mac.

```{r, purl = FALSE}
rna %>%
  filter(sex == "Male") %>%
  select(gene, sample, tissue, expression)
```

In the above code, we use the pipe to send the `rna` dataset first through
`filter()` to keep rows where `sex` is Male, then through `select()`
to keep only the `gene`, `sample`, `tissue`, and `expression`columns.
Since `%>%` takes the object on its left and passes it as the first argument
to the function on its right, we don't need to explicitly include the data frame
as an argument to the `filter()` and `select()` functions any more.

Some may find it helpful to read the pipe like the word "then". For instance,
in the above example, we took the data frame `rna`, *then* we `filter`ed
for rows with `sex == "Male"`, *then* we `select`ed columns `gene`, `sample`,
`tissue`, and `expression`. The **`dplyr`** functions by themselves are somewhat
simple, but by combining them into linear workflows with the pipe, we can accomplish
more complex manipulations of data frames.

If we want to create a new object with this smaller version of the data, we
can assign it a new name:

```{r, purl = FALSE}
rna3 <- rna %>%
  filter(sex == "Male") %>%
  select(gene, sample, tissue, expression)

rna3
```

Note that the final data frame is the leftmost part of this expression.


`r msmbstyle::question_begin()`

Using pipes, subset the `rna` data to genes with an expression higher
than 50000 in male mice at time 0, and retain only the columns `gene`,
`sample`, `time`, `expression` and `age`

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
```{r, eval=TRUE, purl=FALSE}
rna %>%
  filter(expression > 50000, sex == "Male", time == 0 ) %>%
  select(gene, sample, time, expression, age)
```
`r msmbstyle::solution_end()`


## Mutate

Frequently you'll want to create new columns based on the values in existing
columns, for example to do unit conversions, or to find the ratio of values in two
columns. For this we'll use `mutate()`.

To create a new column of time in hours:

```{r, purl = FALSE}
rna %>%
  mutate(time_hours = time * 24) %>%
  select(time, time_hours)
```

You can also create a second new column based on the first new column within the same call of `mutate()`:

```{r, purl = FALSE}
rna %>%
  mutate(time_hours = time * 24, time_mn = time_hours * 60) %>%
  select(time, time_hours, time_mn)
```

If this runs off your screen and you just want to see the first few rows, you
can use a pipe to view the `head()` of the data. (Pipes work with non-`dplyr`
functions, too, as long as the **`dplyr`** or `magrittr` package is loaded).

```{r, purl = FALSE}
rna %>%
  mutate(time_hours = time * 24, time_mn = time_hours * 60) %>%
  select(time, time_hours, time_mn) %>%
  head()
```

Let's imagine we are interested in the human homologs of the mouse
genes analysed in this dataset. This information can be found in the
last column of the `rna` tibble, named `hsapiens_homolog_associated_gene_name`.

```{r, purl = FALSE}
rna %>%
  select(gene, hsapiens_homolog_associated_gene_name)
```

Some mouse gene have no human homologs. These can be retrieved using a `filter()`
in the chain, and the `is.na()` function that determines whether something is an `NA`.

```{r, purl = FALSE}
rna %>%
  select(gene, hsapiens_homolog_associated_gene_name) %>%
  filter(is.na(hsapiens_homolog_associated_gene_name))
```

If we want to keep only mouse gene that have a human homolog, we can
insert a `!`  symbol that negates the result, so we're asking for
every row where `hsapiens_homolog_associated_gene_name` *is not* an
`NA`.

The first few rows of the output are full of `NA`s, so if we wanted to remove
those we could insert a `filter()` in the chain:

```{r, purl = FALSE}
rna %>%
  select(gene, hsapiens_homolog_associated_gene_name) %>%
  filter(!is.na(hsapiens_homolog_associated_gene_name))
```

`r msmbstyle::question_begin()` Create a new data frame from the `rna`
data that meets the following criteria: contains only the `gene`,
`chromosome_name`, `phenotype_description`, `sample`, and `expression`
columns and a new column giving the log expression the gene.  This
data frame must only contain gene located on autosomes and associated
with a `phenotype_description`.

**Hint**: think about how the commands should be ordered to produce
this data frame!

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
```{r, eval=TRUE, purl=FALSE}
rna %>%
  filter(chromosome_name != "X", chromosome_name != "Y") %>%
  mutate(log_expression = log(expression)) %>%
  select(gene, chromosome_name, phenotype_description, sample, log_expression) %>%
  filter(!is.na(phenotype_description))
```
`r msmbstyle::solution_end()`


## Split-apply-combine data analysis

Many data analysis tasks can be approached using the
*split-apply-combine* paradigm: split the data into groups, apply some
analysis to each group, and then combine the results. **`dplyr`**
makes this very easy through the use of the `group_by()` function.

```{r}
rna %>%
  group_by(gene)
```

The `group_by()` function doesn't perform any data processing, it
groups the data into subsets: in the example above, our initial
`tibble` of `r nrow(rna)` observations is split into
`r length(unique(rna$gene))` groups based on the `gene` variable.

Once the data have been combined, subsequent operations will be
applied on each group independently.


### The `summarize()` function

`group_by()` is often used together with `summarize()`, which
collapses each group into a single-row summary of that group.
`group_by()` takes as arguments the column names that contain the
**categorical** variables for which you want to calculate the summary
statistics. So to compute the mean `expression` by gene:

```{r}
rna %>%
  group_by(gene) %>%
  summarize(mean_expression = mean(expression))
```

You may also have noticed that the output from these calls doesn't run off the
screen anymore. It's one of the advantages of `tbl_df` over data frame.

You can also group by multiple columns:

```{r}
rna %>%
  group_by(gene, infection, time) %>%
  summarize(mean_expression = mean(expression))
```

Here, again, the output from these calls doesn't run off the screen
anymore. If you want to display more data, you can use the `print()` function
at the end of your chain with the argument `n` specifying the number of rows to
display:

```{r, purl = FALSE}
rna %>%
  group_by(gene, infection, time) %>%
  summarize(mean_expression = mean(expression)) %>%
  print(n = 15)
```

Once the data is grouped, you can also summarize multiple variables at the same
time (and not necessarily on the same variable). For instance, we could add
columns indicating the median `expression` by gene and by condition:

```{r, purl = FALSE}
rna %>%
  group_by(gene, infection, time) %>%
  summarize(mean_expression = mean(expression),
            median_expression = median(expression))
```

It is sometimes useful to rearrange the result of a query to inspect the values.
For instance, we can sort on `mean_expression` to put the genes lowly expressed first:


```{r, purl = FALSE}
rna %>%
  group_by(gene, infection, time) %>%
  summarize(mean_expression = mean(expression),
            median_expression = median(expression)) %>%
  arrange(mean_expression)
```

To sort in descending order, we need to add the `desc()` function:

```{r, purl = FALSE}
rna %>%
  group_by(gene, infection, time) %>%
  summarize(mean_expression = mean(expression),
            median_expression = median(expression)) %>%
  arrange(desc(mean_expression))
```


### Counting

When working with data, we often want to know the number of observations found
for each factor or combination of factors. For this task, **`dplyr`** provides
`count()`. For example, if we wanted to count the number of rows of data for
each infected and non infected, we would do:

```{r, purl = FALSE}
rna %>%
    count(infection)
```

The `count()` function is shorthand for something we've already seen: grouping by a variable, and summarizing it by counting the number of observations in that group. In other words, `rna %>% count()` is equivalent to:

```{r, purl = FALSE}
rna %>%
    group_by(infection) %>%
    summarise(count = n())
```

For convenience, `count()` provides the `sort` argument:

```{r, purl = FALSE}
rna %>%
    count(infection, sort = TRUE)
```

Previous example shows the use of `count()` to count the number of rows/observations
for *one* factor (i.e., `infection`).
If we wanted to count *combination of factors*, such as `infection` and `time`,
we would specify the first and the second factor as the arguments of `count()`:

```{r purl = FALSE}
rna %>%
    count(infection, time)
```

With the above code, we can proceed with `arrange()` to sort the table
according to a number of criteria so that we have a better comparison.
For instance, we might want to arrange the table above by time:

```{r purl = FALSE}
rna %>%
  count(infection, time) %>%
  arrange(time)
```

or by counts:

```{r purl = FALSE}
rna %>%
  count(infection, time) %>%
  arrange(n)
```

`r msmbstyle::question_begin()`

1. How many genes were analysed in each sample?

2. Use `group_by()` and `summarize()` to evaluate the sequencing depth
(the sum of all counts) in each sample. Which sample has the highest sequencing depth?

3. Calculate the mean expression level of gene "Dok3" by timepoints.

4. Pick one sample and evaluate the number of genes by biotype

5. Identify genes associated with "abnormal DNA methylation" phenotype description,
   and calculate their mean expression (in log) at time 0, time 4 and time 8.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

1. How many genes were analysed in each sample?

```{r, purl=FALSE}
rna %>%
  count(sample)
```

2. Use `group_by()` and `summarize()` to evaluate the sequencing depth
(the sum of all counts) in each sample. Which sample has the highest sequencing depth?

```{r, purl=FALSE}
rna %>%
  group_by(sample) %>%
  summarize(seq_depth = sum(expression)) %>%
  arrange(desc(seq_depth))
```

3. Calculate the mean expression level of gene "Dok3" by timepoints.

```{r, purl=FALSE}
rna %>%
  filter(gene == "Dok3") %>%
  group_by(time) %>%
  summarize(mean = mean(expression)) %>%
  arrange(time)
```

4. Pick one sample and evaluate the number of genes by biotype

```{r, purl=FALSE}
rna %>%
  filter(sample == "GSM2545336") %>%
  group_by(gene_biotype) %>%
  count(gene_biotype) %>%
  arrange(desc(n))
```

5. Identify genes associated with "abnormal DNA methylation" phenotype description,
   and calculate their mean expression (in log) at time 0, time 4 and time 8.

```{r, purl=FALSE}
rna %>%
  filter(phenotype_description == "abnormal DNA methylation") %>%
  group_by(gene, time) %>%
  summarize(mean_expression = mean(log(expression))) %>%
  arrange()
```


`r msmbstyle::solution_end()`


In the spreadsheet lesson (chapter \@ref(sec-dataorg)), we discussed
how to structure our data leading to the four rules defining a tidy
dataset:

## Reshaping data

In `rna`, the rows contain expression values (the unit) that are
associated with a combination of 2 other variables: `gene` and `sample`.
All the other columns correspond to variables describing either
the sample (age, sex, organism...) or the gene (gene_biotype, ENTREZ_ID, product...).
The variables that don’t change genes or samples will have the same value in all the rows.
This structure is called a `long-format`, as one column contains all the values,
and other column(s) list(s) the context of the value.

In certain cases, the `long-format` is not really "human-readable", and another format,
a `wide-format` is preferred, as a more compact way of representing the data.
This is typically the case with gene expression values that scientists are used to
look as matrices, were rows represent genes and columns represent samples.

To convert the gene expression values from `rna` into a wide-format,
we need to create a new table where each row (the units) is composed
of expression values associated with each gene.

In practical terms this means the values of the `sample` column in `rna` would
become the names of column variables, and the cells would contain the expression
values measured on each gene.

The key point here is that we are still following
a tidy data structure, but we have **reshaped** the data according to
the observations of interest: expression levels per gene instead
of recording them per gene and per sample.

With this new table, it would become therefore straightforward
to explore the relationship between the gene expression levels within, and
between, the samples.

The opposite transformation would be to transform column names into
values of a new variable.

We can do both these of transformations with two `tidyr` functions,
`pivot_longer()` and `pivot_wider()` (see
[here](https://tidyr.tidyverse.org/dev/articles/pivot.html) for
details).

### Pivoting the data into a wider format

Let's first select the 3 first columns of `rna` and use `pivot_wider()`
to transform data in a wide-format.

```{r, purl = FALSE}
rna_exp <- rna %>%
  select(gene, sample, expression)
rna_exp
```

`pivot_wider` takes three main arguments:

1. the data to be transformed;
2. the `names_from` column name whose values will become new column
   names;
3. the `values_from` column name whose values will fill the new
   columns.

![](./figs/pivot_wider.png)

Note also the `values_fill` argument which, if set, fills in missing
values with the value provided.





Using `pivot_wider()`, using new columns from the `sample` variable and
values from `expression`, this becomes 1474 gene expression measurements of 22
variables, one row for each gene, one column for each sample. We again use pipes:

```{r, purl = FALSE}
rna_wide <- rna_exp %>%
  pivot_wider(names_from = sample,
              values_from = expression)
rna_wide
```



![](./figs/pivot_wider.png)



We can now easily plot the comparisons between the gene expression levels
in different samples.

Note that the `pivot_wider()` function comes with an optional `values_fill` argument
that can be usefull when dealing with missing values.
Let's imagine that for some reason, we had some missing expression values for some
genes in certain samples. In the following fictive example, the gene Cyp2d22 has only
one expression value, in GSM2545338 sample.


```{r, purl = FALSE, echo = FALSE}
rna_with_missing_values <- rna %>%
  select(gene, sample, expression) %>%
  filter(gene %in% c("Asl", "Apod", "Cyp2d22")) %>%
  filter(sample %in% c("GSM2545336", "GSM2545337", "GSM2545338")) %>%
  arrange(sample) %>%
  filter(!(gene == "Cyp2d22" & sample != "GSM2545338"))
```

```{r, purl = FALSE}
rna_with_missing_values
```

By default, the `pivot_wider()` function will add `NA` for missing values.

```{r, purl = FALSE}
rna_with_missing_values %>%
  pivot_wider(names_from = sample,
              values_from = expression)
```

But in some cases, we may wish to fill in the missing values by setting `values_fill`
to a specific value.

```{r, purl = FALSE}
rna_with_missing_values %>%
  pivot_wider(names_from = sample,
              values_from = expression,
              values_fill = 0)
```



### Pivoting data into a longer format

The opposing situation could occur if we had been provided with data
in the form of `rna_wide`, where the sample IDs are column
names, but we wish to treat them as values of a `sample` variable
instead.

In this situation we are using the column names and turning them into
a pair of new variables. One variable represents the column names as
values, and the other variable contains the values previously
associated with the column names.

`pivot_longer()` takes four main arguments:

1. the data to be transformed;
2. the new `names_to` column we wish to create and populate with the
   current column names;
3. the new `values_to` column we wish to create and populate with
   current values;
4. the names of the columns to be used to populate the `names_to` and
   `values_to` variables (or to drop).

To recreate `rna_long` from `rna_long` we would create a key
called `sample` and value called `expression` and use all columns
except `gene` for the key variable. Here we drop `gene` column
with a minus sign.

Notice how the new variable names are to be quoted here.

```{r}
rna_long <- rna_wide %>%
    pivot_longer(names_to = "sample",
                 values_to = "expression",
                 -gene)
rna_long
```

![](./figs/pivot_longer.png)





Note that if we had missing values in the wide-format, the `NA` would be
included in the new wide format. Pivoting to wider and longer formats can
be a useful way to balance out a dataset so every replicate has the same composition.

```{r}
wide_with_NA <- rna_with_missing_values %>%
  pivot_wider(names_from = sample,
              values_from = expression)
wide_with_NA

wide_with_NA %>%
    pivot_longer(names_to = "sample",
                 values_to = "expression",
                 -gene)
```


We could also have used a specification for what columns to
include. This can be useful if you have a large number of identifying
columns, and it's easier to specify what to gather than what to leave
alone. Here the `starts_with()` function can help to retrieve sample
names without having to list them all!
Another possibility would be to use the `:` operator!

```{r}
rna_wide %>%
    pivot_longer(names_to = "sample",
                 values_to = "expression",
                 cols = starts_with("GSM"))
rna_wide %>%
    pivot_longer(names_to = "sample",
                 values_to = "expression",
                 GSM2545336:GSM2545380)
```


`r msmbstyle::question_begin()`

Subset genes located on X and Y chromosomes from the `rna` data frame and
spread the data frame with `sex` as columns, `chromosome_name` as
rows, and the mean expression of genes located in each chromosome as the values.

![](./figs/Exercise_pivot_W.png)

You will need to summarize before reshaping!

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

Let's have a look to variables of interest

```{r, answer=TRUE, purl=FALSE}
rna %>%
  filter(chromosome_name == "Y" | chromosome_name == "X") %>%
  select(gene, sample, sex, expression, chromosome_name) %>%
  arrange(gene)
```

```{r, answer=TRUE, purl=FALSE}
rna_1 <- rna %>%
  filter(chromosome_name == "Y" | chromosome_name == "X") %>%
  group_by(sex, chromosome_name) %>%
  summarize(mean = mean(expression)) %>%
  pivot_wider(names_from = sex,
              values_from = mean)

rna_1
```

`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Now take that data frame and transform it with `pivot_longer()` so
each row is a unique `chromosome_name` by `gender` combination.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r, answer=TRUE, purl=FALSE}
rna_1 %>%
  pivot_longer(names_to = "gender",
               values_to = "mean",
               - chromosome_name)

```

`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`

Use the `rna` dataset to create an expression matrix were each row represents
the mean expression levels of genes and columns represent the different timepoints.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
rna %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp)
```

Notice that this generates a tibble with some column names starting by a number.
If we wanted to select the column corresponding to the timepoints,
we could not use the column names directly... What happens when we select the colum 4?

```{r}
rna %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  select(gene, 4)
```

To select the timepoint 4, we would have to quote the column name, with backticks "`"

```{r}
rna %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  select(gene, `4`)
```

Another possibility would be to rename the column,
choosing a name that doesn't start by a number :

```{r}
rna %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  rename("time0" = `0`, "time4" = `4`, "time8" = `8`) %>%
  select(gene, time4)
```


`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Use the previous data frame containing mean expression levels per timepoint and create
a new column containing fold-changes between timepoint 8 and timepoint 0, and fold-changes
between timepoint 8 and timepoint 4.
Convert this table in a long-format table gathering the foldchanges calculated.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
rna_FC <- rna %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  mutate(time_8_vs_0 = `8` / `0`, time_8_vs_4 = `8` / `4`)
rna_FC
```

```{r}
rna_FC %>%
  pivot_longer(names_to = "comparisons",
               values_to = "Fold_changes",
               time_8_vs_0:time_8_vs_4)
```

`r msmbstyle::solution_end()`


## Exporting data

Now that you have learned how to use **`dplyr`** to extract information from
or summarize your raw data, you may want to export these new data sets to share
them with your collaborators or for archival.

Similar to the `read_csv()` function used for reading CSV files into R, there is
a `write_csv()` function that generates CSV files from data frames.

Before using `write_csv()`, we are going to create a new folder, `data_output`,
in our working directory that will store this generated dataset. We don't want
to write generated datasets in the same directory as our raw data. It's good
practice to keep them separate. The `data` folder should only contain the raw,
unaltered data, and should be left alone to make sure we don't delete or modify
it. In contrast, our script will generate the contents of the `data_output`
directory, so even if the files it contains are deleted, we can always
re-generate them.

In preparation for our next lesson on plotting, we are going to prepare a
table representing for each gene, the fold-changes (in log values) between timepoint 8
and timepoint 0, and the fold-changes between timepoint 8 and timepoint 0.

```{r}
rna_FC <- rna %>%
  mutate(expression_log = log(expression)) %>%
  group_by(gene, time) %>%
  summarize(mean_exp = mean(expression_log)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  mutate(time_8_vs_0 = `8` - `0`, time_4_vs_0 = `4` - `0`) %>%
  select(gene, time_8_vs_0, time_4_vs_0)
rna_FC
```

We can save the table as a CSV file in our `data_output`
folder.

```{r, purl=FALSE, eval=FALSE}
write_csv(rna_FC, file = "data_output/rna_FC.csv")
```

## Additional exercises

`r msmbstyle::question_begin()`

We are going to re-analyse beer consumption in 48 individuals using
`dplyr`. The data are available in the `rWSBIM1207` package. The data
illustrated the fictive beer consumption in litres per year at
different age according to gender and employment.

- Load the `rWSBIM1207` package. If the package isn't installed of its
  version is older than 0.1.1, install it from the
  `UCLouvain-CBIO/rWSBIM1207' GitHub repository using the
  `devtools::install_github` function.
- Directly load the data by typing
```{r, eval=FALSE}
data(beers)
```

- Remove observations with missing values.

- Using the `Year`, `Month` and `Year` columns, create a new column
  `Date` using `dplyr::mutate` and `lubridate::ymd`. What is the class
  of `Date` ?

- Create a new table, containing observations for women older than 35
  years old, employed, and select all columns except Day, Month and
  Year, and order in descending value of consumption of beers.

- Export the new table to a `csv` file.

Beer consumption analysis:

- Does employment status have an impact on beer consumption?
- Do men drink more than women?
- Does employment status have an influence on beer consumption according
  to gender?
- Do men drink more than women according to age and employment status?


`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
library("dplyr")
library("magrittr")
data("beers")
## Dealing with missing values
anyNA(beers)
beers2 <- beers %>%
    filter(!is.na(Consumption))
anyNA(beers2)
## New Date variable
library("lubridate")
beers2 <- beers2 %>%
    mutate(Date = ymd(paste(beers2$Year,
                        beers2$Month,
                        beers2$Day, sep = "-")))
class(beers2$Date)
## Filter, select and arrange
tab <- beers2 %>%
    filter(Age > 35) %>%
    filter(Gender == "Female") %>%
    filter(Work == "Employed") %>%
    select(-Day, -Month, -Year) %>%
    arrange(desc(Consumption))
tab
```

```{r, echo=FALSE, eval = FALSE}
write_csv(tab, "data_output/tab.csv")
## or
write.csv(tab, "data_output/tab.csv")
```


```{r, echo=FALSE, include=FALSE}
## Beer consumption analysis:
## - Does employment status have an impact on beer consumption?
beers2 %>%
  group_by(Work) %>%
  summarize(mean_consumption = mean(Consumption))
## - Do men drink more than women?
beers2 %>%
  group_by(Gender) %>%
  summarize(mean_consumption = mean(Consumption))
## - Does employmnet status have an influence on beer consumption
##   according to gender?
beers2 %>%
  group_by(Gender, Work) %>%
  summarize(mean_consumption = mean(Consumption))
## - Do men drink more than women according to age and employment
##   status?
beers3 <- beers2 %>%
  group_by(Gender, Age, Work) %>%
    summarize(mean_consumption = mean(Consumption))
beers3
```

As we can see from the last exercise, it become difficult to read and
interpret multiple results. In the next chapter, we will see how to
complement such analysis questions with visualisations such as the
following one, that clearly highlight important patterns in our data.


```{r, results='markup', fig.cap="Visualisation of beer consumption, highlighting different patterns of beer consumption in employed and unemployed males and females.", echo=FALSE, purl=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("./figs/ggplot-beer.png")
```

`r msmbstyle::question_begin()`

The Cancer Genome Atlas (TCGA) is a large scale effort that has
collected high throughput biology data from hundreds of patients
samples. In this exercise, we are going to analyse the clinical
variables recorded for a subset of the patients.

- Load the `rWSBIM1207` package. If the package isn't installed of its
  version is older than 0.1.1, install it from the
  `UCLouvain-CBIO/rWSBIM1207' GitHub repository using the
  `devtools::install_github` function.
- Using the `clinical1.csv()` function from `rWSBIM1207`, find the
  path the `clinical1.csv` file and read it to produce a `data.frame`
  named `clinical`.

- Familiarise yourself with the data.

- Create a smaller data frame called `clinical_mini` containing only
  the columns corresponding to `patientID`, `gender`,
  `age_at_diagnosis`, `smoking_history`, `number_pack_years_smoked`,
  `year_of_tobacco_smoking_onset`, and `stopped_smoking_year`.

- Calculate the number of males and females in the cohort.

- Create a new variable `years_at_diagnosis` corresponding to the age
  at diagnosis converted from days into years.

- Calculate the mean and median age at diagnosis (in years). Pay
  attention to missing values!

- Calculate the mean and median age at diagnosis for males and
  females.

- How many patient were diagnosed before 50 years?

- Compare the mean age at diagnosis between *current smoker* and
  *lifelong non-smoker*.

- Select patients who stopped smoking more than 15 years ago and
  calculate the number of smoking years for these cases. Display only
  cases for which you were able to calculate the data.

- How many of them smoked less than 5 years?

- Try to recreate the following table, reporting the number of smokers
  and lifelong-non smoker between males and females. Note: the layout
  can be different.

```{r, echo = FALSE, message = FALSE}
library("rWSBIM1207")
data(clinical_table_ex1)
knitr::kable(clinical_table_ex1)
```

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
library("readr")
library("dplyr")
library("magrittr")
clinical <- read_csv(clinical1.csv())
str(clinical)
## View(clinical)
clinical_mini <- clinical %>%
    select(patientID, gender, age_at_diagnosis,
           smoking_history, number_pack_years_smoked,
           year_of_tobacco_smoking_onset,
           stopped_smoking_year)
clinical_mini
## Calculate the number of males and females in the cohort.
clinical_mini %>%
    group_by(gender) %>%
    count()
## Create a new variable `years_at_diagnosis` corresponding to the age
## at diagnosis converted from days into years.
clinical_mini <- clinical_mini %>%
    mutate(years_at_diagnosis = age_at_diagnosis / 365)
select(clinical_mini,
       years_at_diagnosis,
       age_at_diagnosis)
## Calculate the mean and median age at diagnosis (in years). Pay
## attention to missing values!
clinical_mini %>%
    summarize(mean = mean(years_at_diagnosis, na.rm = TRUE),
              median = median(years_at_diagnosis, na.rm = TRUE))
## idem but removing NA values before
clinical_mini %>%
  filter(!is.na(years_at_diagnosis)) %>%
    summarise(mean = mean(years_at_diagnosis),
              median = median(years_at_diagnosis))
## Calculate the mean and median age at diagnosis for males and
## females
clinical_mini %>%
    group_by(gender) %>%
    summarise(mean = mean(years_at_diagnosis, na.rm = TRUE),
              median = median(years_at_diagnosis, na.rm = TRUE))
## How many patient were diagnosed before 50 years?
clinical_mini %>%
  filter(years_at_diagnosis < 50) %>%
  count()
## Compare the mean age at diagnosis between "current smoker" and
## "lifelong non-smoker"?
clinical_mini %>%
  filter(smoking_history == 'current smoker' | smoking_history == 'lifelong non-smoker') %>%
  group_by(smoking_history) %>%
  summarize(mean = mean(years_at_diagnosis, na.rm = TRUE))
## Select patients who stopped smoking more than 15 years ago and
## calculate the number of smoking years for these cases.
clinical_mini %>%
    filter(smoking_history == "current reformed smoker for > 15 years") %>%
    mutate(years_smoked = stopped_smoking_year - year_of_tobacco_smoking_onset) %>%
    filter(!is.na(years_smoked)) %>%
    select(patientID, years_smoked)
# How many of them smoked less than 5 years?
clinical_mini %>%
  filter(smoking_history == "current reformed smoker for > 15 years") %>%
  mutate(years_smoked = stopped_smoking_year - year_of_tobacco_smoking_onset) %>%
  filter(years_smoked < 5) %>%
  count()
## Try to recreate the following table, reporting the number of
## smokers and lifelong-non smoker between males and females.
clinical_mini %>%
    filter(smoking_history == 'current smoker' | smoking_history == 'lifelong non-smoker') %>%
    group_by(gender, smoking_history) %>%
    summarize(n = n()) %>%
    pivot_wider(names_from = "smoking_history",
                values_from = "n")
```



`r msmbstyle::question_begin()`

Using the `interroA.csv()` function from the `rWSBIM1207` package to
get the path to the spreadsheet file, read the data into R using the
`read_csv` function. This data is in the wide format, with the results
of each test stored as a separate column.

Using the appropriate pivot function, convert the data into a long
table with a column `interro` informing which test that line refers to
and a column `result` with the student's mark.

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library(rWSBIM1207)
x <- read_csv(interroA.csv())
x
pivot_longer(x,
             names_to = "interro",
             values_to = "result",
             c(interro1, interro2, interro3, interro4))
## or
pivot_longer(x,
             names_to = "interro",
             values_to = "result",
             -c(id, height, gender, X))
## or
pivot_longer(x,
             names_to = "interro",
             values_to = "result",
             starts_with("interro"))
```

<!--chapter:end:30-dplyr.Rmd-->

# Data visualization  {#sec-vis}

```{r vis_setup, echo=FALSE}
rna <- read.csv("data/rnaseq.csv")
```

**Learning Objectives**

* Produce scatter plots, boxplots, line plots, etc. using ggplot.
* Set universal plot settings.
* Describe what faceting is and apply faceting in ggplot.
* Modify the aesthetics of an existing ggplot plot (including axis
  labels and color).
* Build complex and customized plots from data in a data frame.


We start by loading the required packages. **`ggplot2`** is included
in the **`tidyverse`** package.

```{r load-package, message=FALSE}
library("tidyverse")
```

If not still in the workspace, load the data we saved in the previous lesson.


```{r load-data, eval=FALSE}
rna <- read.csv("data/rnaseq.csv")
```

The [Data Visualization Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)
will cover the basics and more advanced features of `ggplot2` and will
help, in addition to serve as a reminder, getting an overview of the
many data representations available in the package. The following
video tutorials ([part 1](https://www.youtube.com/watch?v=h29g21z0a68)
and [2](https://www.youtube.com/watch?v=0m4yywqNPVY)) by Thomas Lin
Pedersen are also very instructive.


## Plotting with **`ggplot2`**

**`ggplot2`** is a plotting package that makes it simple to create
complex plots from data in a data frame. It provides a more
programmatic interface for specifying what variables to plot, how they
are displayed, and general visual properties. The theoretical
foundation that supports the `ggplot2` is the *Grammar of Graphics*
(@Wilkinson:2005). Using this approach, we only need minimal changes
if the underlying data change or if we decide to change from a bar
plot to a scatterplot. This helps in creating publication quality
plots with minimal amounts of adjustments and tweaking.

There is a book about `ggplot2` (@ggplot2book) that provides a good
overview, but it is outdated. The 3rd edition is in preparation and
will be [freely available online](https://ggplot2-book.org/). The
`ggplot2` webpage
([https://ggplot2.tidyverse.org](https://ggplot2.tidyverse.org))
provides ample documentation.

`ggplot2` functions like data in the 'long' format, i.e., a column for
every dimension, and a row for every observation. Well-structured data
will save you lots of time when making figures with `ggplot2`.

ggplot graphics are built step by step by adding new elements. Adding
layers in this fashion allows for extensive flexibility and
customization of plots.

> The idea behind the Grammar of Graphics it is that you can build every 
graph from the same 3 components: (1) a data set, (2) a coordinate system,
and (3) geoms — i.e. visual marks that represent data points [^three_comp_ggplot2].

[^three_comp_ggplot2]: Source: [Data Visualization Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf).


To build a ggplot, we will use the following basic template that can
be used for different types of plots:

```
ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) +  <GEOM_FUNCTION>()
```

- use the `ggplot()` function and bind the plot to a specific data
  frame using the `data` argument

```{r, eval=FALSE}
ggplot(data = rna)
```

- define a mapping (using the aesthetic (`aes`) function), by
  selecting the variables to be plotted and specifying how to present
  them in the graph, e.g. as x/y positions or characteristics such as
  size, shape, color, etc.

```{r, eval=FALSE}
ggplot(data = rna, mapping = aes(x = expression))
```

- add 'geoms' – geometries, or graphical representations of the data
  in the plot (points, lines, bars). **`ggplot2`** offers many
  different geoms; we will use some common ones today, including:

      * `geom_point()` for scatter plots, dot plots, etc.
      * `geom_histogram()` for histograms
      * `geom_boxplot()` for, well, boxplots!
      * `geom_line()` for trend lines, time series, etc.


To add a geom(etry) to the plot use the `+` operator. Let's use `geom_histogram()` first:

```{r first-ggplot, cache=FALSE}
ggplot(data = rna, mapping = aes(x = expression)) +
  geom_histogram()
```

The `+` in the **`ggplot2`** package is particularly useful because it
allows you to modify existing `ggplot` objects. This means you can
easily set up plot templates and conveniently explore different types
of plots, so the above plot can also be generated with code like this:

```{r, first-ggplot-with-plus, eval=FALSE}
# Assign plot to a variable
rna_plot <- ggplot(data = rna,
                   mapping = aes(x = expression))

# Draw the plot
rna_plot + geom_histogram()
```

`r msmbstyle::question_begin()`

You have probably noticed an automatic message that appears when
drawing the histogram:

```{r, echo=FALSE, fig.show='hide'}
ggplot(rna, aes(x = expression)) +
  geom_histogram()
```

Change the arguments `bins` or `binwidth` of `geom_histogram()` to
change the number or width of the bins.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r}
# change bins
ggplot(rna, aes(x = expression)) +
    geom_histogram(bins = 15)

# change binwidth
ggplot(rna, aes(x = expression)) +
    geom_histogram(binwidth = 2000)
```

`r msmbstyle::solution_end()`

We can observe here that the data are skewed to the right. We can
apply log2 transformation to have a more symmetric distribution. Note
that we add here a small constant value (`+1`) to avoid having `-Inf`
values returned for expression values equal to 0.


```{r log-transfo, cache=FALSE}
rna <- rna %>%
  mutate(expression_log = log2(expression + 1))
```

If we now draw the histogram of the log2-transformed expressions, the
distribution is indeed closer to a normal distribution.

```{r second-ggplot, cache=FALSE}
ggplot(rna, aes(x = expression_log)) + geom_histogram()
```

From now on we will work on the log-transformed expression values.

`r msmbstyle::question_begin()`

Another way to visualize this transformation is to consider the scale
of the observations. For example, it may be worth changing the scale
of the axis to better distribute the observations in the space of the
plot. Changing the scale of the axes is done similarly to
adding/modifying other components (i.e., by incrementally adding
commands). Try making this modification:

* Represent the the un-transformed expression on the log10 scale; see
  `scale_x_log10()`. Compare it with the previous graph. Why do you
  now have warning messages appearing?

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r, eval=TRUE, purl=TRUE, echo=TRUE}
ggplot(data = rna,mapping = aes(x = expression))+
  geom_histogram() +
  scale_x_log10()
```
`r msmbstyle::solution_end()`


**Notes**

- Anything you put in the `ggplot()` function can be seen by any geom
  layers that you add (i.e., these are global plot settings). This
  includes the x- and y-axis mapping you set up in `aes()`.
- You can also specify mappings for a given geom independently of the
  mappings defined globally in the `ggplot()` function.
- The `+` sign used to add new layers must be placed at the end of the
  line containing the *previous* layer. If, instead, the `+` sign is
  added at the beginning of the line containing the new layer,
  **`ggplot2`** will not add the new layer and will return an error
  message.


```{r, ggplot-with-plus-position, eval=FALSE}
# This is the correct syntax for adding layers
rna_plot +
  geom_histogram()

# This will not add the new layer and will return an error message
rna_plot
  + geom_histogram()
```


## Building your plots iteratively

We will now draw a scatter plot with two continuous variables and the
`geom_point()` function. This graph will represent the log2 fold
changes of expression values comparing time 8 versus time 0 and time 4 versus time 0. To
this end, we first need to compute the means of the log-transformed
expression values by gene and time then the log fold changes by
subtracting the mean log expressions between time 8 and time 0 and
between time 4 and time 0. Note that we also include here the gene
biotype that we will use later on to represent the genes. We will save the fold changes in a new data frame called `rna_fc.`

```{r rna_fc, cache=FALSE}
rna_fc <- rna %>% select(gene, time,
                         gene_biotype, expression_log) %>%
  group_by(gene, time, gene_biotype) %>%
  summarize(mean_exp = mean(expression_log)) %>%
  pivot_wider(names_from = time,
              values_from = mean_exp) %>%
  mutate(time_8_vs_0 = `8` - `0`, time_4_vs_0 = `4` - `0`)
```

We can then build a ggplot with the newly created dataset
`rna_fc`. Building plots with `ggplot2` is typically an iterative
process. We start by defining the dataset we'll use, lay out the axes,
and choose a geom:

```{r create-ggplot-object, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0)) +
    geom_point()
```

Then, we start modifying this plot to extract more information from
it. For instance, we can add transparency (`alpha`) to avoid
overplotting:

```{r adding-transparency, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0)) +
    geom_point(alpha = 0.3)
```

We can also add colors for all the points:

```{r adding-colors, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0)) +
    geom_point(alpha = 0.3, color = "blue")
```

Or to color each gene in the plot differently, you could use a vector
as an input to the argument **color**. **`ggplot2`** will provide a
different color corresponding to different values in the vector. Here
is an example where we color with **`gene_biotype`**:


```{r color-by-gene_biotype1, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0)) +
    geom_point(alpha = 0.3,
               aes(color = gene_biotype))

```

We can also specify the colors directly inside the mapping provided in
the `ggplot()` function. This will be seen by any geom layers and the
mapping will be determined by the x- and y-axis set up in `aes()`.

```{r color-by-gene_biotype2, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0,
                     color = gene_biotype)) +
    geom_point(alpha = 0.3)
```

Finally, we could also add a diagonal line with the `geom_abline()`
function:

```{r adding-diag, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0,
                     color = gene_biotype)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0)
```

Notice that we can change the geom layer and colors will be still
determined by **`gene_biotype`**

```{r color-by-gene_biotype3, cache=FALSE}
ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0,
                     color = gene_biotype)) +
    geom_jitter(alpha = 0.3) +
    geom_abline(intercept = 0)
```

`r msmbstyle::question_begin()`

Scatter plots can be useful exploratory tools for small datasets. For
data sets with large numbers of observations, such as the `rna_fc`
data set, overplotting of points can be a limitation of scatter
plots. One strategy for handling such settings is to use hexagonal
binning of observations. The plot space is tessellated into
hexagons. Each hexagon is assigned a color based on the number of
observations that fall within its boundaries.

- To use hexagonal binning in `ggplot2`, first install the R package
  `hexbin` from CRAN and load it.

- Then use the `geom_hex()` function to produce the hexbin figure.

- What are the relative strengths and weaknesses of a hexagonal bin
  plot compared to a scatter plot? Examine the above scatter plot and
  compare it with the hexagonal bin plot that you created.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r, eval = FALSE}
install.packages("hexbin")
```

```{r}
library("hexbin")

ggplot(data = rna_fc,
       mapping = aes(x = time_4_vs_0,
                     y = time_8_vs_0)) +
    geom_hex() +
    geom_abline(intercept = 0)

```

`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`

Use what you just learned to create a scatter plot of `expression_log`
over `sample` from the `rna` dataset with the time showing in
different colors. Is this a good way to show this type of data?

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`

```{r, eval = TRUE}
ggplot(data = rna,
       mapping = aes(y = expression_log,
                     x = sample)) +
    geom_point(aes(color = time))
```

`r msmbstyle::solution_end()`

## Boxplot

We can use boxplots to visualize the distribution of gene expressions
within each sample:

```{r boxplot, cache=FALSE}
ggplot(data = rna,
       mapping = aes(y = expression_log, x = sample)) +
    geom_boxplot()
```

By adding points to boxplot, we can have a better idea of the number of
measurements and of their distribution:

```{r boxplot-with-points, cache=FALSE}
ggplot(data = rna,
       mapping = aes(y = expression_log,
                     x = sample)) +
    geom_jitter(alpha = 0.2, color = "tomato") +
    geom_boxplot(alpha = 0)
```

`r msmbstyle::question_begin()`
Note how the boxplot layer is in front of the jitter layer? What do
you need to change in the code to put the boxplot below the points?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
We should switch the order of these two geoms:

```{r boxplot-with-points2, cache=FALSE}
ggplot(data = rna,
         mapping = aes(y = expression_log, x = sample)) +
  geom_boxplot(alpha = 0) +
  geom_jitter(alpha = 0.2, color = "tomato") 
```
`r msmbstyle::solution_end()`


You may notice that the values on the x-axis are still not properly
readable. Let’s change the orientation of the labels and adjust them
vertically and horizontally so they don’t overlap. You can use a
90-degree angle, or experiment to find the appropriate angle for
diagonally oriented labels:

```{r boxplot-xaxis-rotated, cache=FALSE}
ggplot(data = rna,
       mapping = aes(y = expression_log,
                     x = sample)) +
    geom_jitter(alpha = 0.2, color = "tomato") +
    geom_boxplot(alpha = 0) +
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))
```

`r msmbstyle::question_begin()`

Add color to the data points on your boxplot according to the duration
of the infection (`time`).

*Hint:* Check the class for `time`. Consider changing the class of
`time` from integer to factor directly in the ggplot mapping. Why does this change how R makes the graph?

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r boxplot-color-time, cache=FALSE}
# time as integer
ggplot(data = rna,
         mapping = aes(y = expression_log,
                       x = sample)) +
  geom_jitter(alpha = 0.2, aes(color = time)) +
  geom_boxplot(alpha = 0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))

# time as factor
ggplot(data = rna,
         mapping = aes(y = expression_log,
                       x = sample)) +
  geom_jitter(alpha = 0.2, aes(color = as.factor(time))) +
  geom_boxplot(alpha = 0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))
```


`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`

Boxplots are useful summaries, but hide the *shape* of the
distribution. For example, if the distribution is bimodal, we would
not see it in a boxplot. An alternative to the boxplot is the violin
plot, where the shape (of the density of points) is drawn.

- Replace the box plot with a violin plot; see `geom_violin()`. Fill in
the violins according to the time with the argument `fill` in `geom_violin()`.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r, eval=TRUE, echo=TRUE, cache=FALSE}
ggplot(data = rna,
         mapping = aes(y = expression_log,
                       x = sample)) +
    geom_violin(aes(fill = as.factor(time))) +
    theme(axis.text.x = element_text(angle = 90,  hjust = 0.5, vjust = 0.5))
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
- Modify the violin plot to fill in the violins by `sex`.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r, eval=TRUE, echo=TRUE, cache=FALSE}
ggplot(data = rna,
       mapping = aes(y = expression_log,
                     x = sample)) +
    geom_violin(aes(fill = sex)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))
```
`r msmbstyle::solution_end()`

## Line plots

Let's calculate the mean expression per duration of the infection for
the 10 genes having the highest log fold changes comparing time 8
versus time 0.

First, we need to select the genes and create a subset of `rna` called
`sub_rna` containing the 10 selected genes, then we need to group the
data and calculate the mean gene expression within each group:

```{r}
rna_fc <- rna_fc %>%
    arrange(desc(time_8_vs_0))

genes_selected <- rna_fc$gene[1:10]

sub_rna <- rna %>%
    filter(gene %in% genes_selected)

mean_exp_by_time <- sub_rna %>%
    group_by(gene,time) %>%
    summarize(mean_exp = mean(expression_log))
```

We can build the line plot with duration of the infection on the
x-axis and the mean expression on the y-axis:

```{r first-time-series}
ggplot(data = mean_exp_by_time,
       mapping = aes(x = time, y = mean_exp)) +
    geom_line()
```

Unfortunately, this does not work because we plotted data for all the
genes together. We need to tell ggplot to draw a line for each gene by
modifying the aesthetic function to include `group = gene`:

```{r time-series-by-gene}
ggplot(data = mean_exp_by_time,
       mapping = aes(x = time,y = mean_exp,
                     group = gene)) +
    geom_line()
```

We will be able to distinguish genes in the plot if we add colors
(using `color` also automatically groups the data):

```{r time-series-with-colors}
ggplot(data = mean_exp_by_time,
       mapping = aes(x = time, y = mean_exp,
                     color = gene)) +
  geom_line()
```

## Faceting

`ggplot2` has a special technique called *faceting* that allows the
user to split one plot into multiple (sub) plots based on a factor
included in the dataset. These different subplots inherit the same
properties (axes limits, ticks, ...) to facilitate their direct
comparison. We will use it to make a line plot across time for each
gene:

```{r first-facet}
ggplot(data = mean_exp_by_time,
       mapping = aes(x = time, y = mean_exp)) +
    geom_line() +
    facet_wrap(~ gene)
```

Here both x- and y-axis have the same scale for all the sub plots. You
can change this default behavior by modifying `scales` in order to
allow a free scale for the y-axis:

```{r first-facet-scales}
ggplot(data = mean_exp_by_time,
       mapping = aes(x = time,
                     y = mean_exp)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y")
```

Now we would like to split the line in each plot by the sex of the
mice. To do that we need to calculate the mean expression in the data frame
grouped by `gene`, `time`, and `sex`:

```{r data-facet-by-gene-and-sex}
mean_exp_by_time_sex <- sub_rna %>%
  group_by(gene, time, sex) %>%
  summarize(mean_exp = mean(expression_log))
```

We can now make the faceted plot by splitting further by sex using
`color` (within a single plot):

```{r facet-by-gene-and-sex, cache=FALSE}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y")
```

Usually plots with white background look more readable when printed.
We can set the background to white using the function
`theme_bw()`. Additionally, we can remove the grid:

```{r facet-by-gene-and-sex-white-bg, cache=FALSE}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y") +
    theme_bw() +
    theme(panel.grid = element_blank())
```

`r msmbstyle::question_begin()`

Use what you just learned to create a plot that depicts how the
average expression of each chromosome changes through the duration of
infection.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r mean-exp-chromosome-time-series}
mean_exp_by_chromosome <- rna %>%
  group_by(chromosome_name, time) %>%
  summarize(mean_exp = mean(expression_log))

ggplot(data = mean_exp_by_chromosome,
       mapping = aes(x = time,
                     y = mean_exp)) +
    geom_line() +
    facet_wrap(~ chromosome_name, scales = "free_y")
```

`r msmbstyle::solution_end()`


The `facet_wrap` geometry extracts plots into an arbitrary number of
dimensions to allow them to cleanly fit on one page. On the other
hand, the `facet_grid` geometry allows you to explicitly specify how
you want your plots to be arranged via formula notation (`rows ~
columns`; a `.` can be used as a placeholder that indicates only one
row or column).

Let's modify the previous plot to compare how the mean gene expression
of males and females has changed through time:

```{r mean-exp-time-facet-sex-rows}
# One column, facet by rows
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = gene)) +
    geom_line() +
    facet_grid(sex ~ .)
```

```{r mean-exp-time-facet-sex-columns}
# One row, facet by column
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = gene)) +
    geom_line() +
    facet_grid(. ~ sex)
```

## **`ggplot2`** themes

In addition to `theme_bw()`, which changes the plot background to
white, **`ggplot2`** comes with several other themes which can be
useful to quickly change the look of your visualization. The complete
list of themes is available at
<http://docs.ggplot2.org/current/ggtheme.html>. `theme_minimal()` and
`theme_light()` are popular, and `theme_void()` can be useful as a
starting point to create a new hand-crafted theme.

The [ggthemes](https://jrnold.github.io/ggthemes/reference/index.html)
package provides a wide variety of options (including an Excel 2003
theme).  The [**`ggplot2`** extensions
website](https://www.ggplot2-exts.org) provides a list of packages
that extend the capabilities of **`ggplot2`**, including additional
themes.


## Customisation

Let's come back to the faceted plot of mean expression by time and gene, colored by sex.

Take a look at the [**`ggplot2`** cheat
sheet](https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf),
and think of ways you could improve the plot.

Now, we can change names of axes to something more informative than 'time'
and 'mean_exp', and add a title to the figure:

```{r mean_exp-time-with-right-labels, cache=FALSE}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y") +
    theme_bw() +
    theme(panel.grid = element_blank()) +
    labs(title = "Mean gene expression by duration of the infection",
         x = "Duration of the infection (in days)",
         y = "Mean gene expression")

```

The axes have more informative names, but their readability can be
improved by increasing the font size:

```{r mean_exp-time-with-right-labels-xfont-size, cache=FALSE}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y") +
    theme_bw() +
    theme(panel.grid = element_blank()) +
    labs(title = "Mean gene expression by duration of the infection",
       x = "Duration of the infection (in days)",
       y = "Mean gene expression")  +
    theme(text = element_text(size = 16))
```

Note that it is also possible to change the fonts of your plots. If
you are on Windows, you may have to install the [**`extrafont`**
package](https://github.com/wch/extrafont), and follow the
instructions included in the README for this package.

We can further customize the color of x- and y-axis text, the color of
the grid, etc. We can also for example move the legend to the top by
setting `legend.position` to `"top"`.

```{r mean_exp-time-with-theme, cache=FALSE}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time,
                     y = mean_exp,
                     color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y") +
    theme_bw() +
    theme(panel.grid = element_blank()) +
    labs(title = "Mean gene expression by duration of the infection",
       x = "Duration of the infection (in days)",
       y = "Mean gene expression")  +
    theme(text = element_text(size = 16),
          axis.text.x = element_text(colour = "royalblue4", size = 12),
          axis.text.y = element_text(colour = "royalblue4", size = 12),
          panel.grid = element_line(colour="lightsteelblue1"),
          legend.position = "top")
```

If you like the changes you created better than the default theme, you
can save them as an object to be able to easily apply them to other
plots you may create. Here is an example with the histogram we have previously created.

```{r mean_exp-time-with-right-labels-xfont, cache=FALSE}
blue_theme <-
    theme(axis.text.x = element_text(colour = "royalblue4",
                                     size = 12),
          axis.text.y = element_text(colour = "royalblue4",
                                     size = 12),
          text = element_text(size = 16),
          panel.grid = element_line(colour="lightsteelblue1"))

ggplot(rna, aes(x = expression_log)) +
    geom_histogram(bins = 20) +
    blue_theme
```

`r msmbstyle::question_begin()`

With all of this information in hand, please take another five minutes
to either improve one of the plots generated in this exercise or
create a beautiful graph of your own. Use the RStudio [`ggplot2` cheat
sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)
for inspiration. Here are some ideas:

- See if you can change the thickness of the lines.
- Can you find a way to change the name of the legend? What about its labels? (hint: look for a ggplot function starting with `scale_`)
- Try using a different color palette or manually specifying the colors for the lines (see
  [http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/)).

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
For example, based on this plot:

```{r}
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time, y = mean_exp, color = sex)) +
  geom_line() +
  facet_wrap(~ gene, scales = "free_y") +
  theme_bw() +
  theme(panel.grid = element_blank())
```


We can customize it the following ways:
```{r}
# change the thickness of the lines
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time, y = mean_exp, color = sex)) +
  geom_line(size=1.5) +
  facet_wrap(~ gene, scales = "free_y") +
  theme_bw() +
  theme(panel.grid = element_blank())
  
# change the name of the legend and the labels
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time, y = mean_exp, color = sex)) +
  geom_line() +
  facet_wrap(~ gene, scales = "free_y") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_color_discrete(name = "Gender", labels = c("F", "M"))
# using a different color palette
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time, y = mean_exp, color = sex)) +
  geom_line() +
  facet_wrap(~ gene, scales = "free_y") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_color_brewer(name = "Gender", labels = c("F", "M"), palette = "Dark2")
# manually specifying the colors
ggplot(data = mean_exp_by_time_sex,
       mapping = aes(x = time, y = mean_exp, color = sex)) +
  geom_line() +
  facet_wrap(~ gene, scales = "free_y") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  scale_color_manual(name = "Gender",  labels = c("F", "M"), 
                     values = c("royalblue", "deeppink"))
  
```
`r msmbstyle::solution_end()`


## Composing plots

Faceting is a great tool for splitting one plot into multiple
subplots, but sometimes you may want to produce a single figure that
contains multiple independent plots, i.e. plots that are based on
different variables or even different data frames.

Let's start by creating the two plots that we want to arrange next to
each other:

The first graph counts the number of unique genes per chromosome. We
first need to reorder the levels of `chromosome_name` and filter the
unique genes per chromosome. We also change the scale of the y-axis to
a log10 scale for better readability.


```{r sub1}
rna$chromosome_name <- factor(rna$chromosome_name,
                              levels = c(1:19,"X","Y"))

count_gene_chromosome <- rna %>%
    select(chromosome_name, gene) %>%
    distinct() %>%
    ggplot() +
    geom_bar(aes(x = chromosome_name),
             fill = "seagreen",
             position = "dodge",
             stat = "count") +
    labs(y = "log10(n genes)",
         x = "chromosome") +
    scale_y_log10()

count_gene_chromosome
```

Below, we also remove the legend altogether by setting the
`legend.position` to `"none"`.

```{r sub2}
exp_boxplot_sex <- ggplot(rna,
                          aes(y = expression_log,
                              x = as.factor(time),
                              color = sex)) +
    geom_boxplot(alpha = 0) +
    labs(y = "Mean gene exp",
         x = "time") +
    theme(legend.position = "none")

exp_boxplot_sex
```

The [**patchwork**](https://github.com/thomasp85/patchwork) package
provides an elegant approach to combining figures using the `+` to
arrange figures (typically side by side). More specifically the `|`
explicitly arranges them side by side and `/` stacks them on top of
each other.

```{r install-patchwork, message=FALSE, eval=FALSE}
install.packages("patchwork")
```

```{r}
library("patchwork")
count_gene_chromosome + exp_boxplot_sex ## or use | instead of +
```

```{r}
count_gene_chromosome / exp_boxplot_sex
```

We can combine further control the layout of the final composition
with `plot_layout` to create more complex layouts:

```{r}
count_gene_chromosome + exp_boxplot_sex + plot_layout(ncol = 1)
```

```{r}
count_gene_chromosome +
    (count_gene_chromosome + exp_boxplot_sex) +
    exp_boxplot_sex +
    plot_layout(ncol = 1)
```

The last plot can also be created using the `|` and `/` composers:

```{r}
count_gene_chromosome /
    (count_gene_chromosome | exp_boxplot_sex) /
    exp_boxplot_sex
```

Learn more about `patchwork` on its
[webpage](https://patchwork.data-imaginist.com/) or in this
[video](https://www.youtube.com/watch?v=0m4yywqNPVY).

Another option is the **`gridExtra`** package that allows to combine
separate ggplots into a single figure using `grid.arrange()`:

```{r install-gridextra, message=FALSE, eval=FALSE}
install.packages("gridExtra")
```

```{r gridarrange-example, message=FALSE, fig.width=10}
library(gridExtra)
grid.arrange(count_gene_chromosome, exp_boxplot_sex, ncol = 2)
```

In addition to the `ncol` and `nrow` arguments, used to make simple
arrangements, there are tools for [constructing more complex
layouts](https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html).

## Exporting plots

After creating your plot, you can save it to a file in your favorite
format. The Export tab in the **Plot** pane in RStudio will save your
plots at low resolution, which will not be accepted by many journals
and will not scale well for posters.

Instead, use the `ggsave()` function, which allows you easily change
the dimension and resolution of your plot by adjusting the appropriate
arguments (`width`, `height` and `dpi`).

Make sure you have the `fig_output/` folder in your working directory.

```{r ggsave-example, eval=FALSE}
my_plot <- ggplot(data = mean_exp_by_time_sex,
                  mapping = aes(x = time,
                                y = mean_exp,
                                color = sex)) +
    geom_line() +
    facet_wrap(~ gene, scales = "free_y") +
    labs(title = "Mean gene expression by duration of the infection",
         x = "Duration of the infection (in days)",
         y = "Mean gene expression") +
    guides(color=guide_legend(title="Gender")) +
    theme_bw() +
    theme(axis.text.x = element_text(colour = "royalblue4", size = 12),
          axis.text.y = element_text(colour = "royalblue4", size = 12),
          text = element_text(size = 16),
          panel.grid = element_line(colour="lightsteelblue1"),
          legend.position = "top")
ggsave("fig_output/mean_exp_by_time_sex.png", my_plot, width = 15,
       height = 10)

# This also works for grid.arrange() plots
combo_plot <- grid.arrange(count_gene_chromosome, exp_boxplot_sex,
                           ncol = 2, widths = c(4, 6))
ggsave("fig_output/combo_plot_chromosome_sex.png", combo_plot,
       width = 10, dpi = 300)
```

Note: The parameters `width` and `height` also determine the font size
in the saved plot.


```{r final-challenge, eval=FALSE, purl=TRUE, echo=FALSE}
### Final plotting challenge:
##  With all of this information in hand, please take another five
##  minutes to either improve one of the plots generated in this
##  exercise or create a beautiful graph of your own. Use the RStudio
##  ggplot2 cheat sheet for inspiration:
##  https://www.rstudio.com/wp-content/uploads/2015/08/ggplot2-cheatsheet.pdf
```

## Other packages for visualisation

`ggplot2` is a very powerful package that fits very nicely in our
*tidy data* and *tidy tools* pipeline. There are other visualisation
packages in R that shouldn't be ignored.

### Base graphics {-}

The default graphics system that comes with R, often called *base R
graphics* is simple and fast. It is based on the *painter's or canvas
model*, where different output are directly overlaid on top of each
other (see figure \@ref(fig:paintermodel)). This is a fundamental
difference with `ggplot2` (and with `lattice`, described below), that
returns dedicated objects, that are rendered on screen or in a file,
and that can even be updated.

```{r paintermodel, fig.width = 12, fig.height = 4, fig.cap = 'Successive layers added on top of each other.'}
par(mfrow = c(1, 3))
plot(1:20, main = "First layer, produced with plot(1:20)")
plot(1:20, main = "A horizontal red line, added with abline(h = 10)")
abline(h = 10, col = "red")
plot(1:20, main = "A rectangle, added with rect(5, 5, 15, 15)")
abline(h = 10, col = "red")
rect(5, 5, 15, 15, lwd = 3)
```

Another main difference is that base graphics' plotting function try
to do *the right* thing based on their input type, i.e. they will
adapt their behaviour based on the class of their input. This is again
very different from what we have in `ggplot2`, that only accepts
dataframes as input, and that requires plots to be constructed bit by
bit.

```{r plotmethod, fig.width = 8, fig.height = 8, fig.cap = 'Plotting boxplots (top) and histograms (bottom) vectors (left) or a matrices (right).'}
par(mfrow = c(2, 2))
boxplot(rnorm(100),
        main = "Boxplot of rnorm(100)")
boxplot(matrix(rnorm(100), ncol = 10),
        main = "Boxplot of matrix(rnorm(100), ncol = 10)")
hist(rnorm(100))
hist(matrix(rnorm(100), ncol = 10))
```

The out-of-the-box approach in base graphics can be very efficient for
simple, standard figures, that can be produced very quickly with a
single line of code and a single function such as `plot`, or `hist`,
or `boxplot`, ... The defaults are however not always the most
appealing and tuning of figures, especially when they become more
complex (for example to produce facets), can become lengthy and
cumbersome.

### The lattice package {-}

The `lattice` package is similar to `ggplot2` in that is uses
dataframes as input, returns graphical objects and supports
faceting. `lattice` however isn't based on the grammar of graphics
and has a more convoluted interface.

A good reference for the `lattice` package is @latticebook.

## Additional exercises

`r msmbstyle::question_begin()`

Load the beer consuption data with

```{r}
library("rWSBIM1207")
data(beers)
```

Analyse the data to answer the question *Do men drink more than women
according to age and working status?* Now reproduce the figure below.

```{r, results='markup', fig.cap="Visualisation of beer consumption, highlighting different patterns of beer consumption in employed and unemployed males and females.", echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("./figs/ggplot-beer.png")
```
`r msmbstyle::question_end()`

```{r, echo=FALSE, include=FALSE}
library("dplyr")
beers3 <- beers %>%
    filter(!is.na(Consumption)) %>%
    group_by(Gender, Age, Work) %>%
    summarize(mean_consumption = mean(Consumption))

library("ggplot2")
ggplot(data = beers3,
       mapping = aes(x = Age, y = mean_consumption, colour = Work)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ Gender) +
  labs(title = "Mean consumption of beers according to age, employment status and gender",
       y = "Mean consumption (Litre)")
```

`r msmbstyle::question_begin()`

We are now going to analyse transcriptomics data from the TCGA
project.

- Using the file returned by the `expression.csv()` function from the
  `rWSBIM1207` package, create a table called `expression`.

- Inspect the data.

The `sampleID` column gives the TCGA reference of the sample and is
unique. The 12 first characters (TCGA-XX-XXXX) of the samples ID are
unique to each patient. The 3 last characters of the samples ID
indicate the type of sample: samples ending with '-01A'
(TCGA-XX-XXXX-01A) correspond to tumors and samples ending with '-11A'
(TCGA-XX-XXXX-11A) correspond to normal peritumoral tissues.

The `patient` column gives the reference of each patient. Note that
some patient for which both tumor and normal tissue were analysed are
recorded twice. The `type` column gives the nature of each sample
(tumor or normal tissue). The 5 next column give the expression levels
of 5 genes in each sample.

- Using geom_point, draw a plot showing distribution of expression
  levels of A2LD1 in normal tissue samples and in primary tumor
  samples.

- Repeat this visualisation using this time the geom_jitter. Which
  representation is more appropriate? Why?

- Colour the samples according to the tissue type.

- Colour the samples according to their expression level in A2ML1.

- Highlight the points corresponding to patient "TCGA-73-4676".

- Add a transparent boxplot to the graph.

- Change the y scale to log10 scale.

- Visualise the expression of A1BG against that of A2LD1 setting the x
  axis on the log10 scale. Colours the observations based on their
  type and resize the points according to the expression level of the
  A2ML1 gene.

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
library("readr")
library("ggplot2")
expression <- read_csv(expression.csv())

## Using geom_point, draw a plot showing distribution of expression
## levels of A2LD1 in normal tissue samples and in primary tumor samples
ggplot(data = expression,
       mapping = aes(x = type, y = A2LD1)) +
  geom_point()

## Repeat this visualisation using this time the geom_jitter. Which
## representation is more appropriate? Why?
ggplot(data = expression,
       mapping = aes(x = type, y = A2LD1)) +
    geom_jitter()

## Colour the samples according to the tissue type.
ggplot(data = expression,
       mapping = aes(x = type, y = A2LD1, colour = type)) +
  geom_jitter()


## Colour the samples according to their expression level in A2ML1.
ggplot(data = expression,
       mapping = aes(x = type, y = A2LD1, colour = A2ML1)) +
  geom_jitter()

## Highlight the points corresponding to patient "TCGA-73-4676"
ggplot(data = expression,
       mapping = aes(x = type, y = A2LD1,
                     colour = patient == "TCGA-73-4676")) +
  geom_jitter()

## Add a transparent boxplot to the graph.
ggplot(data = expression, mapping = aes(x = type, y = A2LD1)) +
  geom_jitter(aes(colour = type)) +
  geom_boxplot(alpha = 0)

## Change the y scale to log10 scale.
ggplot(data = expression, mapping = aes(x = type, y = A2LD1)) +
  geom_jitter(aes(colour = type)) +
  geom_boxplot(alpha = 0) +
    scale_y_log10()

## Visualise the expression of A1BG against that of A2LD1 setting the
## x axis on the log10 scale. Colours the observations based on their
## type and resize the points according to the expression level of the
## A2ML1 gene.
ggplot(data = expression,
       mapping = aes(x = A1BG, y = A2LD1,
                     colour = type,
                     size = A2ML1)) +
    geom_point() +
    scale_x_log10()

```

`r msmbstyle::question_begin()`

After gathering the *interroA* data from the `rWSBIM1207` package in a
long table format (see additional exercise chapter \@ref(sec-dplyr)),
visualise the result distributions for each test and male/female
students group.

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
x <- read_csv(interroA.csv()) %>%
    pivot_longer(names_to = "interro",
                 values_to = "result",
                 starts_with("interro"))

ggplot(x, aes(x = gender, y = result)) +
    geom_boxplot() +
    facet_grid(. ~ interro)
```

<!--chapter:end:40-visualization.Rmd-->

# Joining tables {#sec-join}

**Learning Objectives**

At the end of this section, students should understand

* the need and concept of table joins,
* different between different types of joins,
* the importance of keys in joins,
* circumstances leading to the appearance of missing values,
* the implications of using non-unique keys.

In many real life situations, data are spread across multiple tables
or spreadsheets. Usually this occurs because different types of
information about a subject, e.g. a patient, are collected from
different sources. It may be desirable for some analyses to combine
data from two or more tables into a single data frame based on a
common column, for example, an attribute that uniquely identifies the
subject.

The `dplyr` package, that we have already used extensively, provides a
set of join functions for combining two data frames based on matches
within specified columns.

For further reading, please refer to the chapter about [table
joins](https://r4ds.had.co.nz/relational-data.html#understanding-joins)
in @r4ds:2017.

```{r cleanup, echo = FALSE}
rm(list = ls())
```

The [Data Transformation Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)
also provides a short overview on table joins.


## Combining tables

We are going to illustrate join using a common example from the
bioinformatics world, where annotations about genes are scattered in
different tables that have one or more shared columns. The data we are
going to use are available in the course package and can be loaded as
shown below.

```{r joindata}
library("rWSBIM1207")
data(jdf)
```

The example data is composed of pairs of tables (we have tibbles here,
but this would equally work with dataframes). The first member of the
pair contains protein [UniProt](https://www.uniprot.org/)[^up] unique
accession number (`uniprot` variable), the most likely sub-cellular
localisation of these respective proteins (`organelle` variable) as
well as the proteins identifier (`entry`).

[^up]: UniProt is the protein information database. Its mission is to *provide the scientific community with a comprehensive, high-quality and freely accessible resource of protein sequence and functional information*.

```{r jdf1}
jdf1
```

The second table contains the name of the gene that codes for the
protein (`gene_name` variable), a description of the gene
(`description` variable), the uniprot accession number (this is the
common variable that can be used to join tables) and the species the
protein information comes from (`organism` variable).

```{r jdf2}
jdf2
```

We now want to join these two tables into a single one containing all
variables. We are going to use `dplyr`'s `full_join` function to do
so, that finds the common variable (in this case `uniprot`) to match
observations from the first and second table.

```{r join1}
library("dplyr")
full_join(jdf1, jdf2)
```

In the examples above, each observation of the `jdf1` and `jdf2`
tables are uniquely identified by their UniProt accession number. Such
variables are called **keys**. Keys are used to match observations
across different tables.

In case none of the variable names match, those to be used can be set
manually using the `by` argument, as shown below with the `jdf1` (as
above) and `jdf3` tables, where the UniProt accession number is
encoded using a different capitalisation.

```{r joinby}
names(jdf3)
full_join(jdf1, jdf3, by = c("uniprot" = "UniProt"))
```

As can be seen above, the variable name of the first table is retained
in the joined one.

`r msmbstyle::question_begin()`
Using the `full_join` function demonstrated above, join tables `jdf4`
and `jdf5`. What has happened for observations `P26039` and `P02468`?
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
```{r joinex1}
full_join(jdf4, jdf5)
```

`P02468` and `P02468` are only present in `jdf4` and `jdf5`
respectively, and their respective values for the variables of the
table have been encoded as missing.

`r msmbstyle::solution_end()`

## Different types of joins

Above, we have used the `full_join` function, that fully joins two
tables and keeps all observations, adding missing values if
necessary. Sometimes, we want to be selective, and keep observations
that are present in only one or both tables.

- An **inner join** keeps observations that are present in both
  tables.

```{r, results='markup', fig.cap="An inner join matches pairs of observation matching in both tables, this dropping those that are unique to one table. Figure taken from *R for Data Science*.", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/join-inner.png")
```

- A **left join** keeps observations that are present in the left
  (first) table, dropping those that are only present in the other.
- A **right join** keeps observations that are present in the right
  (second) table, dropping those that are only present in the other.
- A **full join** keeps all observations.


```{r, results='markup', fig.cap="Outer joins match observations that appear in at least on table, filling up missing values with `NA` values. Figure taken from *R for Data Science*.", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/join-outer.png")
```

`r msmbstyle::question_begin()`
Join tables `jdf4` and `jdf5`, keeping only observations in `jdf4`.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r leftjoinex1}
left_join(jdf4, jdf5)
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Join tables `jdf4` and `jdf5`, keeping only observations in `jdf5`.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r rightjoinex1}
right_join(jdf4, jdf5)
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Join tables `jdf4` and `jdf5`, keeping observations observed in both tables.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r innerjoinex1}
inner_join(jdf4, jdf5)
```
`r msmbstyle::solution_end()`

## Multiple matches

Sometimes, keys aren't unique. In the `jdf6` table below, we see that
the accession number `Q99PL5` is repeated twice. According to this
table, the ribosomial protein binding protein 1 localises in the
[endoplasmic
reticulum](https://en.wikipedia.org/wiki/Endoplasmic_reticulum) (often
abbreviated ER) and in the [Golgi
apparatus](https://en.wikipedia.org/wiki/Golgi_apparatus) (often
abbreviated GA).

```{r jdf6}
jdf6
```

If we now want to join `jdf6` and `jdf2`, the variables of the latter
will be duplicated.

```{r multexple}
inner_join(jdf6, jdf2)
```

In the case above, repeating is useful, as it completes `jdf6` with
correct information from `jdf2`. One needs however to be careful when
duplicated keys exist in both tables. Below, we create an inner join
between `jdf6` and `jdf7`, both having duplicated `Q99PL5` entries.

```{r multproblem}
inner_join(jdf6, jdf7)
```

`r msmbstyle::question_begin()`
Interpret the result of the inner join above, where both tables have duplicated keys.
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
`jdf6` has two entries, one for each possible sub-cellular
localisation of the protein. `jdf7` has also two entries, referring to
two different quantitative measurements (variable `measure`). When
joining the duplicated keys, you get all possible combinations.

```{r, results='markup', fig.cap="Joins with duplicated keys in both tables, producing all possible combinations. Figure taken from *R for Data Science*.", echo=FALSE, purl=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("./figs/join-many-to-many.png")
```

In this case, we obtain wrong information: both proteins in the ER and
in the GA both have value 102 and 3.
`r msmbstyle::solution_end()`


## Matching across multiple keys

So far, we have matched tables using a single key (possibly with
different names in the two tables). Sometimes, it is necessary to
match tables using multiple keys. A typical example is when multiple
variables are needed to discriminate different rows in a tables.

Following up from the last example, we see that the duplicated UniProt
accession numbers in the `jdf6` and `jdf7` tables refer to different
[isoforms](https://en.wikipedia.org/wiki/Protein_isoform) of the same
RRBP1 gene. To uniquely identify isoforms, we need to consider two
keys, namely the UniProt accession number (named `uniprot` in both
tables) as well as the isoform number, called `isoform` and
`isoform_num` respectively.

Because the isoform status was encoded using different variable names
(which is, of course a source of confusion), `jdf6` and `jdf7` are
only automatically joined based on the shared `uniprot` key. Here, we
need to join using both keys and need to explicitly name the variables
used for the join.


```{r morekeys}
inner_join(jdf6, jdf7, by = c("uniprot" = "uniprot", "isoform" = "isoform_num"))
```

We now see that isoform 1 localised to the ER and has a measured value
of 102, while isoform 2, that localised to the GA, has a measured
value of 3.


`r msmbstyle::question_begin()`
Can you think of another way to merge tables `jdf6` and `jdf7` using
the two keys?
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
Ideally, the isoform variables should be named identically in the two
tables, which would enable and automatic join with the two
keys. Below, we first fix the misnamed variable in `jdf7`. Instead of
updating the variable name by checking its index manually, we grep it
programmatically and store it in a new variable `i`. We can then join
the two tables without having to specify the two keys explicitly.

```{r morekeysex}
i <- grep("isoform", names(jdf7))
names(jdf7)[i] <- "isoform"
inner_join(jdf6, jdf7)
```


`r msmbstyle::solution_end()`


## Merge in base R

Above, we have used several join functions from the `dplyr` package as
they are convenient and easy to remember. The equivalent function in
the `base` package, that is installed with R, is the `merge`
function. The table below shows how these are related:

| dplyr              | merge                                     |
|--------------------|-------------------------------------------|
| `inner_join(x, y)` | `merge(x, y)`                             |
| `left_join(x, y)`  | `merge(x, y, all.x = TRUE)`               |
| `right_join(x, y)` | `merge(x, y, all.y = TRUE),`              |
| `full_join(x, y)`  | `merge(x, y, all.x = TRUE, all.y = TRUE)` |


Even if you decide to stick with one of these alternatives, it is
important to be aware of the other one, especially given the
widespread usage of `merge` in many packages and in R itself.

## Row and column binding

There are other two important functions in R, that can be used to
combine two dataframes, but assume that these already match
beforehand, as summerised in figure \@ref(fig:bindfig) below.

```{r bindfig, fig.width = 6, fig.asp = 1, fig.cap = "Matching dimension and names when binding by rows and columns.", echo = FALSE}
plot(1:10, type = "n", bty = "n", axes = FALSE, ann = FALSE,
     xlim = c(1, 11), ylim = c(1, 11))

rect(1, 7.5, 5, 10)
segments(1, 9.55, 5, 9.55)
segments(1, 9.5, 5, 9.5)
text(1, 9.75, labels = paste(paste0("V", 1:5, collapse = " "), "..." , collapse = " "), pos = 4, offset = 0.1)
segments(seq(1, 5, 0.5), rep(7.5, 10),
         seq(1, 5, 0.5), rep(9.55, 10))

rect(1, 1, 5, 7)
segments(1, 6.55, 5, 6.55)
text(1, 6.75, labels = paste(paste0("V", 1:5, collapse = " "), "..." , collapse = " "), pos = 4, offset = 0.1)
segments(seq(1, 5, 0.5), rep(1, 10),
         seq(1, 5, 0.5), rep(6.55, 10))
segments(rep(1, 12), seq(1, 6.5, 0.5),
         rep(5, 12), seq(1, 6.5, 0.5))

rect(6, 1, 11, 7)
segments(6, 6.55, 11, 6.55)
text(6, 6.75, labels = paste(paste0("X", 1:8, collapse = " "), "..." , collapse = " "), pos = 4, offset = 0.1)
segments(rep(6, 12), seq(1, 6.5, 0.5),
         rep(11, 12), seq(1, 6.5, 0.5))

abline(v = seq(1, 5, 0.5), lty = "dotted", col = "grey")
abline(h = seq(1, 6.5, 0.5), lty = "dotted", col = "grey")
```

```{r, echo = FALSE}
d1 <- data.frame(x = 1:3, y = 1:3)
d2 <- data.frame(a = 4:5, b = 4:5)
d3 <- data.frame(v1 = 1:2, v2 = 3:4, v3 = 5:6)
```

We are going to illustrate binding by columns with dataframes `d1` and
`d2`, and then binding by rows using `d2` and `d3`. Lets start with
`d1` and `d2` shown below; both have the same number of columns but,
and this is crucial, do not have the same column names:

```{r}
d1
d2
```

While de number of columns match, the names don't, which results in an
error[^try] when we use `rbind`:

```{r}
try(rbind(d1, d2))
```

[^try]: The failing call to `rbind` is wrapped into a `try` call here
    to stop the error from aborting the document compilation.

Before `rbind`ing two dataframes, we must assure that their number of columns and
rownames match exactly:

```{r}
names(d2) <- names(d1)
rbind(d1, d2)
```

If we want to bind to dataframes along their columns, we must make
sure that their number of rows match; rownames not hinder here:

```{r}
cbind(d2, d3)
```

Note that beyond the dimensions and column names that are required to
match, the real meaning of `rbind` is to bind dataframes that contain
observations for the same set of variables - there is more than only
the column names. Below, we `rbind` dataframes with identical column
names but different variables, which end up all being coerced into
characters.


```{r}
d1
d4 <- data.frame(x = letters[1:2], y = letters[1:2])
str(rbind(d1, d4))
```


**Note**: `rbind` and `cbind` are base R functions. The *tidyverse*
alternatives from the `dplyr` package are `bind_rows` and `bind_cols`
and work similarly.

## Additional exercises


`r msmbstyle::question_begin()`

Using the `jdf4` and `jdf5` tables, emulate the left, right and inner
joins using a the full join and filter functions.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`

Load the `rWSBIM1207` package.  Using the `data` function, directly
load the `clinical2` and `expression` data into your global
environment.

- Inspect the clinical data. What kind of information do we have and
  how many patients are recorded?

- Inspect the expression data. How many samples are recorded?

- Join the `expression` and `clinical2` tables by the patient
  reference, using the `left_join` and the `right_join` functions. Why
  are the results different?

- Join `expression` and `clinical2` tables in order to create a table
  containing merged data exclusively for normal samples.

`r msmbstyle::question_end()`



```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
data(expression)
data(clinical2)

nrow(clinical2)
nrow(expression)

## Join the `expression` and `clinical2` tables by the patient
## reference, using the `left_join` and the `right_join`
## functions. Why are the results different?
right_join(expression, clinical2, by = c("patient" = "patientID"))
left_join(expression, clinical2, by = c("patient" = "patientID"))

## Difference due to 3 patients recorded in clinical data are not
## recorded in expression.
length(clinical2$patientID)
length(unique(expression$patient))

## Join `expression` and `clinical2` tables in order to create a table
## containing merged data exclusively for normal samples.
expression %>%
  filter(type == "normal") %>%
  left_join(clinical2, by = c("patient" = "patientID"))
```

<!--chapter:end:50-joining-tables.Rmd-->

# Reproducible research {#sec-rr}

**Learning Objectives**

- Understand the concept of reproducible research and reproducible
  documents.
- Undertand the process by which a source document in compiled into a
  final report.
- Generate a reproducible report in html or pdf from an Rmarkdown
  document using RStudio.

For a general introduction on the topic in French, see
@Pouzat:2015. If you want to explore the topic of reproducible
research in French, the [Recherche reproductible : principes
méthodologiques pour une science
transparente](https://www.fun-mooc.fr/courses/course-v1:inria+41016+session01bis/about)
MOOC is of interest.

Reproducible research refers to research that can be reproduced under
various conditions and by different people. It applies to every area
of research, both experimental and computational, but is often (but
not always) easier to implement for computational work. The different
levels of reproducibility can formalised[^rrblog] as follows:


[^rrblog]: The nomenclature is taken from [this blog
    post](https://lgatto.github.io/rr-what-should-be-our-goals/) that
    provides links and highlights that confusion that exists around
    the terms and concepts of reproducibility.

- **Repeat** my experiment, i.e. obtain the same tables/graphs/results
  using the same setup (data, software, ...) in the same lab or on the
  same computer. That's basically re-running one of my analysis some
  time after I original developed it.

- **Reproduce** an experiment (not ones own), i.e. obtain the same
  tables/graphs/results in a different lab or on a different computer,
  using the same setup (the data would be downloaded from a public
  repository and the same software, but possibly using a different
  version, or a different operation system).

- **Replicate** an experiment, i.e. obtain the same (similar enough)
  tables/graphs/results in a different set up. The data could still be
  downloaded from the public repository, or possibly
  re-generated/re-simulated, and the analysis would be re-implemented
  based on the original description.

- Finally, **re-use** the information/knowledge from one experiment to
  run a different experiment with the aim to **confirm** results from
  scratch.

The table below summerised these concepts focusing on data and code in
computational projects.


|                    | Same data | Different data |
|--------------------|:---------:|:--------------:|
| **Same code**      | Repeat    | Reproduce      |
| **Different code** | Reproduce | Replicate      |


There are many reasons to work reproducibly, and @Markowetz:2015
nicely summarises 5 good reasons. Importantly, he stressed out that
the first beneficiary of reproducible work are the student/research
that apply these principles:

1. Reproducibility helps to avoid disaster.
2. Reproducibility makes it easier to write papers[^rrexam1].
3. Reproducibility helps reviewers see it your way[^rrexam2].
4. Reproducibility enables continuity of your work.
5. Reproducibility helps to build your reputation.


[^rrexam1]: And course reports! The exam of this course will consist
    in a reproducible report using the tools described below.
[^rrexam2]: Not only reviewers, also professors that read exams. See
    previous footnote.

## `knitr` and `rmarkdown`

Reproducible research is an essential part of any data analysis. With
the tools that are available, one can argue that it has become more
difficult not to produce reproducible reports than to producing then.

Reproducible documents have been a part of R since the very
beginning. See for example @biocwp2, to see how such *compendia* play a
central role within the [Bioconductor](https://bioconductor.org/)
project (more about Bioconductor in it's dedicated
chapter). Originally, these were written in LaTeX, interleaved with R
code chunks, forming so called Sweave documents (with extension
`.Rnw`).

```{r rmarkdownsticker, results='asis', fig.margin=TRUE, fig.cap="The rmarkdown sticker", fig.width=7, fig.height=4, echo=FALSE, purl=FALSE}
knitr::include_graphics("./figs/rmarkdown-200x232.png")
```

More recently, it has become to use the
[markdown](https://daringfireball.net/projects/markdown/) syntax
markup language, rather than LaTeX. Once interleaved with R code
chunks, these documents become **Rmarkdown** files (`.Rmd`). The can be
converted into markdown using `knitr::knit`, that executes the code
chunk and incorporates their output in the resulting markdown
documents, which itself is converted to one of many output formats,
typically pdf of html using [pandoc](http://pandoc.org/). In R, this
final conversion is done using `rmarkdown::render` (that relies on
pandoc).

- `knitr::knit` converts the `Rmd` into `md` by executing the code
  chunks and replacing the code by its output (text, tables, figures,
  ...).

- The `md` file is then compiled into the desired [output
  format](https://rmarkdown.rstudio.com/lesson-9.html) (typically html
  or pdf) using `pandoc`.

- In practice, in R, these two steps are automatically handled in one
  go by `rmarkdown::render()`.


```{r rmarkdownflow, results='asis', fig.cap="The rmarkdown workflow (image from RStudio)", out.width = '100%', echo=FALSE, purl=FALSE}
knitr::include_graphics("./figs/rmarkdownflow.png")
```

The [rmarkdown](http://rmarkdown.rstudio.com/) package is developed
and maintained by RStudio and benefits from excellent documentation,
support and integrates into the RStudio editor.


An Rmarkdown document is composed of

- An optional YAML **header**, delimited by `---`.

- Text in [simple **markdown**
  format](https://pandoc.org/MANUAL.html#pandocs-markdown).

- One or more R **code chunks** delimited by three backticks. Each
  code chunk can be uniquely named and parametrised with a set of code
  chunk [options](http://yihui.name/knitr/options/).

These respective parts of an `Rmd` file are show below and will be
demonstrated during the course.

![](./figs/rmd.png)

RStudio also supports
[Notebook](https://bookdown.org/yihui/rmarkdown/notebook.html)
documents[^jupyter] that execute individual code chunks independently
and display directly in the source document.

![](./figs/rnb.png)

[^jupyter]: See also [Jupyter notebooks](https://jupyter.org/)
initially developed for Python, but that can run R code as well.

Here is an [R markdown cheat
sheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)
provided by RStudio and an [introduction
article](https://rmarkdown.rstudio.com/articles_intro.html).


The following video, [*R Markdown: The bigger
picture*](https://resources.rstudio.com/rstudio-conf-2019/r-markdown-the-bigger-picture)
by Garrett Grolemund at a 2019 RStudio conference provides a very nice
introduction on the many reasons why writing reproducible documents in
essential in data science and biomedical research.

## Additional features

- Among the most options that can set for code chunks is
  `cache`. Setting `cache = TRUE` will avoid that specific code chunk to
  be cached and not recomputed every time the documented in *knitted*,
  unless the code chunk was modified. This is an important feature
  when long computations are necessary.

- The `DT::datatable` function allows to create dynamic tables
  directly from R, as show below.

```{r dtexample}
ip <- installed.packages()
DT::datatable(ip[, c(1, 3, 5, 6, 10)], rownames = FALSE)
```

- It is always useful to finish a Rmarkdown report with a section
  providing all the session information details with the
  `sessionInfo()` function, such at the end of this material. This
  allows readers to review the version of R itself and all the
  packages that were used to produce the report.


Using Rmarkdown, it is also possible to produce
[slides](https://rmarkdown.rstudio.com/lesson-11.html),
[websites](https://rmarkdown.rstudio.com/lesson-13.html), and
[complete books](https://bookdown.org/), [interactive
documents](https://rmarkdown.rstudio.com/lesson-14.html) and [R
package vignettes](http://bioconductor.org/help/package-vignettes/).


`r msmbstyle::question_begin()`

Prepare an Rmarkdown report summarising the portal ecology data. The
report should include a *Material and methods* section where the data
is read in (ideally from the online file) and briefly described, a
*Data preparation* section where rows with missing values are filtered
out, and a *Visualisation* section where one or two plots are
rendered. Finish your report with a *Session information* section.

`r msmbstyle::question_end()`

The [R Markdown Cheat
Sheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf)
and [Reference
Guide](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)
will help you with the markdown synatax, R code chunk options, and
RStudio utlisation.

**Note**: When you prepare an `Rmd` report, it is advised to start
with a code chunk that load all packages, and update that code chunk
as you proceed with your analysis and use new packages. This avoids
the situation where some commands work in your R console (where you
initially loaded the package) but fail when you compile your
report. Indeed, an `Rmd` file is compiled in a new, clean R session,
without access to your working session, the packages that were loaded,
and the variables that were created.

<iframe width="560" height="315" src="https://www.youtube.com/embed/4OXyyMIM6A8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Docker containers

There are other tools for reproducible research, that aim to
disseminate more than code and data. Docker containers for example
enable to share the complete image of an operating system, including
all system dependencies and software/data to repeat a complete
analysis. These are useful tools, even though their aren't necessarily
ideal, and beyond the aim of this course. In the annex, (chapter
\@ref(sec-anx)), we show how to use a pre-build course-specific
RStudio cloud instance based on the [Renku](https://renkulab.io/)
infrastructure.

## Additional exercises

`r msmbstyle::question_begin()`

Repeat the *Beer consumption* analysis using the that was done in
chapters \@ref(sec-dplyr) and \@ref(sec-vis).

To test if your report is fully reproducible, uniquely name the `Rmd`
file as `surname_surname_beers_report.Rmd`, post it on the course
forum and ask your neighbour to download it, compile it into pdf and
provide feedback on whether the document was reproducible and easy to
follow[^asinexam].

[^asinexam]: This is an important exercise, as it will mimic the exam
    situation, where you will hand in your `Rmd` reports that will
    need to be compiled (to pdf) before marking. In addition, it
    demonstrates the challenges of writing and reproducing an
    understandable and reproducible document.

`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`

1. Create a new R markdown file called `student_results.Rmd`, to be
   compiled in either pdf or html. Remove everything but the lines
   containing the header. In the example below, the header is composed
   of the 6 first lines, starting and ending with `---`.

```
---
title: "Student results"
author: "Laurent"
date: "March 14, 2020"
output: pdf_document
---
```

2. Add a first section called *Data input*, in which you will load the
   `rWSBIM1207` package, use the `interroA.csv()` function to get the
   name of a csv file containing test results for a set of students,
   and read these data into R using `read_csv`. [^readf] Display the
   few first observations and write a short sentence explaining the
   data.


[^readf]: It is important here **not** to copy/paste the filename
    returned by `interroA.csv()`. That file is distributed with the
    `rWSBIM1207` package, and has a computer-specific path. Because we
    want our reports to be reproducible, we want to use the filename
    as returned by the function on any computer, not the one of a
    specific computer. Make sure you either pass `interroA.csv()`
    directly to `read_csv()` or store its output into a variable that
    is passed to `read_csv()`.

3. Make sure that you can compile you `Rmd` file into either pdf or html.

4. Create a new section called *Data visualisation*.

   Here, the goal is to visualise the score distributions for the four
   tests using `ggplot2`. These distributions will be visualised using
   boxplots. You will need to visualise these distribution for each
   test separately, and for male and female students.

5. As discussed during the course, we need data in a long format to be
   able to use `ggplot2`. Start by converting these data into a long
   format using `pivot_longer()`. Display the first rows of these new
   data and write a short sentence describing them and the
   transformation you just applied.

6. Use `ggplot2` to visualise the score distributions along boxplots
   for each test and for female and male students.

`r msmbstyle::question_end()`

<!--chapter:end:60-rr.Rmd-->

# Bioinformatics {#sec-bioinfo}

As already alluded to earlier, [Wikipedia
defines](https://en.wikipedia.org/wiki/Bioinformatics) bioinformatics
as

> Bioinformatics is an interdisciplinary field that develops methods
  and software tools for understanding biological data.

Bioinformatics is as varied as biology itself, and ranges from data
analysis, to software development, computational or statistical
methodological development, more theoretical work, as well as any
combination of these.

## Omics data

So far, we have explored broad data science techniques in R. A
widespread and successful area of bioinformatics, and one that you, as
a biology or biomedical science student are likely to be confronted
with, is the analysis and interpretation of omics data.

```{r infoflow, results='markup', fig.margin=TRUE, fig.cap="Information flow in biological systems (Source [Wikipedia](https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology)).", fig.width=7, fig.height=7, echo=FALSE, purl=FALSE}
knitr::include_graphics("./figs/Centraldogma_nodetails.png")
```

It is useful to define these omics data along the flow of information
in biology (Figure \@ref(fig:infoflow)), and define the different
application domains. The technologies that focus on DNA, and the
genome in particular (either whole or parts thereof) are termed
**genomics**, and are currently based on sequencing, in particular
high throughput sequencing (HTS). The domain focusing on the study of
any DNA (or assiciated proteins) modification (such as for example
methylation) is termed **epigenetics**. The study of RNA, and more
specifically the quantitation of RNA levels in biological samples is
termed **transcriptomics**, as it assays the transcription of DNA into
RNA molecules. Without further specification, transcriptomics refers
to the quantitation of message RNA, although one could also focus on
non-coding RNAs such as micro RNAs. HTS is currently the technology of
choice for any transcriptomics study, while a decade ago, prior to the
development of RNA sequencing (called **RNA-Seq**), microarrays were
widely used. **Proteomics** focuses on the identification and
quantitation of proteins, and can also expand into the study of
protein interactions, post-translational modifications or sub-cellular
localisation of proteins. Further downstream of proteins, small
molecules or lipids can also be assayed under the umbrella terms of
**metabolomics** and **lipidomics**. The technology of choice for
protein, lipids or smaller metabolites is mass spectrometry.

In the next couple of sections, some important concepts related to
omics data and their analysis are repeated and emphasised.

### High throughput {-}

By it very nature, omics data is high throughput. The goal is to
measure all, or as many as possible molecules of an omics-domain as
possible: sequence the whole genome or all exomes; identify all
epigenetic histone modifications (defining the compactness of DNA and
hence it's accessibility by the transcription machinery); identify and
quantify as much as possible from the complete proteomics; etc. As a
result, omics data is both large in size and complex in nature, and
requires dedicated software and analysis methods to be processed,
analysed to infer biologically relevant patterns.

### Raw and processed data {-}

The omics data that are produced by the instruments are called raw
data, and their size (generally large), the types of file, and
structure will depend on the technology that is used. Raw data need to
be processed using dedicated software before obtaining data that can
be mapped to the biology that is measured. Below we illustrate two
such examples using Sanger sequencing and mass spectrometry.

In Sanger sequencing (Figure \@ref(fig:sangerseq)), DNA is labelled
using fluorophores, and different nucleotides are marked with
different colours. Upon acquisition, light signal is acquired and
recording of the different colours can be used to reconstruct the DNA
sequence.

```{r sangerseq, out.width = '70%', fig.cap="Processing Sanger sequencing data to a string. (Source [BiteSizeBio](https://bitesizebio.com/27985/sanger-sequencing-genome-won/)).", echo = FALSE}
knitr::include_graphics("./figs/sanger-sequencing.jpg")
```

In mass spectrometry, charged molecules are separated based on their
mass-to-charge (M/Z) ratio and their intensities recorded to produce a
spectrum. In proteomics, the molecules that are assayed are protein
fragments called peptides. Upon fragmentation of peptides, the
different between the M/Z peaks of the peptide fragment ions can be
used to reconstruct the peptide sequence (Figure \@ref(fig:pepseq)).

```{r pepseq, out.width = '100%', fig.cap="De novo peptide sequencing using mass spectrometry. (Source [Creative Proteomics](https://www.creative-proteomics.com/services/de-novo-peptides-proteins-sequencing-service.htm)).", echo = FALSE}
knitr::include_graphics("./figs/de-novo-pep-sequencing.jpg")
```


The size and computational cost of processing raw data often require
more serious hardware, including disk space, computer clusters with
100s or more of compute nodes and/or access to high amounts of memory
(RAM).

Processed data themselves often need to be further transformed to
account for a variety of noise that is inheritent to sample
collection, preparation and measurement acquisition. Data processing
and transformation will be explored in detail in subsequent course
such as *Omics data analysis*
([WSBIM2122](https://github.com/UCLouvain-CBIO/WSBIM2122)).


## Metadata and experimental design


The acquired data, even once processed, is still of very little use
when it comes to understanding biology. Before samples are collected
and data are generated, it is essential to carefully design a question
of interest (research hypothesis) and the experiement that will allow
to answer it. For example, if we want to understand the effect of a
particular drug on cancer cells, and more specifically understand the
effect on the transcription of all the expressed genes, on would need
to measure gene expression (using for example RNA-Seq) in cancer cells
in presence and absence of that drug. The table below describes a
simple experimental design where 3 conditions (control, drug at a
concentrations of 1 and 5) have been simultaneously processed and
measured by the same operator in 4 replicate.

```{r, echo = FALSE}
expd <- data.frame(sample = paste0("S", 1:12),
                   operator = "Kevin", date = '2019-03-02',
                   group = rep(c("CTRL", "DRUG", "DRUG"), each = 4),
                   concentration = factor(rep(c(0, 1, 5), each = 4)),
                   replicate = rep(1:4, 3),
                   stringsAsFactors = FALSE)
knitr::kable(expd)
```

We have seen a much more complex experimental desing, involving many
more samples with the `clinical1` data.

```{r, echo = FALSE, message = FALSE}
library("rWSBIM1207")
data(clinical1)
clinical1
```

When performing experiments, measurements should also be repeated
several times (typically at least three), to quantify the overall
variability (technical and biological) in the measured variables and,
eventually, identify changes that relate to the conditions of interest
(for example differences in genes expression in the presence or
absence of the drug).

```{r, echo = FALSE, fig.cap = "Distribution of the expression of the genes A1CF, BRCA1 and TP53 under the control (no drug) and drug at concentrations 1 and 5."}
set.seed(1)
ge <- expd
ge$A1CF <- rnorm(12, 6, 2)
ge$BRCA1 <- c(abs(rnorm(4, 2, 1)), rnorm(4, 8, 2), rnorm(4, 13, 2))
ge$TP53 <-  c(rnorm(4, 10, 5), rnorm(4, 10, 3), rnorm(4, 10, 2))
ge <- pivot_longer(ge,
                   names_to = "gene",
                   values_to = "expression",
                   c(A1CF, BRCA1, TP53))
ggplot(ge, aes(x = gene, y = expression, colour = concentration)) +
    geom_boxplot()
```

## The Bioconductor project {#sec-bioconductor}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library("SummarizedExperiment")
library("BiocStyle")
```

The [Bioconductor](http://www.bioconductor.org) was initiated by
Robert Gentleman (@Gentleman:2004;@Huber:2015), one of the two
creators of the R language, and centrally offers dedicated R packages
for bioinformatics.

> Bioconductor provides tools for the analysis and comprehension of
> high-throughput genomic data. Bioconductor uses the R statistical
> programming language, and is open source and open development. It
> has two releases each year, and an active user community.

```{r biocwww, fig.cap="The Bioconductor web page.", echo=FALSE, out.width = '100%'}
knitr::include_graphics("./figs/bioc-screenshot.png")
```

Bioconductor packages are managed installed using a dedicated package,
namely `BiocManager`, that can be installed from CRAN with

```{r, eval = FALSE}
install.packages("BiocManager")
```

Individuals package such as `SummarizedExperiment` (see below for
details), `DESeq2` (for transcriptomics), `Spectra` (for mass
spectrometry), `xcms` (metabolomics), ... can then be installed with
`BiocManager::install`.

```{r, eval = FALSE}
BiocManager::install("SummarizedExperiment")
BiocManager::install("DESeq2")
BiocManager::install("Spectra")
BiocManager::install("xcms")
```

Note that we can also use that same function to install packages from GitHub:

```{r, eval = FALSE}
BiocManager::install("UCLouvain-CBIO/rWSBIM1207")
```

## Omics data containers

Data in bioinformatics is often more complex than the basic data types
we have seen so far. In such situations, developers define specialised
data containers (termed classes that) that match the properties of the
data they need to handle.

An example of general data architecture, that is used across many
omics domains in Bioconductor is represented below:

```{r msnset, fig.cap="A data structure to store quantitative data, features (rows) annotation, and samples (column) annotations..", echo=FALSE, out.width = '80%'}
knitr::include_graphics("./figs/msnset.png")
```

- An assay data slot containing the quantitative omics data
  (expression data), stored as a `matrix`. Features (genes,
  transcripts, proteins, ...) are defined along the rows and samples
  along the columns.

- A sample metadata slot containing sample co-variates, stored as a
  table (`data.frame` or `DataFrame`). This dataframe is stored with
  rows representing samples and sample covariate along the columns,
  and its rows match the expression data columns exactly.

- A feature metadata slot containing feature co-variates, stored as
  table annotated (`data.frame` or `DataFrame`). This dataframe's rows
  match the expression data rows exactly.

The coordinated nature of the high throughput data guarantees that the
dimensions of the different slots will always match (i.e the columns
in the expression data and then rows in the sample metadata, as well
as the rows in the expression data and feature metadata) during data
manipulation. The metadata slots can grow additional co-variates
(columns) without affecting the other structures.

To illustrate such an omics data container, we'll use a variable of
class `SummarizedExperiment`. Below, we load the `GSE96870_intro`
dataset from the `rWSBIM1207` package (version >= 0.1.15).


```{r, message = FALSE}
library("SummarizedExperiment")
library("rWSBIM1207")
data(GSE96870_intro)
class(GSE96870_intro)
GSE96870_intro
```

This data contains the same RNA-sequencing data as we have seem in
chapter \@ref(sec-startdata). It is however formatted as a special
type of data, a `SummarizedExperiment`, that is specialised for
quantitative omics data (as opposed to the more general `data.frame`
data structure). We will be learning and using `SummarizedExperiment`
objects a lot in the
[WSBIM1322](https://github.com/UCLouvain-CBIO/WSBIM1322) and
[WSBIM2122](https://github.com/UCLouvain-CBIO/WSBIM2122) courses.


The object contains data for `r nrow(GSE96870_intro)` features
(peptides in this case) and `r ncol(GSE96870_intro)` samples.

```{r}
dim(GSE96870_intro)
nrow(GSE96870_intro)
ncol(GSE96870_intro)
```

The samples (columns) and rows (protein features) are named:

```{r}
colnames(GSE96870_intro)
head(rownames(GSE96870_intro))
tail(rownames(GSE96870_intro))
```

Using this data structure, we can access the expression matrix with
the `assay` function, the feature metadata with the `rowData` function,
and the sample metadata with the `colData` function:

```{r}
head(assay(GSE96870_intro))
head(rowData(GSE96870_intro))
colData(GSE96870_intro)
```

`r msmbstyle::question_begin()`

Verify that the expression data dimensions match with number of rows
and columns in the feature and sample data.

`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
```{r}
nrow(assay(GSE96870_intro)) == nrow(rowData(GSE96870_intro))
ncol(assay(GSE96870_intro)) == nrow(colData(GSE96870_intro))
```
`r msmbstyle::solution_end()`


We can use the `[` operator to subset the whole object: all parts
thereof will be subset correctly.

```{r}
small_se <- GSE96870_intro[c(1, 3, 5), c(2, 4)]
dim(small_se)
head(assay(small_se))
```

We can also add information with:

```{r}
colData(small_se)$new_var  <- c("new_val1", "new_val2")
colData(small_se)
```

## Bioconductor data infrastructure

An essential aspect that is central to Bioconductor and its success is
the availability of core data infrastructure that is used across
packages. Package developers are advised to make use of existing
infrastructure to provide coherence, interoperability and stability to
the project as a whole.

Here are some core classes, taken from the [Common Bioconductor
Methods and
Classes](https://bioconductor.org/developers/how-to/commonMethodsAndClasses/)
page:

#### Importing  {-}

- GTF, GFF, BED, BigWig, etc., - `r Biocpkg("rtracklayer")``::import()`
- VCF – `r Biocpkg("VariantAnnotation")``::readVcf()`
- SAM / BAM – `r Biocpkg("Rsamtools")``::scanBam()`, `r Biocpkg("GenomicAlignments")``:readGAlignment*()`
- FASTA – `r Biocpkg("Biostrings")``::readDNAStringSet()`
- FASTQ – `r Biocpkg("ShortRead")``::readFastq()`
- Mass spectrometry data (XML-based and peaklist formats) – `r Biocpkg("Spectra")``::Spectra()`

#### Common Classes {-}

- Rectangular feature x sample data – `r Biocpkg("SummarizedExperiment")``::SummarizedExperiment()` (RNAseq count matrix, microarray, proteomics, ...)
- Genomic coordinates – `r Biocpkg("GenomicRanges")``::GRanges()` (1-based, closed interval)
- DNA / RNA / AA sequences – `r Biocpkg("Biostrings")``::*StringSet()`
- Gene sets – `r Biocpkg("GSEABase")``::GeneSet()` `r Biocpkg("GSEABase")``::GeneSetCollection()`
- Multi-omics data – `r Biocpkg("MultiAssayExperiment")``::MultiAssayExperiment()`
- Single cell data – `r Biocpkg("SingleCellExperiment")``::SingleCellExperiment()`
- Mass spectrometry data – `r Biocpkg("Spectra")``::Spectra()`


## Navigating the Bioconductor project

Bioconductor has become a large project proposing many packages across
many domains of high throughput biology. It continues to grow, at an
increasing rate, and it can be difficult to get started.

### *biocViews*

One way to find packages of interest is to navigate the *biocViews*
hierarchy. Every package is tagged with a set of *biocViews*
labels. The highest level defines 3 types of packages:

- Software: packages providing a specific functionality.
- AnnotationData: packages providing annotations, such as various
  ontologies, species annotations, microarray annotations, ...
- ExperimentData: packages distributing experiments.

The *biocViews* page is available here

- https://bioconductor.org/packages/release/BiocViews.html#___Software

It is most easily accessed by clicking on the *software packages* link
on the homepage, under *About Bioconductor* (see screenshot above).

See also this
[page](https://bioconductor.org/developers/how-to/biocViews/) for
additional information.

### Workflows

On the other hand, people generally don't approach the Bioconductor
project to learn the whole project, but are interested by a specific
analysis from a Bioconductor package, that they have read in a paper
of interest. In my opinion, it is more effective to restrict ones
attention to a problem or analysis of interest to first immerse
oneself into Bioconductor, then broaden up ones experience to other
topics and packages.

To to that, the project offers workflows that provide a general
introduction to topics such as sequence analysis, annotation
resources, RNA-Seq data analyis, Mass spectrometry and proteomics,
CyTOF analysis, ....

- https://bioconductor.org/help/workflows/

A similar set of resources are published
in [F1000Research](https://f1000research.com/) under the Bioconductor
gateway

- https://f1000research.com/gateways/bioconductor

These peer-reviewed papers describe more complete pipelines involving
one or several packages.

### Learning about specific packages

Each Bioconductor package has it's own *landing pages* that provides
all necessary information about a package, including a short summary,
its current version, the authors, how the cite the package,
installation instructions, and links to all package vignettes.

Any Bioconductor package page can be contructed by appending the
package's name to `https://bioconductor.org/packages/` to produce an
URL like

- https://bioconductor.org/packages/packagename

This works for any type of package (software, annotation or data). For
example, the pages for packages `r Biocpkg("DESeq2")` or `r Biocpkg("QFeatures")`
would be

- https://bioconductor.org/packages/DESeq2

and

- https://bioconductor.org/packages/QFeatures

These short URLs are then resolved to their longer form to redirect to
the longer package URL leading the user to the current release version
of the packge.

### Package vignettes

An important quality of every Bioconductor package is the availability
of a dedicated *vignette*. Vignettes are documentations (generally
provided as pdf or html files) that provide a generic overview of the
package, without necessarily going in detail for every function of the
package.

Below, we show how to list all vignettes available in the `MSnbase`
package and how to open one in particular.

```{r, eval = FALSE}
vignette(package = "QFeatures")
vignette("QFeatures")
```

Vignettes are special in that respect as they are produced as part of
the package building process. The code in a vignette is executed and
its output, whether in the form of simple text, tables and figures,
are inserted in the vignette before the final file (in pdf or html) is
produced. Hence, all the code and outputs are guaranteed to be correct
and reproduced.

Given a vignette, it is this possible to re-generate all the
results. To make reproducing a long vignette as easy as possible
without copy and pasting all code chunks one by one, it is possible to
extract the code into an R script runnung the `Stangle` (from the
`utils` package -
see [here](https://bioconductor.org/help/package-vignettes/) for
details) or `knitr::purl` functions on the vignette source document.

### Getting help

The best way to get help with regard the a Bioconductor package is to
post the question on the *Bioconductor support forum* at
https://support.bioconductor.org/. Package developers generally follow
the support site for questions related to their packages. See this
page for [some details](https://bioconductor.org/help/support/).

To maximise the chances to obtain a answer promptly, it is important
to provide details for other to understand the question and, if
relevant, reproduce the observed errors. The Bioconductor project has
a dedicated
[posting guide](https://bioconductor.org/help/support/posting-guide/). Here's
another useful guide on
[how to write a reproducible question](http://adv-r.had.co.nz/Reproducibility.html).


Packages come with a lot of documentation build in, that users are
advised to read to familiarise themselves with the package and how to
use it. In addition to the package vignettes are describe above, every
function of class in a package is documented in great detail in their
respective *man* page, that can be accessed with `?function`.

There is also a
dedicated
[*developer mailing list*](https://bioconductor.org/help/support/posting-guide/) that
is dedicated for questions and discussions related to package
development.

### Versions

It is also useful to know that at any given time, there are two
Bioconductor versions - there is always a release (stable) and a
development (devel) versions. For example, in October 2017, the
release version is 3.6 and the development is 3.7.

The individual packages have a similar scheme. Every package is
available for the release and the development versions of
Bioconductor. These two versions of the package have also different
version numbers, where the last digit is even for the former and off
for the later. Currently, the `MSnbase` has versions `2.8.2` and
`2.9.3`, respectively.

Finally, every Bioconductor version is tight to an R version. To
access the current Bioconductor release, one needs to use the latest R
version. Hence, it is important to have an up-to-date R installation
to keep up with the latest developments in Bioconductor. More details
[here](https://bioconductor.org/developers/how-to/version-numbering/).


## Exercises

`r msmbstyle::question_begin()`

1. Install a Bioconductor package of your choice, discover the
   vignette(s) it offers, open one, and extract the R code of it.

2. Find a package that allows reading raw mass spectrometry data and
   identify the specific function. Either use the biocViews tree, look
   for a possible workflow, or look in the common methods and classes
   page on the Bioconductor page.

`r msmbstyle::question_end()`


`r msmbstyle::question_begin()`

3. Load the `cptac_se` data from the `rWSBIM1322` package. Verify it
   is of class `SummarizedExperiment`.

4. Extract the quantitative information for the peptides `AIGVLPQLIIDR`,
   `NLDAAPTLR` and `YGLNHVVSLIENKK` for samples `6A_7` and
   `6B_8`. Subsetting works as we have seen for `data.frames` in
   chapter 3.

5. Look and interpret the experimental design stored in the sample
   metadata of this experiment. To help you out, you can also read its
   documentation.

6. What is the average expression of `LSAAQAELAYAETGAHDK` in the
   groups `6A` and `6B`?

7. Calculate the average expression of all peptides belonging to
   protein `P02753ups|RETBP_HUMAN_UPS` for each sample. You can
   indentify which peptides to use by looking for that protein in the
   object's rowData slot.

`r msmbstyle::question_end()`


```{r, echo=FALSE, include=FALSE}
library("rWSBIM1322")
data(cptac_se)
class(cptac_se)

peps <- c("AIGVLPQLIIDR", "NLDAAPTLR", "YGLNHVVSLIENKK")
smpl <- c("6A_7", "6B_8")
assay(cptac_se[peps, smpl])

colData(cptac_se)

mean(assay(cptac_se["LSAAQAELAYAETGAHDK", 1:3]))
mean(assay(cptac_se["LSAAQAELAYAETGAHDK", 4:6]))

peps <- rowData(cptac_se)$Proteins == "P02753ups|RETBP_HUMAN_UPS"
colMeans(assay(cptac_se[peps, ]))
```

`r msmbstyle::question_begin()`

1. To be able to access the data for this exercise, make sure you have
   `rWSBIM1207` version 0.1.5 or later. If needed, install a more
   recent version with

```{r, eval = FALSE}
BiocManager::install("UCLouvain-CBIO/rWSBIM1207")
```

2. Import the data from two tab-separated files into R. The full paths
   to the two files can be accessed with `kem.tsv()`. Read `?kem` for
   details on the content of the two files. In brief, the
   `kem_counts.tsv` file contains RNA-Seq expression counts for 13
   genes and 18 samples and `kem_annot.tsv` contains annotation about
   each sample. Read the data into two `tibbles` names `kem` and
   `annot` respectively and familiarise yourself with the content of
   the two new tables.

3. Convert the counts data into a long table format and annotate
   each sample using the experimental design.

4. Identity the three transcript identifiers that have the highest
   expression count over all samples.

5. Visualise the distribution of the expression for the three
   transcripts selected above in cell types A and B under both
   treatments.

6. For all genes, calculate the mean intensities in each experimental
   group (as defined by the `cell_type` and `treatment` variables).

7. Focusing only on the three most expressed transcripts and cell type
   A, calculate the fold-change induced by the treatment. The
   fold-change is the ratio between the average expressions in two
   conditions.

`r msmbstyle::question_end()`

```{r, echo=FALSE, include=FALSE}
library("rWSBIM1207")
library("tidyverse")
fls <- kem.tsv()
annot <- read_tsv(fls[2])

kem <- read_tsv(fls[1]) %>%
    pivot_longer(
        names_to = "sample_id",
        values_to = "expression",
        -ref) %>%
    left_join(annot)

k <- kem %>%
    group_by(ref) %>%
    summarise(tot_exprs = sum(expression)) %>%
    arrange(desc(tot_exprs)) %>%
    select(ref) %>%
    head(3)

kem3 <- right_join(kem, k)

ggplot(kem3, aes(x = treatment, y = expression)) +
    geom_boxplot() +
    geom_jitter() +
    facet_grid(ref ~ cell_type)

kem %>%
    group_by(ref, cell_type, treatment) %>%
    summarise(mean_expression = mean(expression))

kem3 %>%
    filter(cell_type == "A") %>%
    group_by(ref, cell_type, treatment) %>%
    summarise(mean_expression = mean(expression)) %>%
    pivot_wider(names_from = "treatment",
                values_from = "mean_expression") %>%
    mutate(fold_change = stimulated/none)
```



```{r, echo=FALSE, message=FALSE}
library(SummarizedExperiment)
se <- readRDS("./data/se.rds")
```

`r msmbstyle::question_begin()`

Download the following [file](./data/se.rds) and load its data. The
data is of class `SummarizedExperiment` and contains `r nrow(se)`
genes for `r ncol(se)` samples.

- Explore the object's `colData` and interprete it as the data's
  experimental design.

- Generate a figure similar to the one below for your data own data.

```{r results='markup', fig.margin=FALSE, fig.cap="Distribution of the gene expression in each group.", fig.width=9, fig.height=4, echo=FALSE, purl=FALSE}
knitr::include_graphics("./figs/gene_exp_se.png")
```

- Calculate the average expression for each gene in each group.

- Calculate the log2 fold-change (i.e. the difference of expression
  between two groups) for groups CTRL0 and DRUG5. Which gene shows the
  highest fold-change (i.e. where the expression increases most in
  DRUG5)? Can you recognise this on the figure?

- Use the `var()` function to calculate the variance for each
  gene. Which gene has the largest variance. Can you recognise this on
  the figure?

- Calculate the [coefficient of
  variation](https://en.wikipedia.org/wiki/Coefficient_of_variation)
  of each gene. The coefficient of variation is defined as the ratio
  between a gene's standard deviation (that you can compute using the
  `sd()` function) and it's mean. Which gene has the largest
  variance. Can you recognise this on the figure?

- Visualise the distribution of the gene expression in each
  sample. You can use either a boxplot, a violin plot or plot the
  density of these distributions with `geom_density()`. Colour the
  sample data based on the groups.

`r msmbstyle::question_end()`


```{r, include=FALSE, eval=FALSE}
se <- readRDS("./data/se.rds")

se_long <-
    cbind(assay(se), rowData(se)) %>%
    as_tibble() %>%
    pivot_longer(names_to = "samples",
                 values_to = "gene_expression",
                 -c("gene", "location")) %>%
    full_join(as_tibble(colData(se)))

ggplot(se_long,
       aes(x = group,
           y = gene_expression)) +
    geom_boxplot() +
    facet_wrap(~ gene)


avg_exp <-
    se_long %>%
    group_by(gene, group) %>%
    summarise(avg_exp = mean(gene_expression))

avg_exp %>%
    filter(group %in% c("CTRL0", "DRUG5")) %>%
    pivot_wider(names_from = "group",
                values_from = "avg_exp") %>%
    mutate(fc = DRUG5 - CTRL0) %>%
    arrange(desc(fc))

se_long %>%
    group_by(gene) %>%
    summarise(var = var(gene_expression)) %>%
    arrange(desc(var))

se_long %>%
    group_by(gene) %>%
    summarise(sd = sd(gene_expression),
              mn = mean(gene_expression)) %>%
    mutate(cv = sd/mn) %>%
    arrange(desc(cv))

ggplot(se_long,
       aes(x = samples,
           y = gene_expression,
           fill = group)) +
    geom_boxplot()

ggplot(se_long,
       aes(x = samples,
           y = gene_expression,
           fill = group)) +
    geom_violin()

ggplot(se_long,
       aes(x = gene_expression,
           group = samples,
           colour = group)) +
    geom_density()

```

<!--chapter:end:70-intro-bioinformatics.Rmd-->

# Additional programming concepts {#sec-prog}

**Learning Objectives**

Learn programming concepts, including

- how to handle conditions
- iterate of data structures
- good coding practice
- code re-use through functions

When the size and the complexity of the data increases, or the data
science question of interest becomes more complex, the data analysis
techniques as we have seen them so far need to be complemented with
programming techniques. From a data science point of view, there is no
clear delimitation between data analysis and (data) programming, both
morphing into each other[^dataprog].

[^dataprog]: A fundamental difference however is how data analysis and
    programming are taught. When it comes to researchers, and
    biomedical researchers in particular, teaching programming to
    analyse data isn't successful. Teaching data analysis to
    eventually programme with data, however, has proven a successful
    strategy.

This chapter will introduce some additional programming skills and
demonstrate how to use them in the context of high troughput omics
data.

## Writing clean code


Writing clean code means writing easily readable code, hence easily
understable code and, eventually code with less bugs. One easy way to
achieve this is through consistency, i.e. stick to a **style
guide**. This issue is that there are several style guides available,
often with conflicting suggestions. Two widely used ones are the
[Bioconductor style
guide](http://bioconductor.org/developers/how-to/coding-style/) and
[Hadley Wickhams's R Style
Guide](http://r-pkgs.had.co.nz/style.html). The advice from for naming
variables seen in the first and second chapters are also relevant.

Here are a couple of suggestions:

- Use `<-` to assign variables. Use `=` is also valid, but make sure
  that you stick with one.
- Use either camel case (`camelCaseNaming`) of snake case
  (`snake_case_naming`), and avoid using dots (don't use
  `dot.variable.names`). These conventions apply to functions,
  variables and files (for the latter, a `-` instead of `_` is also
  acceptable).
- Always spell out `TRUE` and `FALSE`, and resist the temptation to
  use `T` and `F` instead[^TF].
- Use 4 spaces for indenting. No tabs.
- No lines longer than 80 characters.
- Use spaces around binary operators.
- No spaces between a function name and the opening parenthesis.

[^TF]: `TRUE` and `FALSE` are reserved words; one can't use them as
variable names. This however doesn't hold for `T` and `F`. One can use
`T` and `F` as regular variable names such, as for example,
`T <- FALSE` and `F <- TRUE`!

Another good advise is to avoid re-writing the same code many
times. We will see below two strategies to do that, namely iteration
and writing new functions. This firstly reduces the amount of code
typed and hence the number of bugs, but more importantly enables
consistency. If something in your code changes, there's only one
change and it applies everywhere, rather than doing that same change
repeatedly (at the first of adding bugs and to miss some updates).

When writing code, **keep it simple and short**[^KISS]. Ideally, the
code should be evident. But when the questions tackled are not
trivial, is becomes essential to add **comments** to clarify aspects
of the script/programme. Make sure to use them to describe why
something is done, rather than explaining how things are done (which
is tyically best done by the code itself).

[^KISS]: The official motto is
[KISS](https://en.wikipedia.org/wiki/KISS_principle), *keep your
functions simple, stupid*, widely used in programming.

A general guideline to avoid bugs is to apply **defensive
programming**:

- making the code work in a predicable manner
- writing code that fails in a well-defined manner
- if something weird happens, either properly deal with it, of fail
  quickly and loudly

Here are some examples of how to do this:

- use functions like `is.numeric(x)`, `is.character(x)`,
  `is.data.frame(x)`, ... for make sure that the variable you are
  going to use is of the expected type.

- Make sure the length or dimensions of what you are going to use are
  what you expect:

```{r, eval = FALSE}
length(x) > 0
nrow(x) > 0
ncol(x) > 0
```

- Failing fast and well! Wrap such test in `stopifnot()`, so that if
  they fail, you get immediately an error, rather than risking that
  you code fails later with obscure error messages or, worse, the code
  runs to completion but returns meaningless results.

```{r, eval = FALSE}
stopifnot(length(x) > 0)
stopifnot(dim(x) > 0) ## same as next one
stopifnot(ncol(x) > 0, nrow(x) > 0)
```

## Iteration

Iteration describes the situtation when a specific operation has to be
repeated many times on different inputs of the same type. For example,
if we have a vector of numeric `x` shown below,

```{r}
(x <- 1:10)
```

and we wanted to apply the logarithm operation on each element of `x`,
it wouldn't be convenient to type

```
log(1)
log(2)
log(3)
...
log(10)
```

The concept of iteration allows us to programme the following command:

> Repeat `log` for each value of my input `x`.


or more formally

> Repeat `log(i)` where `i` takes in turn each value of my input `x`.


We will see different ways of implementing such an iteration.

### Using a `for` loop {-}

```{r}
for (i in x) {
    print(log(i))
}
```

The loop above only prints the results on screen. They aren't stored
and are lost for any further re-use, which would be very annoying if
it took much more time to perform all the calculations. In the code
chunk below, we are going to first inititalise a vector with the
appropriate number of `NA` values and, at each iteration, we then
store the result. We however now need to change our loop and iterate
of the indices of the input vector, so that we can re-use these
indices to save the results in the output vector.


```{r}
res_loop <- rep(NA, length(x))
for (i in seq_along(x)) {
    res_loop[i] <- log(x[i])
}
res_loop
```

### Using the `apply` function {-}

The apply family of functions implements our defintion of iteration
quite literally

> Repeat `log` for each value of my input `x`

is reformulated as

> For each value of my input `x`, apply `log`

and coded as


```{r}
res_apply <- sapply(x, log)
res_apply
```

There are three such function that *apply* a function iteratively:

- `sapply` iterates over a vector and returns a new vector of the same
  length as the input vector[^sapplyexception].

- `lapply` iterates over a linear input (a vector of a list) and
  returns a list of the same length as the input.

- `apply` iterates of the rows or the columns of a `data.frame` or
  `matrix` and returns a list or vector or approritate length (number
  of rows or columns). The dimension over which the iterations
  proceeds is defined by the second argument, where `1` defines rows
  and `2` defined columns[^arrayapply].


[^sapplyexception]: This is a simplification of how `sapply` works,
    that is partly defined by the the `simplify` argument and whether
    the result of applying the function on each element of the input
    can be returned as a vector. The alternative is to return a
    `list`, like `lapply`.

[^arrayapply]: This generalises to arrays with more than 2 dimensions.

When performing the same operations in different ways (using different
implementations), it is essential to verify that the results are
identical or, if not, at least compatible. The most direct way to do
the former is to use the `identical` function:

```{r}
identical(res_loop, res_apply)
```

The `purrr` package a set of `map` functions similar to the `apply`
set described above.

### Vectorisation {-}

We must not forget the obvious, which is vectorisation. Many R
functions work by default iteratively on every element of a vectors,
i.e. they work irrespectively whether the vector is of length
1[^scalar] or longer.

[^scalar]: A vector of length 1 would be called a scalar in other
    programming languages.


```{r}
res_vec <- log(x)
res_vec
```

And, as before, we check that we obtain identical results:

```{r}
identical(res_loop, res_vec)
```

### Which iteration to use? {-}

Even though they produce the same result, the iteration strategies
above aren't equal, and some should be preferred in different
situations.

- When a vectorised solution exists, this is the one that should be
  chosen. It is by far the fastest solution, but only applicable to
  existing *vectorised* functions. If you were to write your own
  function to iterate over, it is advisable to write a vectorised
  function.

- The apply functions are extremely convenient and concise, and hence
  widely used. They have a couple of advantages, including that there
  is no need to explicitly initialise a result variable and that they
  can easily be parallelised.

- For loops are the most generic solution for iteration, and they
  require to initialise the result variable that will be populated
  during the loop. As opposed to popular belief, they aren't slower
  than using apply functions. They are however the best solution if,
  during the iteration, one has the access another element than the
  one currently processed (typically, if `i` is the counter, accessing
  `i + 1` or `i - 1`).


`r msmbstyle::question_begin()`

```{r, message = FALSE, echo = FALSE}
library(rWSBIM1207)
fs <- expressions.csv()
```

Use the `expressions.csv()` function from the `rWSBIM1207` package to
get the path to `r length(fs)` csv files containing gene expression data
for a single gene and hundreds of patience. Read all data in and
combine them into a single dataframe.

Before starting, open at least two of these files them to familiarise
yourself with their structure and identify how and with what function
they need to be combined.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

Let's start by loading the required packages and check two file. We
will also use the *tidyverse* to work with the data.

```{r}
library("tidyverse")
library("rWSBIM1207")
fls <- expressions.csv()
read_csv(fls[1])
read_csv(fls[2])
```

We see that the data will have to be combined column-wise. We could do
it manually, adding news columns one by on to the first data, but this
would require to check that the observations are in the same
order. Instead, we will use a `full_join`, as seen in chapter
\@ref(sec-join).

```{r, message = FALSE}
## read the first one
res <- read_csv(fls[1])
## iterate over the other data
for (i in 2:length(fls)) {
    data_i <- read_csv(fls[i], progress = FALSE)
    res <- full_join(res, data_i)
}
res
```

`r msmbstyle::solution_end()`

<!-- Possibly also mention `while`. -->


## Conditionals

When an operation has to be executed when a condition is met, one
typically uses a `if` and `else` construct:

```
if (CONDITION) {
   ## DO SOMETHING
} else {
  ## DO SOMETHING ELSE
}
```

For examples

```{r, echo=FALSE}
set.seed(1)
```

```{r}
(x <- rnorm(1))

if (x > 0) {
   print(log(x))
} else {
   print(log(-x))
}
```

But note that in the example above, it would be much better to
simplify the code and use the absolute value of `x` before taking the
log, which will generalise the calculation for positive and negative
values ...

```{r}
log(abs(x))
```

... and works for vectors of any length thanks to vectorisation.

```{r}
x <- rnorm(10)
log(abs(x))
```

There are also in-line, vectorised versions of the `if/else`
statements seen above: the `ifelse` function that ships with the
`base` package, and `if_else` from `dplyr`. Both work similarly (the
differences are beyond the scope of this course) and have the form:

```
if_else(condition, true, false)
```

- where `condition` is the condition to be tested, and will return
  either `TRUE` or `FALSE`;
- `true` is the expression that is executed if the condition is
  `TRUE`;
- `false` is the expression that is executed otherwise (i.e if the
  condition is `FALSE`).

Here is an example, that will return 1 if `x` is strictly positive,
and 0 otherwise:


```{r}
x <- 0.5
if_else(x > 0, 1, 0)
```

The function being vectorised, the condition can be a vector of length
greater than 1:


```{r}
x <- rnorm(10)
x
if_else(x > 0, 1, 0)
```

The vectorised conditional functions can directly be used in a
standard data analysis pipeline:

```{r, echo=FALSE}
set.seed(1)
```

```{r}
x <- tibble(x = letters[1:5],
            y = rnorm(5))
x
mutate(x, z = if_else(y > 0, 1, 0))
```

## Writing new functions

A function is composed of a name, inputs (inside the parenthesis), a
body (between curly brackets) and an ouput (last statement or variably
inside the `return` statement).

```{r myfun}
my_fun <- function(x, y) {
    message("First input: ", x)
    message("Second input: ", y)
    z <- x * abs(y)
    return(z)
}
my_fun(2, -5)
```

`r msmbstyle::question_begin()`

```{r, echo = FALSE}
fun <- function(x, y) {
    res <- NA
    if (x > y) {
        res <- sample(x, y)
    } else { ## here x <= y
        res <- rnorm(1000, x, y)
    }
    return(sum(res))
}
set.seed(1L)
res1 <- fun(5, 15)
set.seed(1L)
res2 <- fun(15, 5)
```


Complete the following function. It is supposed to take two inputs,
`x` and `y` and, depending whether the `x > y` or `x <= y`, it generates
the permutation `sample(x, y)` in the first case or draws a sample
from `rnorm(1000, x, y)` in the second case. Finally, it returns the
sum of all values.


```{r, eval = FALSE}
fun <- function(x, y) {
    res <- NA
    if (   ) {
        res <- sample(, )
    } else {
        res <- rnorm(, , )
    }
    return()
}
```

To check your answer, run it with inputs 5, 15 and 15, 5, after
setting the random number generator to 1 each time with `set.seed(1)`
and you should get `r res1` and `r res2`.

`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
fun
set.seed(1L)
fun(5, 15)
set.seed(1L)
fun(15, 5)
```

`r msmbstyle::solution_end()`


## Pattern matching

One skill that comes handy more often than not is the ability to find
patterns in text and replace these. We are going to see two functions
out of many to perform such tasks. As an illustration, we are going to
mine a vector or peptides identified by mass spectrometry. The data
can be loaded from the `rWSBIM1207` data:

```{r}
library("rWSBIM1207")
data(peptides)
length(peptides)
head(peptides)
```

1. Among the `r length(peptides)` petides, which ones contain the pattern `"AAGE"`?

```{r}
pattern <- "AAGE"
## indices of matching peptides
grep(pattern, peptides)
## matching peptides
grep(pattern, peptides, value = TRUE)
## position of matching peptides
head(grepl(pattern, peptides))
```

2. Replace a pattern by another string.

```{r}
head(peptides)
head(sub("AE", "XX", peptides))
```

But careful that `sub` only replaced the first occurrence in a string!

```{r}
head(peptides)
head(sub("A", "X", peptides))
head(gsub("A", "X", peptides))
```

## Analysing data from multiple files

This exercise recapitulates the most important material that we have
seem in the course. We are going to analyse the student tests A and B
results using functionality from the `dplyr` package (seen in chapter
\@ref(sec-dplyr)). The student test files are available in the
`rWSBIM1207` package (the `interroA.csv` and `interroB.csv` functions
return their respective paths). We want to compare how the male and
female students in groups A and B have performed. To do this, we want
to calculate the mean scores and visualise the score distributions in
each groups.

`r msmbstyle::question_begin()`

Before writing any code to answer the questions above, take a couple
of minutes to think about what packages are needed to answer the
questions above and identify the individual steps needed to first
prepare the data needed to find the answers and then to produce the
answers.

`r msmbstyle::question_end()`


1. Start by loading the `rWSBIM1207` and `tidyverse` package, to get
   all the functionality we will need.


`r msmbstyle::solution_begin()`
```{r, message}
library("rWSBIM1207")
library("tidyverse")
```
`r msmbstyle::solution_end()`


2. Using the `interroA.csv()` and `interroB.csv()` functions from the
   `rWSBIM1207` package, create a vector containing the two csv file
   names.

`r msmbstyle::solution_begin()`

```{r}
fls <- c(interroA.csv(),
         interroB.csv())
fls
```

`r msmbstyle::solution_end()`


3. Iterate over the files and read each into an element of a
   list. Then, bind the two elements of the list into a single
   dataframe containing all results.

`r msmbstyle::solution_begin()`

```{r}
l <- as.list(fls)
for (i in seq_along(fls))
    l[[i]] <- read_csv(fls[i])
interros <- rbind(l[[1]], l[[2]])
interros
dim(interros)
```

or

```{r}
l <- lapply(fls, read_csv)
interros2 <- rbind(l[[1]], l[[2]])
identical(interros, interros2)
```

`r msmbstyle::solution_end()`

4. What are the mean scores for male and female students in each test
   (`interro1` to `interro4`).

`r msmbstyle::solution_begin()`

```{r}
interros %>%
    group_by(gender) %>%
    summarise(m1 = mean(interro1, na.rm = TRUE),
              m2 = mean(interro2, na.rm = TRUE),
              m3 = mean(interro3, na.rm = TRUE),
              m4 = mean(interro4, na.rm = TRUE))
```

`r msmbstyle::solution_end()`


5. What are the mean scores for male and female students in each test
   (`interro1` to `interro4`) and in each group (A and B).

`r msmbstyle::solution_begin()`

To answer this question, we need to be able to tell the two groups
apart. This information is however not available in the full
dataset. We could analyse the two test individually to answer this
particular question, but a preferred approach is the add this
information, as it will be needed later anyway.

One way would be to check the number of students that tool the two
tests, and add a new column based on these values. We still have the
two dataframes in the `l` list, which allows us to calculate the
respective number of rows and use this information to create a group
variables.

```{r}
sapply(l, nrow)
interros$group <- c(rep("A", nrow(l[[1]])),
                    rep("B", nrow(l[[2]])))
interros
```

The assumption here is that the order of the observations in the large
dataframe hasn't changed!

An alternative is to add the group information before combining the
dataframes.

```{r}
l[[1]]$group <- "A"
l[[2]]$group <- "B"
interros2 <- rbind(l[[1]], l[[2]])
identical(interros, interros2)
```

Now we can group by `gender` and `group` to perform our summary
calculations:

```{r}
interros %>%
    group_by(gender, group) %>%
    summarise(m1 = mean(interro1, na.rm = TRUE),
              m2 = mean(interro2, na.rm = TRUE),
              m3 = mean(interro3, na.rm = TRUE),
              m4 = mean(interro4, na.rm = TRUE))
```

`r msmbstyle::solution_end()`

6. Instead of comparing the means for the four tests in each group and
   gender, visualise their distributions using boxplots. In addition
   to the boxplots, overlay the actual score values (using the
   *jitter* geom) and colour the points based on the students height.


`r msmbstyle::solution_begin()`

The produce this visualisation, we first need to convert the data we
have in a long format using `pivot_longer()`. Below, we create two new
variables: `test`, that contains `interro1`, ..., `interro4`, and
`score`, that contains the students marks the actual test.

We obtain a new tibble that contains 4 times more rows that initially,
as the tests, that were in 4 different columns, have now been gathered
as a single new variable.

```{r}
interros2 <- pivot_longer(interros,
                          names_to = "test",
                          values_to = "score",
                          starts_with("interro"))
interros2
```

We can now map the new `test` and `score` variables to the *x* and *y*
axes and construct the visualisation.

```{r}
ggplot(interros2, aes(y = score, x = test)) +
    geom_boxplot() +
    geom_jitter(aes(colour = height)) +
    facet_grid(gender ~ group)

```

`r msmbstyle::solution_end()`


## Additional exercises

`r msmbstyle::question_begin()`

Complete the following function that converts meters to yards, knowing
that that 1 yard corresponds to 0.9144 meters

```{r, eval = FALSE}
meters_to_yards <- function(argument goes here) {
  yards <- _code goes here!_
  return(yards)
}

```
`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`

Write functions that convert temperatures:

- from Fahrenheit to Kelvin, where $T_K = (T_F - 32) \times 5/9 + 273.15$
- fom Celsius to Kelvin, where $T_K = T_C + 273.15$
- from Kelvin to Celsis
- from Fahrenheit to Celsius

`r msmbstyle::question_end()`

<!--chapter:end:80-prog.Rmd-->

# Conclusions {#sec-ccl}

In this course, we have seen the importance of structure data. Good
data structure starts with simple, tidy tabular data, whether it is
manually encoded in spreadsheet, or handled in R as dataframes or
tibbles. More complex data, that doesn't fit in tabular data, can be
modelled into dedicated objects that display specialised
behaviour. Structured data allows us to reason on that data, without
having to look it at. Reasoning on and generalisation of data in turn
allows to manipulate and visualise it, i.e. to explore, analyse and
understand it. The cherry on top of the data analysis cake is to be
able to reproduce an analysis, either oneself or share it in a way
that other can.

As mentioned in the preamble, the goal of this course is obviously not
for students that take it to qualify as bioinformaticians at the
end. However, what is important is to appreciate the importance of
data and their analysis, and to become fluent in exploring, discussing
and communicating around data. A shared appreciation of data and their
complexity will hopefully reduce the distinction between
bioinformaticians and experimental scientists. Indeed, at the end of
the end, it's useful to remember that

> We are all biologists, in that we study biology. Some use wet lab
> experiments, others dry lab techniques.

## Next steps

- Statistics and machine learning (see your statistics courses and the
  follow course
  [WSBIM1322](https://github.com/UCLouvain-CBIO/WSBIM1322)).
- Getting better at programming and data analysis. See
  [@r4ds:2017] and [@advancedR].
- Evolving scripts into tools/packages [@rpkgs:2015].
- Other tools: unix command line and git/GitHub [@Perez-Riverol:2016].
  See also this [short
  tutorial](https://lgatto.github.io/github-intro/).
- Omics data analysis (see upcoming
  [WSBIM2122](https://github.com/UCLouvain-CBIO/WSBIM2122) course).

<!--chapter:end:90-ccl.Rmd-->

# Annex {#sec-anx}

## Local installation

This section describes how to install R, Rstudio, and a set of
important packages for the course.

1. Download R from the CRAN page:
   [https://cloud.r-project.org/](https://cloud.r-project.org/). At
   the top of that page, choose the *Download R* link corresponding to
   your operating system.

   If you use Windows, follow *install R for the first time*, then
   click the link to download R. The installation procedure is like
   any other software, and you can safely use all default options.

   If you use Mac (OS X), download the *pkg* installer that matches
   you OS version and install like any other software

   Linux users are advised to use their package manager.

2. Download and install the Rstudio Desktop Open source edition:
   [https://rstudio.com/products/rstudio/download/#download](https://rstudio.com/products/rstudio/download/#download). Choose
   the installer for your operating system and version. Install as any
   other software.

3. Start Rstudio and install the following packages:

```{r, eval = FALSE}
install.packages(c("tidyverse", "rmarkdown", "remotes", "devtools"))
install.packages("BiocManager")
BiocManager::install("UCLouvain-CBIO/rWSBIM1207")
```

If during any of the package installation steps, you get asked to
update some packages, for example with a message like

```
Old packages: 'tinytex', 'TSP', 'tximeta', 'umap', 'vctrs', 'waveslim',
  'xcms', 'xlsx', 'xml2'
Update all/some/none? [a/s/n]:
```

You can answer `a` to update all your packages.

4. To be able to compile Rmd files into pdf, you will also need to
   install an additional software. On Windows, the easiest is to
   install `tinytex` direct from the R console with
   `tinytex::install_tinytex`[^tinytex].  For Max OSX, you will need
   to install
   [MacTeX](https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-windows-10-952b0c48bfa9). [This
   post](https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-macos-high-sierra-e7b5705c9fd)
   provides details. For linux, use your package manager to install
   the tex-live suite.

   Compilation to html will not require any additional installations
   and can also be used.

[^tinytex]: There might be messages or event errors during the
    installation of `tinytex`. At the end, you can run
    `tinytex:::is_tiny()` to verify if it installed successfully (if
    `TRUE` is returned).

## Using a cloud R/RStudio instance

An alternative to local installation is to use *virtual machines* that
run in the cloud. Here, we will use Renku platform provided by the
Swiss Data Science Center (SDSC).

1. Create a new renku user - click to *Login - Sign up* button on
   [https://renkulab.io/](https://renkulab.io/), click *Register* at
   the bottom of the login screen and follow instructions.

   Once completed, you will get to the following (or similar) screen,
   belonging here to user `WSBIM1207.student`.

```{r, fig.cap = "Renky labding page.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen1.png")
```

2. Now that you have created a Renku account, you will need to fork
   (i.e. copy) the course renku project into your own account. To do
   so, open the following link:
   [https://renkulab.io/projects/laurent.gatto/wsbim-bioinfo](https://renkulab.io/projects/laurent.gatto/wsbim-bioinfo)


```{r, fig.cap = "The WSBIM-bioinfo renku project page.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen2.png")
```
   The second support video below also demonstrates the following steps for another renku project.

3. Fork the project under your own account by click the `fork` button
   in the rop right corner (see previous screenshot). You can keep all
   the default values in the popup window and click on `Fork` again.

```{r, fig.cap = "Forking the WSBIM-bioinfor renku project page.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen3.png")
```

4. The next window shows your personal copy of the WSBIM-bioinfo Renku
   project.

<!-- ```{r, fig.cap = "Forked WSBIM1207 renku project page.", echo = FALSE, message = FALSE} -->
<!-- knitr::include_graphics("./figs/renku_screen4.png") -->
<!-- ``` -->

5. Click the `Environments` tab and the `New` button to create a
   runnable cloud environment that contains R, RStudio, and all
   pre-installed packages. At this stage, if you already see an
   interactive environment (with a green tick box), you can
   immediately skip to step 8. Otherwise, the building could take up
   to 50 minutes (depending on caching). You can safely shut your
   computer down at this stage, as the environment is build in the
   cloud.


```{r, fig.cap = "The *Interactive Environment* tab.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen5.png")
```


```{r, fig.cap = "The Renku environment is being build.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen6.png")
```

6. Once build, you can start the cloud environment. Make sure you keep
   the default settings, i.e default environment `/rstudio` and `0.25`
   CPUs and `1G` of RAM. For this course, these low resources are
   enough[^res].

[^res]: Given that the resources are provided for free by the SDSC and
    are shared by all users, it is good practice to only use the
    resources that are needed.

```{r, fig.cap = "The environment build settings.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen7.png")
```

7. It will take some more time to get the environment started, shown
   below.

```{r, fig.cap = "The Renku environment is being build.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen8.png")
```

8. Once ready, you now only have to click the blue `Connect` button to
   start RStudio in the cloud.

```{r, fig.cap = "The Renku environment is now built.", echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen9.png")
```

```{r, fig.cap = "The Renku environment running RStudio.", fig.fullwidth = TRUE,  echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_screen10.png")
```

The RStudio instance has already all the required packages
pre-installed. To close the remote RStudio, either use `q()` in the R
console, click `File` and `Quit Session`, or the red round icon in the
top corner. Next time, it won't be necessary to repeat all the
steps. Simply navigate to your project's environment tab, rebuild the
environment in a matter of seconds and connect. The video below
illustrates these very same steps.

<iframe width="560" height="315" src="https://www.youtube.com/embed/veSrycUFcVs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

It is easey to upload files directly to the remote RStudio instance by
using the `Upload` button in RStudio's file navigator menu. To
download files or directories, select the files/folders in the file
menu and click on `More` and then `Export`. The video below briefly
illustrates this and demonstrates how to save any changes (including
new files and folders) back to Renku.

<iframe width="560" height="315" src="https://www.youtube.com/embed/0DQhX7Kwbgo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


Note that projects can only be forked once. If you want or need to
fork the WSBIM1207 Renku project again, you first need to delete your
existing fork:

1. Navigate to your project page.
2. In the top right corner, click on **View in GitLab**.
3. In the bottom left corner, click on **Settings**.
4. Click on **Advanced** at the bottom of the page.
5. Navigate to the very bottom and click on the red button **Remove
   project**.
6. You will be to type the name of the project (wsbim1207) en then
   confirm that you want to permanently delete it.

You can now restart the procedure described above.

### Saving your work

```{r, fig.cap = "Save your files to RenkuLab.", fig.margin = TRUE,  echo = FALSE, message = FALSE}
knitr::include_graphics("./figs/renku_save.png")
```

1. Make sure you regularly save the scripts that you are editing, to
   save you from intermittent connection issues.
2. Once you have finished working, save you data back to RenkuLab
   files using the `Addins` and `Save to RenkuLab`. This will make
   sure that your files are safe even when the environment is closed
   completely.
3. In addition, it is recommended to download (export) all your files
   as an additional backup.

### More about Renku {-}

Here are a set of introductory videos (in French) prepared by
[Christine Choirat](https://scholar.harvard.edu/cchoirat/home):

- [The Renku
  project](https://www.dropbox.com/s/gf0p3jjevt4e13s/V1_Renku_Intro_FR.mov?dl=0)
  (and
  [slides](https://docs.google.com/presentation/d/18tPAb1B3PKcOBhMnITm0BNQfPG8RbVWq4iCODehljjE/edit#slide=id.p1))
- [Getting started with a (forked) project](https://www.dropbox.com/s/fus0g6f3hzglpjy/V2_Launching_RStudio_FR.mov?dl=0)
- [Saving changes back to Renku](https://www.dropbox.com/s/l119urtzf21vm06/V3_Saving_Changes_to_Renku.mov?dl=0) (this video is the command-line version of
   the *Save to RenkuLab* Addin).


## Universal Desktop Service at UCLouvain

Students at the UCLouvain can use the [Universal Desktop
Service](https://intranet.uclouvain.be/fr/myucl/services-informatiques/uds-universal-desktop-service.html)
(UDS) to access the software and files installed on the university
network through a virtual working environment.

<!--chapter:end:95-annex.Rmd-->

# Session information

The following packages have been used to generate this document.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'msmbstyle'
), 'packages.bib')
```

```{r si}
sessionInfo()
```

## R package setup {#sec-setup2}

To install all necessary packages to run all the code, please execute
the following code:

```{r pkgs, eval=TRUE, echo=FALSE, results='markup', comment=''}
pkgs <- .packages()
pkgs <- paste0('"', pkgs, '"')
cmd <- paste(sort(pkgs), collapse = ", ")
cmd <- paste0("pkgs <- c(", cmd, ")")
cmd <- strwrap(cmd)
for (i in 2:length(cmd))
    cmd[i] <- paste0("          ", cmd[i])
cat(cmd, sep = "\n")
```

```{r setup, eval=FALSE}
if (!require("BiocManager"))
   install.packages("BiocManager")
BiocManager::install(pkgs)
```

<!--chapter:end:99-si.Rmd-->

